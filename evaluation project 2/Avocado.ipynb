{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab76b869",
   "metadata": {},
   "source": [
    "# Problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a3e417",
   "metadata": {},
   "source": [
    "Avocado is a bright green color fruit, which has a rich source of  vitamins C, E, K, and B6 and it is consumed heavily in the United States \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cd0842",
   "metadata": {},
   "source": [
    "the dataset that will be used for building the model has been collected from Hass Avocado Board website in May of 2018. The dataset contains information about retail scan data for National retail volume (units) and price.\n",
    "some of the important columns in the dataset are:\n",
    "\n",
    "\n",
    "Date - The date of the observation\n",
    "AveragePrice - the average price of a single avocado\n",
    "type - conventional or organic\n",
    "year - the year\n",
    "Region - the city or region of the observation\n",
    "Total Volume - Total number of avocados sold\n",
    "4046 - Total number of avocados with PLU 4046 sold\n",
    "4225 - Total number of avocados with PLU 4225 sold\n",
    "4770 - Total number of avocados with PLU 4770 sold\n",
    "\n",
    "Goal:\n",
    "the goal of the model is to predict the average price of the different types of avocado, which is continuous in nature "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f033906",
   "metadata": {},
   "source": [
    "# loading all the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6291dc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c23445d",
   "metadata": {},
   "source": [
    "# loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "04d95357",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('avocado.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bfe82ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Date</th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2015-12-27</td>\n",
       "      <td>1.33</td>\n",
       "      <td>64236.62</td>\n",
       "      <td>1036.74</td>\n",
       "      <td>54454.85</td>\n",
       "      <td>48.16</td>\n",
       "      <td>8696.87</td>\n",
       "      <td>8603.62</td>\n",
       "      <td>93.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-12-20</td>\n",
       "      <td>1.35</td>\n",
       "      <td>54876.98</td>\n",
       "      <td>674.28</td>\n",
       "      <td>44638.81</td>\n",
       "      <td>58.33</td>\n",
       "      <td>9505.56</td>\n",
       "      <td>9408.07</td>\n",
       "      <td>97.49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2015-12-13</td>\n",
       "      <td>0.93</td>\n",
       "      <td>118220.22</td>\n",
       "      <td>794.70</td>\n",
       "      <td>109149.67</td>\n",
       "      <td>130.50</td>\n",
       "      <td>8145.35</td>\n",
       "      <td>8042.21</td>\n",
       "      <td>103.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2015-12-06</td>\n",
       "      <td>1.08</td>\n",
       "      <td>78992.15</td>\n",
       "      <td>1132.00</td>\n",
       "      <td>71976.41</td>\n",
       "      <td>72.58</td>\n",
       "      <td>5811.16</td>\n",
       "      <td>5677.40</td>\n",
       "      <td>133.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2015-11-29</td>\n",
       "      <td>1.28</td>\n",
       "      <td>51039.60</td>\n",
       "      <td>941.48</td>\n",
       "      <td>43838.39</td>\n",
       "      <td>75.78</td>\n",
       "      <td>6183.95</td>\n",
       "      <td>5986.26</td>\n",
       "      <td>197.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>2015-11-22</td>\n",
       "      <td>1.26</td>\n",
       "      <td>55979.78</td>\n",
       "      <td>1184.27</td>\n",
       "      <td>48067.99</td>\n",
       "      <td>43.61</td>\n",
       "      <td>6683.91</td>\n",
       "      <td>6556.47</td>\n",
       "      <td>127.44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>2015-11-15</td>\n",
       "      <td>0.99</td>\n",
       "      <td>83453.76</td>\n",
       "      <td>1368.92</td>\n",
       "      <td>73672.72</td>\n",
       "      <td>93.26</td>\n",
       "      <td>8318.86</td>\n",
       "      <td>8196.81</td>\n",
       "      <td>122.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>2015-11-08</td>\n",
       "      <td>0.98</td>\n",
       "      <td>109428.33</td>\n",
       "      <td>703.75</td>\n",
       "      <td>101815.36</td>\n",
       "      <td>80.00</td>\n",
       "      <td>6829.22</td>\n",
       "      <td>6266.85</td>\n",
       "      <td>562.37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>2015-11-01</td>\n",
       "      <td>1.02</td>\n",
       "      <td>99811.42</td>\n",
       "      <td>1022.15</td>\n",
       "      <td>87315.57</td>\n",
       "      <td>85.34</td>\n",
       "      <td>11388.36</td>\n",
       "      <td>11104.53</td>\n",
       "      <td>283.83</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>2015-10-25</td>\n",
       "      <td>1.07</td>\n",
       "      <td>74338.76</td>\n",
       "      <td>842.40</td>\n",
       "      <td>64757.44</td>\n",
       "      <td>113.00</td>\n",
       "      <td>8625.92</td>\n",
       "      <td>8061.47</td>\n",
       "      <td>564.45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        Date  AveragePrice  Total Volume     4046       4225  \\\n",
       "0           0  2015-12-27          1.33      64236.62  1036.74   54454.85   \n",
       "1           1  2015-12-20          1.35      54876.98   674.28   44638.81   \n",
       "2           2  2015-12-13          0.93     118220.22   794.70  109149.67   \n",
       "3           3  2015-12-06          1.08      78992.15  1132.00   71976.41   \n",
       "4           4  2015-11-29          1.28      51039.60   941.48   43838.39   \n",
       "5           5  2015-11-22          1.26      55979.78  1184.27   48067.99   \n",
       "6           6  2015-11-15          0.99      83453.76  1368.92   73672.72   \n",
       "7           7  2015-11-08          0.98     109428.33   703.75  101815.36   \n",
       "8           8  2015-11-01          1.02      99811.42  1022.15   87315.57   \n",
       "9           9  2015-10-25          1.07      74338.76   842.40   64757.44   \n",
       "\n",
       "     4770  Total Bags  Small Bags  Large Bags  XLarge Bags          type  \\\n",
       "0   48.16     8696.87     8603.62       93.25          0.0  conventional   \n",
       "1   58.33     9505.56     9408.07       97.49          0.0  conventional   \n",
       "2  130.50     8145.35     8042.21      103.14          0.0  conventional   \n",
       "3   72.58     5811.16     5677.40      133.76          0.0  conventional   \n",
       "4   75.78     6183.95     5986.26      197.69          0.0  conventional   \n",
       "5   43.61     6683.91     6556.47      127.44          0.0  conventional   \n",
       "6   93.26     8318.86     8196.81      122.05          0.0  conventional   \n",
       "7   80.00     6829.22     6266.85      562.37          0.0  conventional   \n",
       "8   85.34    11388.36    11104.53      283.83          0.0  conventional   \n",
       "9  113.00     8625.92     8061.47      564.45          0.0  conventional   \n",
       "\n",
       "   year  region  \n",
       "0  2015  Albany  \n",
       "1  2015  Albany  \n",
       "2  2015  Albany  \n",
       "3  2015  Albany  \n",
       "4  2015  Albany  \n",
       "5  2015  Albany  \n",
       "6  2015  Albany  \n",
       "7  2015  Albany  \n",
       "8  2015  Albany  \n",
       "9  2015  Albany  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a48cfbd",
   "metadata": {},
   "source": [
    "# exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e85c158a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18249, 14)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320a3f33",
   "metadata": {},
   "source": [
    "in the dataset, there are 18249 rows and 14 columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "389fb16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18249 entries, 0 to 18248\n",
      "Data columns (total 14 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   Unnamed: 0    18249 non-null  int64  \n",
      " 1   Date          18249 non-null  object \n",
      " 2   AveragePrice  18249 non-null  float64\n",
      " 3   Total Volume  18249 non-null  float64\n",
      " 4   4046          18249 non-null  float64\n",
      " 5   4225          18249 non-null  float64\n",
      " 6   4770          18249 non-null  float64\n",
      " 7   Total Bags    18249 non-null  float64\n",
      " 8   Small Bags    18249 non-null  float64\n",
      " 9   Large Bags    18249 non-null  float64\n",
      " 10  XLarge Bags   18249 non-null  float64\n",
      " 11  type          18249 non-null  object \n",
      " 12  year          18249 non-null  int64  \n",
      " 13  region        18249 non-null  object \n",
      "dtypes: float64(9), int64(2), object(3)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45006d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0      0\n",
       "Date            0\n",
       "AveragePrice    0\n",
       "Total Volume    0\n",
       "4046            0\n",
       "4225            0\n",
       "4770            0\n",
       "Total Bags      0\n",
       "Small Bags      0\n",
       "Large Bags      0\n",
       "XLarge Bags     0\n",
       "type            0\n",
       "year            0\n",
       "region          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543dfa17",
   "metadata": {},
   "source": [
    "there is absence of any null value in the dataset and the columns are mainly of int 64 and float 64 data type. only columns date and region are of object type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f816f2a",
   "metadata": {},
   "source": [
    "the column names containing continous values are:\n",
    "AveragePrice    \n",
    "Total Volume   \n",
    "4046            \n",
    "4225            \n",
    "4770            \n",
    "Total Bags      \n",
    "Small Bags      \n",
    "Large Bags      \n",
    "XLarge Bags     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa898bed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['conventional', 'organic'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"type\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbdd133a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Albany', 'Atlanta', 'BaltimoreWashington', 'Boise', 'Boston',\n",
       "       'BuffaloRochester', 'California', 'Charlotte', 'Chicago',\n",
       "       'CincinnatiDayton', 'Columbus', 'DallasFtWorth', 'Denver',\n",
       "       'Detroit', 'GrandRapids', 'GreatLakes', 'HarrisburgScranton',\n",
       "       'HartfordSpringfield', 'Houston', 'Indianapolis', 'Jacksonville',\n",
       "       'LasVegas', 'LosAngeles', 'Louisville', 'MiamiFtLauderdale',\n",
       "       'Midsouth', 'Nashville', 'NewOrleansMobile', 'NewYork',\n",
       "       'Northeast', 'NorthernNewEngland', 'Orlando', 'Philadelphia',\n",
       "       'PhoenixTucson', 'Pittsburgh', 'Plains', 'Portland',\n",
       "       'RaleighGreensboro', 'RichmondNorfolk', 'Roanoke', 'Sacramento',\n",
       "       'SanDiego', 'SanFrancisco', 'Seattle', 'SouthCarolina',\n",
       "       'SouthCentral', 'Southeast', 'Spokane', 'StLouis', 'Syracuse',\n",
       "       'Tampa', 'TotalUS', 'West', 'WestTexNewMexico'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"region\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "899374d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2015, 2016, 2017, 2018], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"year\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "1f8a3aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year  type        \n",
       "2015  conventional    2808\n",
       "      organic         2807\n",
       "2016  conventional    2808\n",
       "      organic         2808\n",
       "2017  conventional    2862\n",
       "      organic         2860\n",
       "2018  conventional     648\n",
       "      organic          648\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking relation of the type of avocado with the average price\n",
    "df.groupby('year')['type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747a2893",
   "metadata": {},
   "source": [
    "observation: it can be seen that in the year 2017, highest number of avocados have been sold and in every year since 2015, the sale of both types of avocadi is nearly the same "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba507c0",
   "metadata": {},
   "source": [
    "# descriptive analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "d389d327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>18249.000000</td>\n",
       "      <td>18249.000000</td>\n",
       "      <td>1.824900e+04</td>\n",
       "      <td>1.824900e+04</td>\n",
       "      <td>1.824900e+04</td>\n",
       "      <td>1.824900e+04</td>\n",
       "      <td>1.824900e+04</td>\n",
       "      <td>1.824900e+04</td>\n",
       "      <td>1.824900e+04</td>\n",
       "      <td>18249.000000</td>\n",
       "      <td>18249.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>24.232232</td>\n",
       "      <td>1.405978</td>\n",
       "      <td>8.506440e+05</td>\n",
       "      <td>2.930084e+05</td>\n",
       "      <td>2.951546e+05</td>\n",
       "      <td>2.283974e+04</td>\n",
       "      <td>2.396392e+05</td>\n",
       "      <td>1.821947e+05</td>\n",
       "      <td>5.433809e+04</td>\n",
       "      <td>3106.426507</td>\n",
       "      <td>2016.147899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>15.481045</td>\n",
       "      <td>0.402677</td>\n",
       "      <td>3.453545e+06</td>\n",
       "      <td>1.264989e+06</td>\n",
       "      <td>1.204120e+06</td>\n",
       "      <td>1.074641e+05</td>\n",
       "      <td>9.862424e+05</td>\n",
       "      <td>7.461785e+05</td>\n",
       "      <td>2.439660e+05</td>\n",
       "      <td>17692.894652</td>\n",
       "      <td>0.939938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>8.456000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2015.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>1.083858e+04</td>\n",
       "      <td>8.540700e+02</td>\n",
       "      <td>3.008780e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.088640e+03</td>\n",
       "      <td>2.849420e+03</td>\n",
       "      <td>1.274700e+02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2015.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>24.000000</td>\n",
       "      <td>1.370000</td>\n",
       "      <td>1.073768e+05</td>\n",
       "      <td>8.645300e+03</td>\n",
       "      <td>2.906102e+04</td>\n",
       "      <td>1.849900e+02</td>\n",
       "      <td>3.974383e+04</td>\n",
       "      <td>2.636282e+04</td>\n",
       "      <td>2.647710e+03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2016.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.660000</td>\n",
       "      <td>4.329623e+05</td>\n",
       "      <td>1.110202e+05</td>\n",
       "      <td>1.502069e+05</td>\n",
       "      <td>6.243420e+03</td>\n",
       "      <td>1.107834e+05</td>\n",
       "      <td>8.333767e+04</td>\n",
       "      <td>2.202925e+04</td>\n",
       "      <td>132.500000</td>\n",
       "      <td>2017.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>52.000000</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>6.250565e+07</td>\n",
       "      <td>2.274362e+07</td>\n",
       "      <td>2.047057e+07</td>\n",
       "      <td>2.546439e+06</td>\n",
       "      <td>1.937313e+07</td>\n",
       "      <td>1.338459e+07</td>\n",
       "      <td>5.719097e+06</td>\n",
       "      <td>551693.650000</td>\n",
       "      <td>2018.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0  AveragePrice  Total Volume          4046          4225  \\\n",
       "count  18249.000000  18249.000000  1.824900e+04  1.824900e+04  1.824900e+04   \n",
       "mean      24.232232      1.405978  8.506440e+05  2.930084e+05  2.951546e+05   \n",
       "std       15.481045      0.402677  3.453545e+06  1.264989e+06  1.204120e+06   \n",
       "min        0.000000      0.440000  8.456000e+01  0.000000e+00  0.000000e+00   \n",
       "25%       10.000000      1.100000  1.083858e+04  8.540700e+02  3.008780e+03   \n",
       "50%       24.000000      1.370000  1.073768e+05  8.645300e+03  2.906102e+04   \n",
       "75%       38.000000      1.660000  4.329623e+05  1.110202e+05  1.502069e+05   \n",
       "max       52.000000      3.250000  6.250565e+07  2.274362e+07  2.047057e+07   \n",
       "\n",
       "               4770    Total Bags    Small Bags    Large Bags    XLarge Bags  \\\n",
       "count  1.824900e+04  1.824900e+04  1.824900e+04  1.824900e+04   18249.000000   \n",
       "mean   2.283974e+04  2.396392e+05  1.821947e+05  5.433809e+04    3106.426507   \n",
       "std    1.074641e+05  9.862424e+05  7.461785e+05  2.439660e+05   17692.894652   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00       0.000000   \n",
       "25%    0.000000e+00  5.088640e+03  2.849420e+03  1.274700e+02       0.000000   \n",
       "50%    1.849900e+02  3.974383e+04  2.636282e+04  2.647710e+03       0.000000   \n",
       "75%    6.243420e+03  1.107834e+05  8.333767e+04  2.202925e+04     132.500000   \n",
       "max    2.546439e+06  1.937313e+07  1.338459e+07  5.719097e+06  551693.650000   \n",
       "\n",
       "               year  \n",
       "count  18249.000000  \n",
       "mean    2016.147899  \n",
       "std        0.939938  \n",
       "min     2015.000000  \n",
       "25%     2015.000000  \n",
       "50%     2016.000000  \n",
       "75%     2017.000000  \n",
       "max     2018.000000  "
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b59758",
   "metadata": {},
   "source": [
    "observation: huge number of outliers are present in some of the columns because of large difference between the 75th quartile and max\n",
    "    \n",
    "there are no null values in the dataset\n",
    "\n",
    "skewness is present in some of the columns and it can be idneitified from the difference  between the mean value and 50th quartile value of the columns \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08f9a56",
   "metadata": {},
   "source": [
    "# visualisation of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "caddf64b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='type', ylabel='count'>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(df['type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04f053e",
   "metadata": {},
   "source": [
    "comparison of type of avocado with price "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "e6b9bebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='type', ylabel='AveragePrice'>"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(x=df['type'],y=df['AveragePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed4f602",
   "metadata": {},
   "source": [
    "it can be seen that the average price of type of conventional avocado is lesser than that of organic types. the price of conventional type ranges between 0 to less than 1.2, while organic type have average price range between 0 to 1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25b0766d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='year', ylabel='count'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(df['year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44039ee5",
   "metadata": {},
   "source": [
    "observation: in the dataset, maximum observation have been collected for the year 2015, 2016, 2017 and the minimum observation for 2018.\n",
    "    2018 has nearly 1000 observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "95b4811f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='year', ylabel='AveragePrice'>"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(x=df['year'],y=df['AveragePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6600fb20",
   "metadata": {},
   "source": [
    "observation: the average price of the avocados were comparatively higher in the year 2017 and low in 2018 and 2016. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cf007199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='AveragePrice', ylabel='Density'>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    " sns.distplot(df['AveragePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e799c52b",
   "metadata": {},
   "source": [
    "observation: it is the plot of the first target variable and it can be seen that it is a continuous data. the maximum density of data is between 0.75 to 1.75. it can be concluded that people are prefering to buy the avocados within this price range and the data is normally distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "09935045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Total Volume', ylabel='Density'>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    " sns.distplot(df['Total Volume'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1493acb",
   "metadata": {},
   "source": [
    "observation: the maximum total volume of avocado is between 0.25*10^7 to 1*10^7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "596047b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='4046', ylabel='Density'>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    " sns.distplot(df['4046'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c90403ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='4225', ylabel='Density'>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    " sns.distplot(df['4225'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cf74ae68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='4770', ylabel='Density'>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    " sns.distplot(df['4770'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "424079ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Total Bags', ylabel='Density'>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    " sns.distplot(df['Total Bags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7297a4eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Small Bags', ylabel='Density'>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    " sns.distplot(df['Small Bags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e94504db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Large Bags', ylabel='Density'>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    " sns.distplot(df['Large Bags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d1f3bbed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='XLarge Bags', ylabel='Density'>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    " sns.distplot(df['XLarge Bags'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad82795",
   "metadata": {},
   "source": [
    "from the above distribution plots, it can be observed that the data are skewed and needs to be adjusted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efca05f7",
   "metadata": {},
   "source": [
    "we can drop the Unnamed: 0 because it has no correlation with the target variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "88a36713",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Unnamed: 0','Date'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb08bef",
   "metadata": {},
   "source": [
    "# Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "85e4097d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le=LabelEncoder()\n",
    "l=[\"type\",\"year\",\"region\"]\n",
    "for i in l:\n",
    "    df[i]=le.fit_transform(df[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "13409b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    5722\n",
       "1    5616\n",
       "0    5615\n",
       "3    1296\n",
       "Name: year, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"year\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "54fb6ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    9126\n",
       "1    9123\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"type\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2b071c",
   "metadata": {},
   "source": [
    "# visualisation using the heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3a500128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "sns.heatmap(df.corr(),annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "be623e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4046           -0.208317\n",
       "Total Volume   -0.192752\n",
       "4770           -0.179446\n",
       "Total Bags     -0.177088\n",
       "Small Bags     -0.174730\n",
       "Large Bags     -0.172940\n",
       "4225           -0.172928\n",
       "Unnamed: 0     -0.133008\n",
       "XLarge Bags    -0.117592\n",
       "region         -0.011716\n",
       "year            0.093197\n",
       "type            0.615845\n",
       "AveragePrice    1.000000\n",
       "Name: AveragePrice, dtype: float64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()['AveragePrice'].sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49886382",
   "metadata": {},
   "source": [
    "# further visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412e8fd5",
   "metadata": {},
   "source": [
    "from the heatmap, we can see that Total Bags,  Small Bags,   Large Bags and  XLarge Bags have strong correlation   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "e897c29c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Small Bags', ylabel='Total Volume'>"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.regplot(x=df[\"Small Bags\"], y=df[\"Total Volume\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "c84fa41f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Large Bags', ylabel='Total Volume'>"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.regplot(x=df[\"Large Bags\"], y=df[\"Total Volume\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "31d269da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='XLarge Bags', ylabel='Total Volume'>"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.regplot(x=df[\"XLarge Bags\"], y=df[\"Total Volume\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "211c4d0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='4046', ylabel='Total Volume'>"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.regplot(x=df[\"4046\"], y=df[\"Total Volume\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d50db9a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='4225', ylabel='Total Volume'>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.regplot(x=df[\"4225\"], y=df[\"Total Volume\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "ef7dcb6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='4770', ylabel='Total Volume'>"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.regplot(x=df[\"4770\"], y=df[\"Total Volume\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638e7b30",
   "metadata": {},
   "source": [
    "observation: from the plots, it can be seen that the columns have strong correlation with the Total Volume column  we will be dropping both these column to remove multicollinearity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d67a41f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnew=df.drop([\"Total Volume\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c131cb4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['AveragePrice', '4046', '4225', '4770', 'Total Bags', 'Small Bags',\n",
       "       'Large Bags', 'XLarge Bags', 'type', 'year', 'region'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnew.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "534747ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking correlation with the column TotalBags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "24c5c113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1800 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,25))\n",
    "plotnumber =1\n",
    "for column in df:\n",
    "    if plotnumber <= 10:\n",
    "        ax = plt.subplot(11,3,plotnumber)\n",
    "        sns.regplot(x=df[column],y=df[\"AveragePrice\"])\n",
    "        plt.xlabel(column,fontsize=20)\n",
    "    plotnumber+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ee9bb3",
   "metadata": {},
   "source": [
    "we can also see that the columns have correlation with the TotalBags column, we can drop it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8bd24916",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnew=df.drop([\"Total Bags\",\"Total Volume\"],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994946c0",
   "metadata": {},
   "source": [
    "# variance inflation factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "17fe85f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries to calculate the variance inflation factor, which may result in low accuracy\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cae910a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_vif(x1):\n",
    "    vif=pd.DataFrame()\n",
    "    vif[\"Variables\"]=x1.columns\n",
    "    vif[\"VIF factor\"]=[variance_inflation_factor(x1.values,i) for i in range(x1.shape[1])]\n",
    "    return (vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "89d25d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df.drop([\"AveragePrice\"],axis=1)\n",
    "y=df[\"AveragePrice\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c7c911c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variables</th>\n",
       "      <th>VIF factor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total Volume</td>\n",
       "      <td>5.218092e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4046</td>\n",
       "      <td>6.954550e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4225</td>\n",
       "      <td>6.339859e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4770</td>\n",
       "      <td>4.978784e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Total Bags</td>\n",
       "      <td>2.502000e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Small Bags</td>\n",
       "      <td>1.429714e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Large Bags</td>\n",
       "      <td>1.521486e+13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>XLarge Bags</td>\n",
       "      <td>7.855778e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>type</td>\n",
       "      <td>1.787467e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>year</td>\n",
       "      <td>1.982500e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>region</td>\n",
       "      <td>2.371649e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Variables    VIF factor\n",
       "0   Total Volume  5.218092e+09\n",
       "1           4046  6.954550e+08\n",
       "2           4225  6.339859e+08\n",
       "3           4770  4.978784e+06\n",
       "4     Total Bags  2.502000e+14\n",
       "5     Small Bags  1.429714e+14\n",
       "6     Large Bags  1.521486e+13\n",
       "7    XLarge Bags  7.855778e+10\n",
       "8           type  1.787467e+00\n",
       "9           year  1.982500e+00\n",
       "10        region  2.371649e+00"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_vif(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140dfeb2",
   "metadata": {},
   "source": [
    "it can be observed that the VIF is critically high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c7b46aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=dnew.drop([\"AveragePrice\"],axis=1)\n",
    "y=dnew[\"AveragePrice\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e9ed0624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variables</th>\n",
       "      <th>VIF factor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4046</td>\n",
       "      <td>10.628848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4225</td>\n",
       "      <td>14.077990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4770</td>\n",
       "      <td>5.595269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Small Bags</td>\n",
       "      <td>20.647290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Large Bags</td>\n",
       "      <td>5.833962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XLarge Bags</td>\n",
       "      <td>3.517597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>type</td>\n",
       "      <td>1.785626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>year</td>\n",
       "      <td>1.979974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>region</td>\n",
       "      <td>2.371146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Variables  VIF factor\n",
       "0         4046   10.628848\n",
       "1         4225   14.077990\n",
       "2         4770    5.595269\n",
       "3   Small Bags   20.647290\n",
       "4   Large Bags    5.833962\n",
       "5  XLarge Bags    3.517597\n",
       "6         type    1.785626\n",
       "7         year    1.979974\n",
       "8       region    2.371146"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_vif(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa28f36b",
   "metadata": {},
   "source": [
    "## removal of outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "94f6fa07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AveragePrice       AxesSubplot(0.125,0.536818;0.0945122x0.343182)\n",
       "4046            AxesSubplot(0.238415,0.536818;0.0945122x0.343182)\n",
       "4225            AxesSubplot(0.351829,0.536818;0.0945122x0.343182)\n",
       "4770            AxesSubplot(0.465244,0.536818;0.0945122x0.343182)\n",
       "Small Bags      AxesSubplot(0.578659,0.536818;0.0945122x0.343182)\n",
       "Large Bags      AxesSubplot(0.692073,0.536818;0.0945122x0.343182)\n",
       "XLarge Bags     AxesSubplot(0.805488,0.536818;0.0945122x0.343182)\n",
       "type                  AxesSubplot(0.125,0.125;0.0945122x0.343182)\n",
       "year               AxesSubplot(0.238415,0.125;0.0945122x0.343182)\n",
       "region             AxesSubplot(0.351829,0.125;0.0945122x0.343182)\n",
       "dtype: object"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x1008 with 14 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dnew.plot(kind='box',subplots=True,layout=(2,7),figsize=(12,14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "b5105fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AveragePrice    float64\n",
       "4046            float64\n",
       "4225            float64\n",
       "4770            float64\n",
       "Small Bags      float64\n",
       "Large Bags      float64\n",
       "XLarge Bags     float64\n",
       "type              int32\n",
       "year              int64\n",
       "region            int32\n",
       "dtype: object"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnew.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f4c0ab0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "z=np.abs(zscore(dnew))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dd4fb324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  346,   359,   780, ..., 17304, 17402, 17428], dtype=int64),\n",
       " array([1, 1, 6, ..., 0, 0, 0], dtype=int64))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(z>3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3ca533c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnew=dnew[(z<3).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cd7be155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17651, 10)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfnew.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f6ecad4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18249, 12)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680221be",
   "metadata": {},
   "source": [
    "percentage of data loss= (18249-17651)/18249*100=3.2%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44526245",
   "metadata": {},
   "source": [
    "# checking the correlation with the heatmap once more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "17e590f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "sns.heatmap(dfnew.corr(),annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7b0dc4",
   "metadata": {},
   "source": [
    "from the heatmap it can be seen that the multicollinearity in the dataframe has been eliminated to improve the overall accuracy of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44436476",
   "metadata": {},
   "source": [
    "# skewness removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6ad75279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type           -0.037741\n",
       "region          0.012798\n",
       "year            0.229976\n",
       "AveragePrice    0.377432\n",
       "Small Bags      4.222706\n",
       "4225            4.455745\n",
       "4046            4.909848\n",
       "Large Bags      5.053434\n",
       "4770            5.117170\n",
       "XLarge Bags     6.135607\n",
       "dtype: float64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfnew.skew().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a73bd7",
   "metadata": {},
   "source": [
    "observation: it can be seen that high skewness is there for columns like Small Bags, 4225, 4046, 4770, XLarge bags and Large Bags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5e8b650c",
   "metadata": {},
   "outputs": [],
   "source": [
    "list=['type','region','year','Small Bags', '4225', '4046', '4770', 'XLarge Bags','Large Bags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9aa4bcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list:\n",
    "    if dfnew.skew().loc[i]>0.5:\n",
    "        dfnew[i]=np.log1p(dfnew[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7a4695dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Small Bags     -0.788988\n",
       "Large Bags     -0.610874\n",
       "4225           -0.604619\n",
       "4046           -0.420273\n",
       "type           -0.037741\n",
       "region          0.012798\n",
       "4770            0.068532\n",
       "year            0.229976\n",
       "AveragePrice    0.377432\n",
       "XLarge Bags     1.216238\n",
       "dtype: float64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfnew.skew().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89604d8e",
   "metadata": {},
   "source": [
    "it can be seen that skewness in the dataset has been removed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c804ee",
   "metadata": {},
   "source": [
    "# scaling of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0e10a3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bb1b111d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=dfnew.drop([\"AveragePrice\"],axis=1)\n",
    "y=dfnew[\"AveragePrice\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7c86c6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx=sc.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "77a9b9bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.5744216127984896e-17"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfx.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce9568d",
   "metadata": {},
   "source": [
    "# application of machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cb9fd564",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5490208f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_selection(instance):\n",
    "    maxacc=0\n",
    "    rs=0\n",
    "    for i in range(0,100):\n",
    "        x_train,x_test,y_train,y_test=train_test_split(dfx,y,random_state=i,test_size=0.30)\n",
    "        instance.fit(x_train,y_train)\n",
    "        pred_train=instance.predict(x_train)\n",
    "        pred_test=instance.predict(x_test)\n",
    "\n",
    "        print(f\"at random state {i}, training accuracy is {r2_score(y_train,pred_train)}\")\n",
    "        print(f\"at random state {i}, testing accuracy is {r2_score(y_test,pred_test)}\")\n",
    "        print(f\"at random state {i}, mean squared error is {mean_squared_error(y_test,pred_test)}\")\n",
    "        print(f\"at random state {i}, mean absolute error is {mean_absolute_error(y_test,pred_test)}\")\n",
    "        print(\"\\n\")\n",
    "        if((r2_score(y_test,pred_test))>maxacc):\n",
    "        maxacc=r2_score(y_test,pred_test)\n",
    "        rs=i\n",
    "    print(\"Max accuracy at random state\",rs, \"=\",maxacc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbb74d6",
   "metadata": {},
   "source": [
    "## testing with linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "d46ea154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 0, training accuracy is 0.4721954004941026\n",
      "at random state 0, testing accuracy is 0.4650274291535692\n",
      "at random state 0, mean squared error is 0.08017180954377391\n",
      "at random state 0, mean absolute error is 0.2212709231098439\n",
      "\n",
      "\n",
      "at random state 1, training accuracy is 0.47164840687209764\n",
      "at random state 1, testing accuracy is 0.4673221237277062\n",
      "at random state 1, mean squared error is 0.08026074204162453\n",
      "at random state 1, mean absolute error is 0.22219291219605763\n",
      "\n",
      "\n",
      "at random state 2, training accuracy is 0.4737643161523939\n",
      "at random state 2, testing accuracy is 0.45871624614081774\n",
      "at random state 2, mean squared error is 0.08042499950236547\n",
      "at random state 2, mean absolute error is 0.22444873918799885\n",
      "\n",
      "\n",
      "at random state 3, training accuracy is 0.46810492320766617\n",
      "at random state 3, testing accuracy is 0.48144686385239\n",
      "at random state 3, mean squared error is 0.07662690772158996\n",
      "at random state 3, mean absolute error is 0.2190227513807934\n",
      "\n",
      "\n",
      "at random state 4, training accuracy is 0.4707563020658726\n",
      "at random state 4, testing accuracy is 0.47074752492674743\n",
      "at random state 4, mean squared error is 0.0771769435342423\n",
      "at random state 4, mean absolute error is 0.22077239196221574\n",
      "\n",
      "\n",
      "at random state 5, training accuracy is 0.46660724589393887\n",
      "at random state 5, testing accuracy is 0.4864925419793422\n",
      "at random state 5, mean squared error is 0.07748927860725126\n",
      "at random state 5, mean absolute error is 0.21958545048635403\n",
      "\n",
      "\n",
      "at random state 6, training accuracy is 0.4652885926793191\n",
      "at random state 6, testing accuracy is 0.49206752064103887\n",
      "at random state 6, mean squared error is 0.07667492196157939\n",
      "at random state 6, mean absolute error is 0.21799685407545785\n",
      "\n",
      "\n",
      "at random state 7, training accuracy is 0.46857901595165163\n",
      "at random state 7, testing accuracy is 0.4789353555976096\n",
      "at random state 7, mean squared error is 0.07850451194975633\n",
      "at random state 7, mean absolute error is 0.21931100836551268\n",
      "\n",
      "\n",
      "at random state 8, training accuracy is 0.46741369055668935\n",
      "at random state 8, testing accuracy is 0.4831251401833023\n",
      "at random state 8, mean squared error is 0.07925798023947461\n",
      "at random state 8, mean absolute error is 0.2213020303870955\n",
      "\n",
      "\n",
      "at random state 9, training accuracy is 0.4691106529316347\n",
      "at random state 9, testing accuracy is 0.4772032129226248\n",
      "at random state 9, mean squared error is 0.07832732329780638\n",
      "at random state 9, mean absolute error is 0.22046642069495617\n",
      "\n",
      "\n",
      "at random state 10, training accuracy is 0.4762652005965301\n",
      "at random state 10, testing accuracy is 0.4482999532136187\n",
      "at random state 10, mean squared error is 0.08073847725845248\n",
      "at random state 10, mean absolute error is 0.2235439047984961\n",
      "\n",
      "\n",
      "at random state 11, training accuracy is 0.4686091596552924\n",
      "at random state 11, testing accuracy is 0.4793618163829009\n",
      "at random state 11, mean squared error is 0.0783493464367802\n",
      "at random state 11, mean absolute error is 0.2201889740666082\n",
      "\n",
      "\n",
      "at random state 12, training accuracy is 0.4725131104259652\n",
      "at random state 12, testing accuracy is 0.4638649741730927\n",
      "at random state 12, mean squared error is 0.0795656737453826\n",
      "at random state 12, mean absolute error is 0.22035037871559573\n",
      "\n",
      "\n",
      "at random state 13, training accuracy is 0.47144264980543393\n",
      "at random state 13, testing accuracy is 0.4679680337146782\n",
      "at random state 13, mean squared error is 0.07825306059521837\n",
      "at random state 13, mean absolute error is 0.21839503669372712\n",
      "\n",
      "\n",
      "at random state 14, training accuracy is 0.46747508756854905\n",
      "at random state 14, testing accuracy is 0.4843070543473432\n",
      "at random state 14, mean squared error is 0.07548725315167776\n",
      "at random state 14, mean absolute error is 0.2162391066456283\n",
      "\n",
      "\n",
      "at random state 15, training accuracy is 0.46980583708667234\n",
      "at random state 15, testing accuracy is 0.4745701052578155\n",
      "at random state 15, mean squared error is 0.07671023586033231\n",
      "at random state 15, mean absolute error is 0.21853057693994254\n",
      "\n",
      "\n",
      "at random state 16, training accuracy is 0.46952593631946116\n",
      "at random state 16, testing accuracy is 0.4757897813251222\n",
      "at random state 16, mean squared error is 0.0767196304729782\n",
      "at random state 16, mean absolute error is 0.21640495328623752\n",
      "\n",
      "\n",
      "at random state 17, training accuracy is 0.46961680919494164\n",
      "at random state 17, testing accuracy is 0.47547898423433876\n",
      "at random state 17, mean squared error is 0.0755343676495378\n",
      "at random state 17, mean absolute error is 0.21619741049437743\n",
      "\n",
      "\n",
      "at random state 18, training accuracy is 0.4701622367482796\n",
      "at random state 18, testing accuracy is 0.4727729816849888\n",
      "at random state 18, mean squared error is 0.07842391928098932\n",
      "at random state 18, mean absolute error is 0.21997583387354816\n",
      "\n",
      "\n",
      "at random state 19, training accuracy is 0.4668301668771476\n",
      "at random state 19, testing accuracy is 0.4868327922288943\n",
      "at random state 19, mean squared error is 0.07408765609403185\n",
      "at random state 19, mean absolute error is 0.216595058700637\n",
      "\n",
      "\n",
      "at random state 20, training accuracy is 0.4694129743789882\n",
      "at random state 20, testing accuracy is 0.47618691534908353\n",
      "at random state 20, mean squared error is 0.07824416116641378\n",
      "at random state 20, mean absolute error is 0.2200392264321266\n",
      "\n",
      "\n",
      "at random state 21, training accuracy is 0.4753972454685296\n",
      "at random state 21, testing accuracy is 0.45217090763301904\n",
      "at random state 21, mean squared error is 0.08092475661937285\n",
      "at random state 21, mean absolute error is 0.2245745442294751\n",
      "\n",
      "\n",
      "at random state 22, training accuracy is 0.47500216373911897\n",
      "at random state 22, testing accuracy is 0.45346027340215256\n",
      "at random state 22, mean squared error is 0.0798653744423932\n",
      "at random state 22, mean absolute error is 0.22137912393570577\n",
      "\n",
      "\n",
      "at random state 23, training accuracy is 0.4696897077524629\n",
      "at random state 23, testing accuracy is 0.4745647361628653\n",
      "at random state 23, mean squared error is 0.07765136803541145\n",
      "at random state 23, mean absolute error is 0.22048484843648208\n",
      "\n",
      "\n",
      "at random state 24, training accuracy is 0.47230368263113887\n",
      "at random state 24, testing accuracy is 0.46441070118060257\n",
      "at random state 24, mean squared error is 0.07800249771381776\n",
      "at random state 24, mean absolute error is 0.21814955199821406\n",
      "\n",
      "\n",
      "at random state 25, training accuracy is 0.467728906661556\n",
      "at random state 25, testing accuracy is 0.4831224693941738\n",
      "at random state 25, mean squared error is 0.07556806019195195\n",
      "at random state 25, mean absolute error is 0.21596623744785273\n",
      "\n",
      "\n",
      "at random state 26, training accuracy is 0.4700409136252982\n",
      "at random state 26, testing accuracy is 0.47350583775504085\n",
      "at random state 26, mean squared error is 0.08125499735323286\n",
      "at random state 26, mean absolute error is 0.22334540728088587\n",
      "\n",
      "\n",
      "at random state 27, training accuracy is 0.47031521761023043\n",
      "at random state 27, testing accuracy is 0.4721317070572705\n",
      "at random state 27, mean squared error is 0.07878898735695924\n",
      "at random state 27, mean absolute error is 0.22135422107582997\n",
      "\n",
      "\n",
      "at random state 28, training accuracy is 0.4660760254516325\n",
      "at random state 28, testing accuracy is 0.4897783977822301\n",
      "at random state 28, mean squared error is 0.07438853146650215\n",
      "at random state 28, mean absolute error is 0.2158691129959697\n",
      "\n",
      "\n",
      "at random state 29, training accuracy is 0.4689712428083974\n",
      "at random state 29, testing accuracy is 0.4776105637895659\n",
      "at random state 29, mean squared error is 0.07975232807424874\n",
      "at random state 29, mean absolute error is 0.22250613885182285\n",
      "\n",
      "\n",
      "at random state 30, training accuracy is 0.4738982471429787\n",
      "at random state 30, testing accuracy is 0.45756689416426466\n",
      "at random state 30, mean squared error is 0.07844916374316323\n",
      "at random state 30, mean absolute error is 0.22023727117808564\n",
      "\n",
      "\n",
      "at random state 31, training accuracy is 0.46604254001904344\n",
      "at random state 31, testing accuracy is 0.4894459893767663\n",
      "at random state 31, mean squared error is 0.07655397184902375\n",
      "at random state 31, mean absolute error is 0.21771065963724054\n",
      "\n",
      "\n",
      "at random state 32, training accuracy is 0.47154397928905867\n",
      "at random state 32, testing accuracy is 0.4677510469546792\n",
      "at random state 32, mean squared error is 0.0786893065061957\n",
      "at random state 32, mean absolute error is 0.22255498471747476\n",
      "\n",
      "\n",
      "at random state 33, training accuracy is 0.47225950490398905\n",
      "at random state 33, testing accuracy is 0.4642391622547174\n",
      "at random state 33, mean squared error is 0.07708763610565819\n",
      "at random state 33, mean absolute error is 0.21760372358558597\n",
      "\n",
      "\n",
      "at random state 34, training accuracy is 0.4727075203781066\n",
      "at random state 34, testing accuracy is 0.46262413825374227\n",
      "at random state 34, mean squared error is 0.08101593391004662\n",
      "at random state 34, mean absolute error is 0.2220001434201688\n",
      "\n",
      "\n",
      "at random state 35, training accuracy is 0.4680956088785775\n",
      "at random state 35, testing accuracy is 0.4811413215988295\n",
      "at random state 35, mean squared error is 0.07681863349665595\n",
      "at random state 35, mean absolute error is 0.21946204526033175\n",
      "\n",
      "\n",
      "at random state 36, training accuracy is 0.4717977059553973\n",
      "at random state 36, testing accuracy is 0.466274269024076\n",
      "at random state 36, mean squared error is 0.07658892192726612\n",
      "at random state 36, mean absolute error is 0.21839391969940264\n",
      "\n",
      "\n",
      "at random state 37, training accuracy is 0.46929905423499485\n",
      "at random state 37, testing accuracy is 0.476848715411864\n",
      "at random state 37, mean squared error is 0.07786614947692179\n",
      "at random state 37, mean absolute error is 0.21947749671406844\n",
      "\n",
      "\n",
      "at random state 38, training accuracy is 0.46920584239693364\n",
      "at random state 38, testing accuracy is 0.47667320355209564\n",
      "at random state 38, mean squared error is 0.07728091423688561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 38, mean absolute error is 0.21835706740068966\n",
      "\n",
      "\n",
      "at random state 39, training accuracy is 0.47648621926743184\n",
      "at random state 39, testing accuracy is 0.44687023830151495\n",
      "at random state 39, mean squared error is 0.08007688519346379\n",
      "at random state 39, mean absolute error is 0.22203537281628694\n",
      "\n",
      "\n",
      "at random state 40, training accuracy is 0.4702546001099873\n",
      "at random state 40, testing accuracy is 0.47291227597561714\n",
      "at random state 40, mean squared error is 0.07796408392587204\n",
      "at random state 40, mean absolute error is 0.21929630641620765\n",
      "\n",
      "\n",
      "at random state 41, training accuracy is 0.4699231517081828\n",
      "at random state 41, testing accuracy is 0.47402227242184924\n",
      "at random state 41, mean squared error is 0.07760740213923206\n",
      "at random state 41, mean absolute error is 0.2218379123852688\n",
      "\n",
      "\n",
      "at random state 42, training accuracy is 0.47064963501736423\n",
      "at random state 42, testing accuracy is 0.4707260501308673\n",
      "at random state 42, mean squared error is 0.0781820252971664\n",
      "at random state 42, mean absolute error is 0.21841131081224657\n",
      "\n",
      "\n",
      "at random state 43, training accuracy is 0.4679262929323432\n",
      "at random state 43, testing accuracy is 0.48196197054252776\n",
      "at random state 43, mean squared error is 0.07790914298507363\n",
      "at random state 43, mean absolute error is 0.2207664683954994\n",
      "\n",
      "\n",
      "at random state 44, training accuracy is 0.47696042345403766\n",
      "at random state 44, testing accuracy is 0.4458981031220617\n",
      "at random state 44, mean squared error is 0.08353251929434131\n",
      "at random state 44, mean absolute error is 0.22765736193612096\n",
      "\n",
      "\n",
      "at random state 45, training accuracy is 0.4715377977437957\n",
      "at random state 45, testing accuracy is 0.4669918903594211\n",
      "at random state 45, mean squared error is 0.07813668567940957\n",
      "at random state 45, mean absolute error is 0.22082993054449782\n",
      "\n",
      "\n",
      "at random state 46, training accuracy is 0.4679926852471905\n",
      "at random state 46, testing accuracy is 0.48165787324500786\n",
      "at random state 46, mean squared error is 0.07791171031712715\n",
      "at random state 46, mean absolute error is 0.21902024645806653\n",
      "\n",
      "\n",
      "at random state 47, training accuracy is 0.47153045992493403\n",
      "at random state 47, testing accuracy is 0.4675925370905951\n",
      "at random state 47, mean squared error is 0.07739747006979439\n",
      "at random state 47, mean absolute error is 0.21964103833806664\n",
      "\n",
      "\n",
      "at random state 48, training accuracy is 0.47056131809581103\n",
      "at random state 48, testing accuracy is 0.4713754737163425\n",
      "at random state 48, mean squared error is 0.0781380251853562\n",
      "at random state 48, mean absolute error is 0.22118547706130715\n",
      "\n",
      "\n",
      "at random state 49, training accuracy is 0.46914898335760324\n",
      "at random state 49, testing accuracy is 0.47678226836633075\n",
      "at random state 49, mean squared error is 0.07801455912843969\n",
      "at random state 49, mean absolute error is 0.2201398776890198\n",
      "\n",
      "\n",
      "at random state 50, training accuracy is 0.4705670526578193\n",
      "at random state 50, testing accuracy is 0.4715769803324831\n",
      "at random state 50, mean squared error is 0.07691677503424087\n",
      "at random state 50, mean absolute error is 0.2196920469856546\n",
      "\n",
      "\n",
      "at random state 51, training accuracy is 0.4746877587191075\n",
      "at random state 51, testing accuracy is 0.4550768343448496\n",
      "at random state 51, mean squared error is 0.08061766651345478\n",
      "at random state 51, mean absolute error is 0.2226926479115835\n",
      "\n",
      "\n",
      "at random state 52, training accuracy is 0.4732271854311233\n",
      "at random state 52, testing accuracy is 0.4610192164252861\n",
      "at random state 52, mean squared error is 0.08206151842602608\n",
      "at random state 52, mean absolute error is 0.22496908937633311\n",
      "\n",
      "\n",
      "at random state 53, training accuracy is 0.4674614067005506\n",
      "at random state 53, testing accuracy is 0.483828645843845\n",
      "at random state 53, mean squared error is 0.07773369134576855\n",
      "at random state 53, mean absolute error is 0.2199548917968184\n",
      "\n",
      "\n",
      "at random state 54, training accuracy is 0.4762159054885907\n",
      "at random state 54, testing accuracy is 0.44859184260621676\n",
      "at random state 54, mean squared error is 0.08180879066267256\n",
      "at random state 54, mean absolute error is 0.2245103878463849\n",
      "\n",
      "\n",
      "at random state 55, training accuracy is 0.47286861373671263\n",
      "at random state 55, testing accuracy is 0.4610910565192031\n",
      "at random state 55, mean squared error is 0.08250114655461585\n",
      "at random state 55, mean absolute error is 0.22725555544144324\n",
      "\n",
      "\n",
      "at random state 56, training accuracy is 0.4694153263451617\n",
      "at random state 56, testing accuracy is 0.47589268346144054\n",
      "at random state 56, mean squared error is 0.07553454102384746\n",
      "at random state 56, mean absolute error is 0.21746635841188147\n",
      "\n",
      "\n",
      "at random state 57, training accuracy is 0.46805370916469136\n",
      "at random state 57, testing accuracy is 0.48113006722697815\n",
      "at random state 57, mean squared error is 0.07936632060640578\n",
      "at random state 57, mean absolute error is 0.221151618780428\n",
      "\n",
      "\n",
      "at random state 58, training accuracy is 0.4694445785055198\n",
      "at random state 58, testing accuracy is 0.4757178584671795\n",
      "at random state 58, mean squared error is 0.07912562928196099\n",
      "at random state 58, mean absolute error is 0.22294828077618353\n",
      "\n",
      "\n",
      "at random state 59, training accuracy is 0.4737133144836112\n",
      "at random state 59, testing accuracy is 0.45859643511076853\n",
      "at random state 59, mean squared error is 0.07763432073752123\n",
      "at random state 59, mean absolute error is 0.21830884852288485\n",
      "\n",
      "\n",
      "at random state 60, training accuracy is 0.4651358515143116\n",
      "at random state 60, testing accuracy is 0.4921537954114956\n",
      "at random state 60, mean squared error is 0.07819222868238457\n",
      "at random state 60, mean absolute error is 0.2193269330309344\n",
      "\n",
      "\n",
      "at random state 61, training accuracy is 0.47325276302817953\n",
      "at random state 61, testing accuracy is 0.4604950294306548\n",
      "at random state 61, mean squared error is 0.07994864274017513\n",
      "at random state 61, mean absolute error is 0.22267572784548656\n",
      "\n",
      "\n",
      "at random state 62, training accuracy is 0.47713749490608237\n",
      "at random state 62, testing accuracy is 0.4448465137360653\n",
      "at random state 62, mean squared error is 0.08229767134312825\n",
      "at random state 62, mean absolute error is 0.2251171491928757\n",
      "\n",
      "\n",
      "at random state 63, training accuracy is 0.47116081910333985\n",
      "at random state 63, testing accuracy is 0.46931574079421456\n",
      "at random state 63, mean squared error is 0.08176744243026542\n",
      "at random state 63, mean absolute error is 0.225843344489041\n",
      "\n",
      "\n",
      "at random state 64, training accuracy is 0.4703873526341561\n",
      "at random state 64, testing accuracy is 0.47223505259588483\n",
      "at random state 64, mean squared error is 0.07839698732428597\n",
      "at random state 64, mean absolute error is 0.21922103912927293\n",
      "\n",
      "\n",
      "at random state 65, training accuracy is 0.4740619893223562\n",
      "at random state 65, testing accuracy is 0.45762007000624283\n",
      "at random state 65, mean squared error is 0.07998060912655856\n",
      "at random state 65, mean absolute error is 0.22178439654225743\n",
      "\n",
      "\n",
      "at random state 66, training accuracy is 0.47476046498485913\n",
      "at random state 66, testing accuracy is 0.4535157804051688\n",
      "at random state 66, mean squared error is 0.07943824751564542\n",
      "at random state 66, mean absolute error is 0.2197631885757448\n",
      "\n",
      "\n",
      "at random state 67, training accuracy is 0.4723780868427774\n",
      "at random state 67, testing accuracy is 0.4644281040597519\n",
      "at random state 67, mean squared error is 0.07996088751323402\n",
      "at random state 67, mean absolute error is 0.22068721375991535\n",
      "\n",
      "\n",
      "at random state 68, training accuracy is 0.4717084997999065\n",
      "at random state 68, testing accuracy is 0.4670250747727426\n",
      "at random state 68, mean squared error is 0.0810802184883736\n",
      "at random state 68, mean absolute error is 0.22436279287879182\n",
      "\n",
      "\n",
      "at random state 69, training accuracy is 0.46768388059220833\n",
      "at random state 69, testing accuracy is 0.4825433667110107\n",
      "at random state 69, mean squared error is 0.07866806293659838\n",
      "at random state 69, mean absolute error is 0.22218544819938163\n",
      "\n",
      "\n",
      "at random state 70, training accuracy is 0.4714487501519914\n",
      "at random state 70, testing accuracy is 0.4676254674864885\n",
      "at random state 70, mean squared error is 0.08216634241468555\n",
      "at random state 70, mean absolute error is 0.22493046980643247\n",
      "\n",
      "\n",
      "at random state 71, training accuracy is 0.4713269886309859\n",
      "at random state 71, testing accuracy is 0.46864173163153944\n",
      "at random state 71, mean squared error is 0.07930809080172697\n",
      "at random state 71, mean absolute error is 0.2223425692144361\n",
      "\n",
      "\n",
      "at random state 72, training accuracy is 0.4699697679552628\n",
      "at random state 72, testing accuracy is 0.473171834541538\n",
      "at random state 72, mean squared error is 0.08067092087875713\n",
      "at random state 72, mean absolute error is 0.2217623563835012\n",
      "\n",
      "\n",
      "at random state 73, training accuracy is 0.4734318109911694\n",
      "at random state 73, testing accuracy is 0.4601609579985263\n",
      "at random state 73, mean squared error is 0.0803383978000148\n",
      "at random state 73, mean absolute error is 0.22262453511669184\n",
      "\n",
      "\n",
      "at random state 74, training accuracy is 0.4689054175883607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 74, testing accuracy is 0.47840195894615156\n",
      "at random state 74, mean squared error is 0.07603335762400566\n",
      "at random state 74, mean absolute error is 0.21781751654376103\n",
      "\n",
      "\n",
      "at random state 75, training accuracy is 0.4703304989820062\n",
      "at random state 75, testing accuracy is 0.47249766321952635\n",
      "at random state 75, mean squared error is 0.07890686727592873\n",
      "at random state 75, mean absolute error is 0.22013417151312664\n",
      "\n",
      "\n",
      "at random state 76, training accuracy is 0.46960458612592926\n",
      "at random state 76, testing accuracy is 0.47547546891858417\n",
      "at random state 76, mean squared error is 0.0752937417666969\n",
      "at random state 76, mean absolute error is 0.2165256936330698\n",
      "\n",
      "\n",
      "at random state 77, training accuracy is 0.4723899192717471\n",
      "at random state 77, testing accuracy is 0.46417297148479086\n",
      "at random state 77, mean squared error is 0.07893621226197782\n",
      "at random state 77, mean absolute error is 0.22004402403805468\n",
      "\n",
      "\n",
      "at random state 78, training accuracy is 0.47093089776006447\n",
      "at random state 78, testing accuracy is 0.4701988474704979\n",
      "at random state 78, mean squared error is 0.07997340835589793\n",
      "at random state 78, mean absolute error is 0.22063505961918492\n",
      "\n",
      "\n",
      "at random state 79, training accuracy is 0.46850640444527636\n",
      "at random state 79, testing accuracy is 0.47986535936184616\n",
      "at random state 79, mean squared error is 0.07688315210775153\n",
      "at random state 79, mean absolute error is 0.21946622491813847\n",
      "\n",
      "\n",
      "at random state 80, training accuracy is 0.4707365361232757\n",
      "at random state 80, testing accuracy is 0.47097174848631884\n",
      "at random state 80, mean squared error is 0.07473617982563464\n",
      "at random state 80, mean absolute error is 0.21431257770984033\n",
      "\n",
      "\n",
      "at random state 81, training accuracy is 0.47086084505105363\n",
      "at random state 81, testing accuracy is 0.47037545226823885\n",
      "at random state 81, mean squared error is 0.07940597984248975\n",
      "at random state 81, mean absolute error is 0.22304973249380083\n",
      "\n",
      "\n",
      "at random state 82, training accuracy is 0.46609858330952314\n",
      "at random state 82, testing accuracy is 0.48909628970491636\n",
      "at random state 82, mean squared error is 0.07656005401694616\n",
      "at random state 82, mean absolute error is 0.21725470210007541\n",
      "\n",
      "\n",
      "at random state 83, training accuracy is 0.4747138111944962\n",
      "at random state 83, testing accuracy is 0.4546533301456498\n",
      "at random state 83, mean squared error is 0.08038889582278995\n",
      "at random state 83, mean absolute error is 0.22315438658818418\n",
      "\n",
      "\n",
      "at random state 84, training accuracy is 0.4688653517615865\n",
      "at random state 84, testing accuracy is 0.478452708629976\n",
      "at random state 84, mean squared error is 0.07529879812851654\n",
      "at random state 84, mean absolute error is 0.21602059170761753\n",
      "\n",
      "\n",
      "at random state 85, training accuracy is 0.4652308293220443\n",
      "at random state 85, testing accuracy is 0.4905941147195998\n",
      "at random state 85, mean squared error is 0.08151733243838302\n",
      "at random state 85, mean absolute error is 0.2248079839011236\n",
      "\n",
      "\n",
      "at random state 86, training accuracy is 0.4712698260328203\n",
      "at random state 86, testing accuracy is 0.46891640452679684\n",
      "at random state 86, mean squared error is 0.08193763312345596\n",
      "at random state 86, mean absolute error is 0.22490399365200903\n",
      "\n",
      "\n",
      "at random state 87, training accuracy is 0.4716620146869619\n",
      "at random state 87, testing accuracy is 0.46720466351173984\n",
      "at random state 87, mean squared error is 0.08034985948331204\n",
      "at random state 87, mean absolute error is 0.22229000687177408\n",
      "\n",
      "\n",
      "at random state 88, training accuracy is 0.47191183699033445\n",
      "at random state 88, testing accuracy is 0.46614983968968215\n",
      "at random state 88, mean squared error is 0.07871672769018505\n",
      "at random state 88, mean absolute error is 0.22110868013998583\n",
      "\n",
      "\n",
      "at random state 89, training accuracy is 0.4704818959208984\n",
      "at random state 89, testing accuracy is 0.47201855072421206\n",
      "at random state 89, mean squared error is 0.07766737294122084\n",
      "at random state 89, mean absolute error is 0.22004452014552053\n",
      "\n",
      "\n",
      "at random state 90, training accuracy is 0.47785906999139716\n",
      "at random state 90, testing accuracy is 0.441170627074405\n",
      "at random state 90, mean squared error is 0.08151057811594042\n",
      "at random state 90, mean absolute error is 0.22398387519784002\n",
      "\n",
      "\n",
      "at random state 91, training accuracy is 0.4717485216738233\n",
      "at random state 91, testing accuracy is 0.4663507329109823\n",
      "at random state 91, mean squared error is 0.07982250720468281\n",
      "at random state 91, mean absolute error is 0.22225178044829033\n",
      "\n",
      "\n",
      "at random state 92, training accuracy is 0.4726527811165273\n",
      "at random state 92, testing accuracy is 0.4630118628675445\n",
      "at random state 92, mean squared error is 0.07824097601328084\n",
      "at random state 92, mean absolute error is 0.21979967134515166\n",
      "\n",
      "\n",
      "at random state 93, training accuracy is 0.47182162607306344\n",
      "at random state 93, testing accuracy is 0.466596750010441\n",
      "at random state 93, mean squared error is 0.07993166147745641\n",
      "at random state 93, mean absolute error is 0.22356469299437498\n",
      "\n",
      "\n",
      "at random state 94, training accuracy is 0.4761834723158953\n",
      "at random state 94, testing accuracy is 0.44885002385408157\n",
      "at random state 94, mean squared error is 0.08157862177905138\n",
      "at random state 94, mean absolute error is 0.22372697372025474\n",
      "\n",
      "\n",
      "at random state 95, training accuracy is 0.4690190269875032\n",
      "at random state 95, testing accuracy is 0.47767624359007144\n",
      "at random state 95, mean squared error is 0.07908282110527444\n",
      "at random state 95, mean absolute error is 0.22123622848886387\n",
      "\n",
      "\n",
      "at random state 96, training accuracy is 0.46892472208660174\n",
      "at random state 96, testing accuracy is 0.47796791700059216\n",
      "at random state 96, mean squared error is 0.0799024077393612\n",
      "at random state 96, mean absolute error is 0.221300647966335\n",
      "\n",
      "\n",
      "at random state 97, training accuracy is 0.47498250065141523\n",
      "at random state 97, testing accuracy is 0.4537590087669787\n",
      "at random state 97, mean squared error is 0.08114231612757192\n",
      "at random state 97, mean absolute error is 0.22442542531388165\n",
      "\n",
      "\n",
      "at random state 98, training accuracy is 0.4705648704979568\n",
      "at random state 98, testing accuracy is 0.47158153930655644\n",
      "at random state 98, mean squared error is 0.07714454114276327\n",
      "at random state 98, mean absolute error is 0.21596253330538975\n",
      "\n",
      "\n",
      "at random state 99, training accuracy is 0.46970040445980243\n",
      "at random state 99, testing accuracy is 0.47475527005324125\n",
      "at random state 99, mean squared error is 0.07646992812886036\n",
      "at random state 99, mean absolute error is 0.21540502300407954\n",
      "\n",
      "\n",
      "Max accuracy at random state 60 = 0.4921537954114956\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lm=LinearRegression()\n",
    "model_selection(lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b66d8e",
   "metadata": {},
   "source": [
    "CV score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "f5b8ed5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.46368434  0.07534713 -0.15508942  0.44871333  0.12614908  0.11790035\n",
      "  0.08815421]\n",
      "0.033927190745051\n",
      "0.2606597691806302\n"
     ]
    }
   ],
   "source": [
    "score=cross_val_score(lm,dfx,y,cv=7)\n",
    "print(score)\n",
    "print(score.mean())\n",
    "print(score.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821173d8",
   "metadata": {},
   "source": [
    "## KNeighbors Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2c639059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 0, training accuracy is 0.8711242533973079\n",
      "at random state 0, testing accuracy is 0.7932982885824444\n",
      "at random state 0, mean squared error is 0.03071467824773414\n",
      "at random state 0, mean absolute error is 0.12327945619335348\n",
      "\n",
      "\n",
      "at random state 1, training accuracy is 0.8724947162101151\n",
      "at random state 1, testing accuracy is 0.7946524077812535\n",
      "at random state 1, mean squared error is 0.030914116314199394\n",
      "at random state 1, mean absolute error is 0.12400528700906344\n",
      "\n",
      "\n",
      "at random state 2, training accuracy is 0.8750221986144251\n",
      "at random state 2, testing accuracy is 0.7951591536289885\n",
      "at random state 2, mean squared error is 0.030718841389728097\n",
      "at random state 2, mean absolute error is 0.12440936555891238\n",
      "\n",
      "\n",
      "at random state 3, training accuracy is 0.8715352098312765\n",
      "at random state 3, testing accuracy is 0.8042910274952633\n",
      "at random state 3, mean squared error is 0.02945454833836858\n",
      "at random state 3, mean absolute error is 0.12248716012084593\n",
      "\n",
      "\n",
      "at random state 4, training accuracy is 0.8732634307901657\n",
      "at random state 4, testing accuracy is 0.7913686418891417\n",
      "at random state 4, mean squared error is 0.030646854229607254\n",
      "at random state 4, mean absolute error is 0.12474509063444109\n",
      "\n",
      "\n",
      "at random state 5, training accuracy is 0.8693945921828816\n",
      "at random state 5, testing accuracy is 0.7960043880599919\n",
      "at random state 5, mean squared error is 0.030359512084592144\n",
      "at random state 5, mean absolute error is 0.12293429003021147\n",
      "\n",
      "\n",
      "at random state 6, training accuracy is 0.8705178674955458\n",
      "at random state 6, testing accuracy is 0.8083798868886534\n",
      "at random state 6, mean squared error is 0.028638707703927496\n",
      "at random state 6, mean absolute error is 0.11920883685800605\n",
      "\n",
      "\n",
      "at random state 7, training accuracy is 0.8751835042560802\n",
      "at random state 7, testing accuracy is 0.7805464518431484\n",
      "at random state 7, mean squared error is 0.032700539274924466\n",
      "at random state 7, mean absolute error is 0.12633157099697886\n",
      "\n",
      "\n",
      "at random state 8, training accuracy is 0.8706592327752967\n",
      "at random state 8, testing accuracy is 0.8024624362624274\n",
      "at random state 8, mean squared error is 0.030090468277945624\n",
      "at random state 8, mean absolute error is 0.12286404833836857\n",
      "\n",
      "\n",
      "at random state 9, training accuracy is 0.8700617117458604\n",
      "at random state 9, testing accuracy is 0.7926796826781066\n",
      "at random state 9, mean squared error is 0.030355249999999993\n",
      "at random state 9, mean absolute error is 0.1235026435045317\n",
      "\n",
      "\n",
      "at random state 10, training accuracy is 0.8726360645342366\n",
      "at random state 10, testing accuracy is 0.7772925267452736\n",
      "at random state 10, mean squared error is 0.032158399546827796\n",
      "at random state 10, mean absolute error is 0.1258024924471299\n",
      "\n",
      "\n",
      "at random state 11, training accuracy is 0.8718166126762465\n",
      "at random state 11, testing accuracy is 0.8028140204208927\n",
      "at random state 11, mean squared error is 0.029687596676737156\n",
      "at random state 11, mean absolute error is 0.12190483383685802\n",
      "\n",
      "\n",
      "at random state 12, training accuracy is 0.8726576376313955\n",
      "at random state 12, testing accuracy is 0.8007526277248435\n",
      "at random state 12, mean squared error is 0.029302461480362536\n",
      "at random state 12, mean absolute error is 0.12127454682779455\n",
      "\n",
      "\n",
      "at random state 13, training accuracy is 0.8705985951483044\n",
      "at random state 13, testing accuracy is 0.798034287540266\n",
      "at random state 13, mean squared error is 0.030518503021148037\n",
      "at random state 13, mean absolute error is 0.12315332326283988\n",
      "\n",
      "\n",
      "at random state 14, training accuracy is 0.8731088737934982\n",
      "at random state 14, testing accuracy is 0.7940791117773989\n",
      "at random state 14, mean squared error is 0.030088423716012086\n",
      "at random state 14, mean absolute error is 0.12238708459214502\n",
      "\n",
      "\n",
      "at random state 15, training accuracy is 0.8723355197380771\n",
      "at random state 15, testing accuracy is 0.7949160164422951\n",
      "at random state 15, mean squared error is 0.02970543655589124\n",
      "at random state 15, mean absolute error is 0.12212009063444107\n",
      "\n",
      "\n",
      "at random state 16, training accuracy is 0.8726247007166473\n",
      "at random state 16, testing accuracy is 0.7952168941616481\n",
      "at random state 16, mean squared error is 0.029920822507552867\n",
      "at random state 16, mean absolute error is 0.1215169939577039\n",
      "\n",
      "\n",
      "at random state 17, training accuracy is 0.8734174221684977\n",
      "at random state 17, testing accuracy is 0.8003004731186665\n",
      "at random state 17, mean squared error is 0.029039937311178245\n",
      "at random state 17, mean absolute error is 0.12181533232628398\n",
      "\n",
      "\n",
      "at random state 18, training accuracy is 0.8709309353467465\n",
      "at random state 18, testing accuracy is 0.8021850186816307\n",
      "at random state 18, mean squared error is 0.02960806193353474\n",
      "at random state 18, mean absolute error is 0.1217998489425982\n",
      "\n",
      "\n",
      "at random state 19, training accuracy is 0.8733040742999425\n",
      "at random state 19, testing accuracy is 0.7940101878735726\n",
      "at random state 19, mean squared error is 0.030255780211480364\n",
      "at random state 19, mean absolute error is 0.12328058912386707\n",
      "\n",
      "\n",
      "at random state 20, training accuracy is 0.8707807175356206\n",
      "at random state 20, testing accuracy is 0.8045747357597656\n",
      "at random state 20, mean squared error is 0.02937150679758308\n",
      "at random state 20, mean absolute error is 0.12143617824773413\n",
      "\n",
      "\n",
      "at random state 21, training accuracy is 0.8689913207703842\n",
      "at random state 21, testing accuracy is 0.8022641257595264\n",
      "at random state 21, mean squared error is 0.02977832250755287\n",
      "at random state 21, mean absolute error is 0.12350793051359515\n",
      "\n",
      "\n",
      "at random state 22, training accuracy is 0.8725540010963323\n",
      "at random state 22, testing accuracy is 0.7904077782948062\n",
      "at random state 22, mean squared error is 0.03094878398791541\n",
      "at random state 22, mean absolute error is 0.1240868580060423\n",
      "\n",
      "\n",
      "at random state 23, training accuracy is 0.8715558051853446\n",
      "at random state 23, testing accuracy is 0.8031611044504661\n",
      "at random state 23, mean squared error is 0.029615514350453172\n",
      "at random state 23, mean absolute error is 0.12038028700906345\n",
      "\n",
      "\n",
      "at random state 24, training accuracy is 0.8715643507258747\n",
      "at random state 24, testing accuracy is 0.7879520989315263\n",
      "at random state 24, mean squared error is 0.030665341389728092\n",
      "at random state 24, mean absolute error is 0.1222228096676737\n",
      "\n",
      "\n",
      "at random state 25, training accuracy is 0.8706293060050093\n",
      "at random state 25, testing accuracy is 0.8021331012018963\n",
      "at random state 25, mean squared error is 0.029717595166163138\n",
      "at random state 25, mean absolute error is 0.1228844410876133\n",
      "\n",
      "\n",
      "at random state 26, training accuracy is 0.869598908600387\n",
      "at random state 26, testing accuracy is 0.8032557867060521\n",
      "at random state 26, mean squared error is 0.03017179078549849\n",
      "at random state 26, mean absolute error is 0.12301623867069487\n",
      "\n",
      "\n",
      "at random state 27, training accuracy is 0.8740650941766559\n",
      "at random state 27, testing accuracy is 0.7902295507776185\n",
      "at random state 27, mean squared error is 0.030960978851963743\n",
      "at random state 27, mean absolute error is 0.12429909365558912\n",
      "\n",
      "\n",
      "at random state 28, training accuracy is 0.8695432843449924\n",
      "at random state 28, testing accuracy is 0.8079830180105473\n",
      "at random state 28, mean squared error is 0.02861313897280967\n",
      "at random state 28, mean absolute error is 0.12005060422960726\n",
      "\n",
      "\n",
      "at random state 29, training accuracy is 0.8696669004911837\n",
      "at random state 29, testing accuracy is 0.7995465970990627\n",
      "at random state 29, mean squared error is 0.030465339879154077\n",
      "at random state 29, mean absolute error is 0.12364803625377642\n",
      "\n",
      "\n",
      "at random state 30, training accuracy is 0.8743971890111116\n",
      "at random state 30, testing accuracy is 0.7955693979663372\n",
      "at random state 30, mean squared error is 0.029798689577039277\n",
      "at random state 30, mean absolute error is 0.12215672205438066\n",
      "\n",
      "\n",
      "at random state 31, training accuracy is 0.871773031303915\n",
      "at random state 31, testing accuracy is 0.8012162040135868\n",
      "at random state 31, mean squared error is 0.030031967522658607\n",
      "at random state 31, mean absolute error is 0.12271034743202418\n",
      "\n",
      "\n",
      "at random state 32, training accuracy is 0.872729652638776\n",
      "at random state 32, testing accuracy is 0.7967925099774131\n",
      "at random state 32, mean squared error is 0.030480518126888217\n",
      "at random state 32, mean absolute error is 0.12507477341389728\n",
      "\n",
      "\n",
      "at random state 33, training accuracy is 0.8733231756809356\n",
      "at random state 33, testing accuracy is 0.793047606133741\n",
      "at random state 33, mean squared error is 0.029727917673716006\n",
      "at random state 33, mean absolute error is 0.11988028700906343\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 34, training accuracy is 0.8705344782809565\n",
      "at random state 34, testing accuracy is 0.7970619675767416\n",
      "at random state 34, mean squared error is 0.030491691842900303\n",
      "at random state 34, mean absolute error is 0.12183383685800606\n",
      "\n",
      "\n",
      "at random state 35, training accuracy is 0.8707839307220322\n",
      "at random state 35, testing accuracy is 0.7996576161198664\n",
      "at random state 35, mean squared error is 0.029566263595166162\n",
      "at random state 35, mean absolute error is 0.12158723564954683\n",
      "\n",
      "\n",
      "at random state 36, training accuracy is 0.8728489440386258\n",
      "at random state 36, testing accuracy is 0.7980445834700525\n",
      "at random state 36, mean squared error is 0.02977484969788519\n",
      "at random state 36, mean absolute error is 0.12192182779456195\n",
      "\n",
      "\n",
      "at random state 37, training accuracy is 0.8706761043750996\n",
      "at random state 37, testing accuracy is 0.802852521779321\n",
      "at random state 37, mean squared error is 0.029629265861027185\n",
      "at random state 37, mean absolute error is 0.12039274924471298\n",
      "\n",
      "\n",
      "at random state 38, training accuracy is 0.869145691888819\n",
      "at random state 38, testing accuracy is 0.7963991453721162\n",
      "at random state 38, mean squared error is 0.030534240181268883\n",
      "at random state 38, mean absolute error is 0.12203927492447128\n",
      "\n",
      "\n",
      "at random state 39, training accuracy is 0.872867001374911\n",
      "at random state 39, testing accuracy is 0.7900605732439321\n",
      "at random state 39, mean squared error is 0.03071490785498489\n",
      "at random state 39, mean absolute error is 0.12455966767371601\n",
      "\n",
      "\n",
      "at random state 40, training accuracy is 0.8712960262852747\n",
      "at random state 40, testing accuracy is 0.8063275828085783\n",
      "at random state 40, mean squared error is 0.028598050604229607\n",
      "at random state 40, mean absolute error is 0.12071790030211481\n",
      "\n",
      "\n",
      "at random state 41, training accuracy is 0.872207224756439\n",
      "at random state 41, testing accuracy is 0.7926152768840677\n",
      "at random state 41, mean squared error is 0.03062365936555891\n",
      "at random state 41, mean absolute error is 0.1231189577039275\n",
      "\n",
      "\n",
      "at random state 42, training accuracy is 0.8722136906688124\n",
      "at random state 42, testing accuracy is 0.7852550462895647\n",
      "at random state 42, mean squared error is 0.03199856344410876\n",
      "at random state 42, mean absolute error is 0.12530815709969786\n",
      "\n",
      "\n",
      "at random state 43, training accuracy is 0.8717858754190797\n",
      "at random state 43, testing accuracy is 0.7929836534807525\n",
      "at random state 43, mean squared error is 0.031190756797583076\n",
      "at random state 43, mean absolute error is 0.12437311178247736\n",
      "\n",
      "\n",
      "at random state 44, training accuracy is 0.8725662391945475\n",
      "at random state 44, testing accuracy is 0.7996176707173306\n",
      "at random state 44, mean squared error is 0.030010574018126884\n",
      "at random state 44, mean absolute error is 0.12299395770392749\n",
      "\n",
      "\n",
      "at random state 45, training accuracy is 0.8738618672687357\n",
      "at random state 45, testing accuracy is 0.79060633182521\n",
      "at random state 45, mean squared error is 0.031100565709969785\n",
      "at random state 45, mean absolute error is 0.12429267371601209\n",
      "\n",
      "\n",
      "at random state 46, training accuracy is 0.8724821927772605\n",
      "at random state 46, testing accuracy is 0.795556294369149\n",
      "at random state 46, mean squared error is 0.030835403323262834\n",
      "at random state 46, mean absolute error is 0.12346827794561932\n",
      "\n",
      "\n",
      "at random state 47, training accuracy is 0.8735971560409153\n",
      "at random state 47, testing accuracy is 0.794892626456165\n",
      "at random state 47, mean squared error is 0.030200826283987917\n",
      "at random state 47, mean absolute error is 0.1230166163141994\n",
      "\n",
      "\n",
      "at random state 48, training accuracy is 0.8719845739437965\n",
      "at random state 48, testing accuracy is 0.7962429538690081\n",
      "at random state 48, mean squared error is 0.03057699924471299\n",
      "at random state 48, mean absolute error is 0.1227428247734139\n",
      "\n",
      "\n",
      "at random state 49, training accuracy is 0.8742617777309905\n",
      "at random state 49, testing accuracy is 0.7882571036675189\n",
      "at random state 49, mean squared error is 0.03172139274924471\n",
      "at random state 49, mean absolute error is 0.12317598187311177\n",
      "\n",
      "\n",
      "at random state 50, training accuracy is 0.8699142180832332\n",
      "at random state 50, testing accuracy is 0.7972284971535557\n",
      "at random state 50, mean squared error is 0.029928460725075528\n",
      "at random state 50, mean absolute error is 0.1220891238670695\n",
      "\n",
      "\n",
      "at random state 51, training accuracy is 0.8720508479520961\n",
      "at random state 51, testing accuracy is 0.7875200200463441\n",
      "at random state 51, mean squared error is 0.03206484516616314\n",
      "at random state 51, mean absolute error is 0.12566729607250757\n",
      "\n",
      "\n",
      "at random state 52, training accuracy is 0.8763849307520225\n",
      "at random state 52, testing accuracy is 0.7838737921331659\n",
      "at random state 52, mean squared error is 0.0326388912386707\n",
      "at random state 52, mean absolute error is 0.12569712990936555\n",
      "\n",
      "\n",
      "at random state 53, training accuracy is 0.871996011021667\n",
      "at random state 53, testing accuracy is 0.7942316196477563\n",
      "at random state 53, mean squared error is 0.030635459969788514\n",
      "at random state 53, mean absolute error is 0.12300868580060421\n",
      "\n",
      "\n",
      "at random state 54, training accuracy is 0.8744273038505799\n",
      "at random state 54, testing accuracy is 0.7945824559199421\n",
      "at random state 54, mean squared error is 0.030522789274924467\n",
      "at random state 54, mean absolute error is 0.12250793051359515\n",
      "\n",
      "\n",
      "at random state 55, training accuracy is 0.8744887990869449\n",
      "at random state 55, testing accuracy is 0.7926710524517186\n",
      "at random state 55, mean squared error is 0.030901940332326284\n",
      "at random state 55, mean absolute error is 0.12471714501510574\n",
      "\n",
      "\n",
      "at random state 56, training accuracy is 0.8721182670480407\n",
      "at random state 56, testing accuracy is 0.7944282141936144\n",
      "at random state 56, mean squared error is 0.03016010347432024\n",
      "at random state 56, mean absolute error is 0.12243919939577039\n",
      "\n",
      "\n",
      "at random state 57, training accuracy is 0.8699276244725903\n",
      "at random state 57, testing accuracy is 0.79630022569965\n",
      "at random state 57, mean squared error is 0.030798047583081564\n",
      "at random state 57, mean absolute error is 0.12356986404833838\n",
      "\n",
      "\n",
      "at random state 58, training accuracy is 0.8689654693694102\n",
      "at random state 58, testing accuracy is 0.7989081372962361\n",
      "at random state 58, mean squared error is 0.029976975830815707\n",
      "at random state 58, mean absolute error is 0.12327190332326284\n",
      "\n",
      "\n",
      "at random state 59, training accuracy is 0.8736169244774603\n",
      "at random state 59, testing accuracy is 0.793017289277339\n",
      "at random state 59, mean squared error is 0.02997159894259819\n",
      "at random state 59, mean absolute error is 0.12193391238670694\n",
      "\n",
      "\n",
      "at random state 60, training accuracy is 0.8698778991655474\n",
      "at random state 60, testing accuracy is 0.8075507822623657\n",
      "at random state 60, mean squared error is 0.029303299848942595\n",
      "at random state 60, mean absolute error is 0.1216476586102719\n",
      "\n",
      "\n",
      "at random state 61, training accuracy is 0.8723833079765972\n",
      "at random state 61, testing accuracy is 0.7864530368764847\n",
      "at random state 61, mean squared error is 0.03233214728096676\n",
      "at random state 61, mean absolute error is 0.1275260574018127\n",
      "\n",
      "\n",
      "at random state 62, training accuracy is 0.8703642896836876\n",
      "at random state 62, testing accuracy is 0.7974954578314226\n",
      "at random state 62, mean squared error is 0.03020710422960725\n",
      "at random state 62, mean absolute error is 0.12394184290030211\n",
      "\n",
      "\n",
      "at random state 63, training accuracy is 0.8719433006335949\n",
      "at random state 63, testing accuracy is 0.8036333114533794\n",
      "at random state 63, mean squared error is 0.02992786329305136\n",
      "at random state 63, mean absolute error is 0.12331835347432026\n",
      "\n",
      "\n",
      "at random state 64, training accuracy is 0.8721749515688952\n",
      "at random state 64, testing accuracy is 0.7880770665604698\n",
      "at random state 64, mean squared error is 0.031619378398791545\n",
      "at random state 64, mean absolute error is 0.12421110271903321\n",
      "\n",
      "\n",
      "at random state 65, training accuracy is 0.8704622465268584\n",
      "at random state 65, testing accuracy is 0.8074498511133181\n",
      "at random state 65, mean squared error is 0.02846494335347432\n",
      "at random state 65, mean absolute error is 0.12071185800604228\n",
      "\n",
      "\n",
      "at random state 66, training accuracy is 0.8730039969391294\n",
      "at random state 66, testing accuracy is 0.7929105558044736\n",
      "at random state 66, mean squared error is 0.030273244712990935\n",
      "at random state 66, mean absolute error is 0.12290558912386708\n",
      "\n",
      "\n",
      "at random state 67, training accuracy is 0.8722512653110405\n",
      "at random state 67, testing accuracy is 0.7879556114146219\n",
      "at random state 67, mean squared error is 0.031190740181268883\n",
      "at random state 67, mean absolute error is 0.1229833836858006\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 68, training accuracy is 0.8703043592266422\n",
      "at random state 68, testing accuracy is 0.791839095084055\n",
      "at random state 68, mean squared error is 0.030585536253776434\n",
      "at random state 68, mean absolute error is 0.12234138972809668\n",
      "\n",
      "\n",
      "at random state 69, training accuracy is 0.8710395689506123\n",
      "at random state 69, testing accuracy is 0.7984249827431558\n",
      "at random state 69, mean squared error is 0.03050553776435045\n",
      "at random state 69, mean absolute error is 0.12320921450151054\n",
      "\n",
      "\n",
      "at random state 70, training accuracy is 0.8751481652771773\n",
      "at random state 70, testing accuracy is 0.7875072661316136\n",
      "at random state 70, mean squared error is 0.032639009818731114\n",
      "at random state 70, mean absolute error is 0.12697847432024167\n",
      "\n",
      "\n",
      "at random state 71, training accuracy is 0.8731124206460419\n",
      "at random state 71, testing accuracy is 0.7945500855171008\n",
      "at random state 71, mean squared error is 0.030592097432024165\n",
      "at random state 71, mean absolute error is 0.12433043806646525\n",
      "\n",
      "\n",
      "at random state 72, training accuracy is 0.8702876143432505\n",
      "at random state 72, testing accuracy is 0.7942709054340955\n",
      "at random state 72, mean squared error is 0.030885186555891238\n",
      "at random state 72, mean absolute error is 0.12308043806646526\n",
      "\n",
      "\n",
      "at random state 73, training accuracy is 0.8714092949725394\n",
      "at random state 73, testing accuracy is 0.7928174124446662\n",
      "at random state 73, mean squared error is 0.030538976586102717\n",
      "at random state 73, mean absolute error is 0.12331004531722053\n",
      "\n",
      "\n",
      "at random state 74, training accuracy is 0.8697315426390337\n",
      "at random state 74, testing accuracy is 0.7959778172648939\n",
      "at random state 74, mean squared error is 0.030188198640483384\n",
      "at random state 74, mean absolute error is 0.12368013595166163\n",
      "\n",
      "\n",
      "at random state 75, training accuracy is 0.8697806101835893\n",
      "at random state 75, testing accuracy is 0.8029295193252981\n",
      "at random state 75, mean squared error is 0.029615529456193352\n",
      "at random state 75, mean absolute error is 0.12241956193353473\n",
      "\n",
      "\n",
      "at random state 76, training accuracy is 0.873617507362308\n",
      "at random state 76, testing accuracy is 0.7982971797666574\n",
      "at random state 76, mean squared error is 0.029496589123867066\n",
      "at random state 76, mean absolute error is 0.12133157099697883\n",
      "\n",
      "\n",
      "at random state 77, training accuracy is 0.8690795721500284\n",
      "at random state 77, testing accuracy is 0.8001036717345807\n",
      "at random state 77, mean squared error is 0.029555998489425982\n",
      "at random state 77, mean absolute error is 0.12248716012084593\n",
      "\n",
      "\n",
      "at random state 78, training accuracy is 0.8699511740883722\n",
      "at random state 78, testing accuracy is 0.8035582944626349\n",
      "at random state 78, mean squared error is 0.029731954682779456\n",
      "at random state 78, mean absolute error is 0.12203776435045317\n",
      "\n",
      "\n",
      "at random state 79, training accuracy is 0.8728951995518356\n",
      "at random state 79, testing accuracy is 0.793755834961883\n",
      "at random state 79, mean squared error is 0.030973622356495468\n",
      "at random state 79, mean absolute error is 0.12333912386706948\n",
      "\n",
      "\n",
      "at random state 80, training accuracy is 0.8771478638814505\n",
      "at random state 80, testing accuracy is 0.7915909947035935\n",
      "at random state 80, mean squared error is 0.030110106495468275\n",
      "at random state 80, mean absolute error is 0.12153361027190333\n",
      "\n",
      "\n",
      "at random state 81, training accuracy is 0.8726955785429731\n",
      "at random state 81, testing accuracy is 0.797098161371452\n",
      "at random state 81, mean squared error is 0.030605148036253773\n",
      "at random state 81, mean absolute error is 0.12352039274924471\n",
      "\n",
      "\n",
      "at random state 82, training accuracy is 0.87276316089996\n",
      "at random state 82, testing accuracy is 0.7936427560070716\n",
      "at random state 82, mean squared error is 0.030744438066465256\n",
      "at random state 82, mean absolute error is 0.12361253776435047\n",
      "\n",
      "\n",
      "at random state 83, training accuracy is 0.8715153259308135\n",
      "at random state 83, testing accuracy is 0.8054972265208459\n",
      "at random state 83, mean squared error is 0.028568726586102718\n",
      "at random state 83, mean absolute error is 0.12066540785498486\n",
      "\n",
      "\n",
      "at random state 84, training accuracy is 0.8736015096485205\n",
      "at random state 84, testing accuracy is 0.7939703167523413\n",
      "at random state 84, mean squared error is 0.030110054380664653\n",
      "at random state 84, mean absolute error is 0.12233383685800604\n",
      "\n",
      "\n",
      "at random state 85, training accuracy is 0.8706031499316387\n",
      "at random state 85, testing accuracy is 0.7959951127937427\n",
      "at random state 85, mean squared error is 0.031907921450151054\n",
      "at random state 85, mean absolute error is 0.12608534743202418\n",
      "\n",
      "\n",
      "at random state 86, training accuracy is 0.8734697663717873\n",
      "at random state 86, testing accuracy is 0.7959999149272405\n",
      "at random state 86, mean squared error is 0.03097058383685801\n",
      "at random state 86, mean absolute error is 0.124875\n",
      "\n",
      "\n",
      "at random state 87, training accuracy is 0.8730401353928462\n",
      "at random state 87, testing accuracy is 0.7840620384447553\n",
      "at random state 87, mean squared error is 0.032373099697885196\n",
      "at random state 87, mean absolute error is 0.12568051359516616\n",
      "\n",
      "\n",
      "at random state 88, training accuracy is 0.8698495666329399\n",
      "at random state 88, testing accuracy is 0.8003623351881326\n",
      "at random state 88, mean squared error is 0.0299405332326284\n",
      "at random state 88, mean absolute error is 0.12191691842900303\n",
      "\n",
      "\n",
      "at random state 89, training accuracy is 0.8732883695314004\n",
      "at random state 89, testing accuracy is 0.7914304531005589\n",
      "at random state 89, mean squared error is 0.03120436858006042\n",
      "at random state 89, mean absolute error is 0.12397205438066466\n",
      "\n",
      "\n",
      "at random state 90, training accuracy is 0.874613277738408\n",
      "at random state 90, testing accuracy is 0.7908198874305912\n",
      "at random state 90, mean squared error is 0.03058703851963746\n",
      "at random state 90, mean absolute error is 0.12365143504531721\n",
      "\n",
      "\n",
      "at random state 91, training accuracy is 0.8733721225393773\n",
      "at random state 91, testing accuracy is 0.7855513670815467\n",
      "at random state 91, mean squared error is 0.03169221148036254\n",
      "at random state 91, mean absolute error is 0.12483081570996978\n",
      "\n",
      "\n",
      "at random state 92, training accuracy is 0.8716285358245784\n",
      "at random state 92, testing accuracy is 0.7957794286758596\n",
      "at random state 92, mean squared error is 0.029628071752265856\n",
      "at random state 92, mean absolute error is 0.12274584592145013\n",
      "\n",
      "\n",
      "at random state 93, training accuracy is 0.8762372691334931\n",
      "at random state 93, testing accuracy is 0.7782209246584787\n",
      "at random state 93, mean squared error is 0.032723969788519644\n",
      "at random state 93, mean absolute error is 0.1261472809667674\n",
      "\n",
      "\n",
      "at random state 94, training accuracy is 0.8737723895891129\n",
      "at random state 94, testing accuracy is 0.7919832508035121\n",
      "at random state 94, mean squared error is 0.030450713746223563\n",
      "at random state 94, mean absolute error is 0.12222167673716013\n",
      "\n",
      "\n",
      "at random state 95, training accuracy is 0.8705221614940005\n",
      "at random state 95, testing accuracy is 0.8017760721969837\n",
      "at random state 95, mean squared error is 0.029740260574018122\n",
      "at random state 95, mean absolute error is 0.12186291540785497\n",
      "\n",
      "\n",
      "at random state 96, training accuracy is 0.8705461425946314\n",
      "at random state 96, testing accuracy is 0.7977840650594877\n",
      "at random state 96, mean squared error is 0.030885170694864045\n",
      "at random state 96, mean absolute error is 0.12400302114803625\n",
      "\n",
      "\n",
      "at random state 97, training accuracy is 0.8713830418604699\n",
      "at random state 97, testing accuracy is 0.8002281434536895\n",
      "at random state 97, mean squared error is 0.03025210574018127\n",
      "at random state 97, mean absolute error is 0.12512839879154078\n",
      "\n",
      "\n",
      "at random state 98, training accuracy is 0.8765266701329267\n",
      "at random state 98, testing accuracy is 0.7796114529562058\n",
      "at random state 98, mean squared error is 0.032240285498489424\n",
      "at random state 98, mean absolute error is 0.12589199395770392\n",
      "\n",
      "\n",
      "at random state 99, training accuracy is 0.8746269849520496\n",
      "at random state 99, testing accuracy is 0.7970305214844222\n",
      "at random state 99, mean squared error is 0.029231626888217524\n",
      "at random state 99, mean absolute error is 0.12078247734138972\n",
      "\n",
      "\n",
      "Max accuracy at random state 6 = 0.8083798868886534\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "knr=KNeighborsRegressor()\n",
    "model_selection(knr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ba81a8",
   "metadata": {},
   "source": [
    "CV score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "55ba2c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.07305524 -0.31773364  0.0844167   0.10033252 -0.20758233  0.14972695\n",
      "  0.19883881  0.29118848  0.22587806 -0.20026004]\n",
      "0.039786075858580704\n",
      "0.19713355305821367\n"
     ]
    }
   ],
   "source": [
    "score=cross_val_score(knr,dfx,y,cv=10)\n",
    "print(score)\n",
    "print(score.mean())\n",
    "print(score.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf5e77f",
   "metadata": {},
   "source": [
    "# DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "232e5f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 0, training accuracy is 1.0\n",
      "at random state 0, testing accuracy is 0.6774532202415088\n",
      "at random state 0, mean squared error is 0.04833735485698102\n",
      "at random state 0, mean absolute error is 0.1464797507788162\n",
      "\n",
      "\n",
      "at random state 1, training accuracy is 1.0\n",
      "at random state 1, testing accuracy is 0.68320534822074\n",
      "at random state 1, mean squared error is 0.0477327386009629\n",
      "at random state 1, mean absolute error is 0.14468139337298216\n",
      "\n",
      "\n",
      "at random state 2, training accuracy is 1.0\n",
      "at random state 2, testing accuracy is 0.6682543438048768\n",
      "at random state 2, mean squared error is 0.04929141886151232\n",
      "at random state 2, mean absolute error is 0.14656471254602096\n",
      "\n",
      "\n",
      "at random state 3, training accuracy is 1.0\n",
      "at random state 3, testing accuracy is 0.6844887972954702\n",
      "at random state 3, mean squared error is 0.046623279524214106\n",
      "at random state 3, mean absolute error is 0.145154347210422\n",
      "\n",
      "\n",
      "at random state 4, training accuracy is 1.0\n",
      "at random state 4, testing accuracy is 0.6986242424782265\n",
      "at random state 4, mean squared error is 0.04394738034551119\n",
      "at random state 4, mean absolute error is 0.14052109883885586\n",
      "\n",
      "\n",
      "at random state 5, training accuracy is 1.0\n",
      "at random state 5, testing accuracy is 0.6803780447911426\n",
      "at random state 5, mean squared error is 0.0482315774568111\n",
      "at random state 5, mean absolute error is 0.14544604927782495\n",
      "\n",
      "\n",
      "at random state 6, training accuracy is 1.0\n",
      "at random state 6, testing accuracy is 0.6948605836770854\n",
      "at random state 6, mean squared error is 0.04606230529595016\n",
      "at random state 6, mean absolute error is 0.13983007646559048\n",
      "\n",
      "\n",
      "at random state 7, training accuracy is 1.0\n",
      "at random state 7, testing accuracy is 0.6519638191171218\n",
      "at random state 7, mean squared error is 0.05243574058340413\n",
      "at random state 7, mean absolute error is 0.14753327669215519\n",
      "\n",
      "\n",
      "at random state 8, training accuracy is 1.0\n",
      "at random state 8, testing accuracy is 0.6819942585132046\n",
      "at random state 8, mean squared error is 0.04876323987538941\n",
      "at random state 8, mean absolute error is 0.14624752194845653\n",
      "\n",
      "\n",
      "at random state 9, training accuracy is 1.0\n",
      "at random state 9, testing accuracy is 0.6863479512469588\n",
      "at random state 9, mean squared error is 0.04699249504389691\n",
      "at random state 9, mean absolute error is 0.14328235627301047\n",
      "\n",
      "\n",
      "at random state 10, training accuracy is 1.0\n",
      "at random state 10, testing accuracy is 0.6823385033581353\n",
      "at random state 10, mean squared error is 0.0464881336731804\n",
      "at random state 10, mean absolute error is 0.1437892948173322\n",
      "\n",
      "\n",
      "at random state 11, training accuracy is 1.0\n",
      "at random state 11, testing accuracy is 0.6763701734115115\n",
      "at random state 11, mean squared error is 0.04870212404418012\n",
      "at random state 11, mean absolute error is 0.1454148966298499\n",
      "\n",
      "\n",
      "at random state 12, training accuracy is 1.0\n",
      "at random state 12, testing accuracy is 0.6558096812875869\n",
      "at random state 12, mean squared error is 0.05107992070235061\n",
      "at random state 12, mean absolute error is 0.14770886434437838\n",
      "\n",
      "\n",
      "at random state 13, training accuracy is 1.0\n",
      "at random state 13, testing accuracy is 0.6839777580138529\n",
      "at random state 13, mean squared error is 0.0464816199376947\n",
      "at random state 13, mean absolute error is 0.14407250070801472\n",
      "\n",
      "\n",
      "at random state 14, training accuracy is 1.0\n",
      "at random state 14, testing accuracy is 0.6829033530339447\n",
      "at random state 14, mean squared error is 0.046416680826961196\n",
      "at random state 14, mean absolute error is 0.14184367034834325\n",
      "\n",
      "\n",
      "at random state 15, training accuracy is 1.0\n",
      "at random state 15, testing accuracy is 0.6741259648104148\n",
      "at random state 15, mean squared error is 0.04757604078164825\n",
      "at random state 15, mean absolute error is 0.14112715944491644\n",
      "\n",
      "\n",
      "at random state 16, training accuracy is 1.0\n",
      "at random state 16, testing accuracy is 0.669759984694944\n",
      "at random state 16, mean squared error is 0.04833154913622203\n",
      "at random state 16, mean absolute error is 0.1435882186349476\n",
      "\n",
      "\n",
      "at random state 17, training accuracy is 1.0\n",
      "at random state 17, testing accuracy is 0.6686883484765164\n",
      "at random state 17, mean squared error is 0.04771098838855848\n",
      "at random state 17, mean absolute error is 0.14500424808836024\n",
      "\n",
      "\n",
      "at random state 18, training accuracy is 1.0\n",
      "at random state 18, testing accuracy is 0.7068247684157678\n",
      "at random state 18, mean squared error is 0.043609204191447176\n",
      "at random state 18, mean absolute error is 0.1391702067403002\n",
      "\n",
      "\n",
      "at random state 19, training accuracy is 1.0\n",
      "at random state 19, testing accuracy is 0.6614813221079088\n",
      "at random state 19, mean squared error is 0.048873067119796096\n",
      "at random state 19, mean absolute error is 0.1450127442650807\n",
      "\n",
      "\n",
      "at random state 20, training accuracy is 1.0\n",
      "at random state 20, testing accuracy is 0.6581012138722051\n",
      "at random state 20, mean squared error is 0.051070858113848774\n",
      "at random state 20, mean absolute error is 0.1467233078448032\n",
      "\n",
      "\n",
      "at random state 21, training accuracy is 1.0\n",
      "at random state 21, testing accuracy is 0.6885984863336546\n",
      "at random state 21, mean squared error is 0.045999915038232794\n",
      "at random state 21, mean absolute error is 0.14369583687340695\n",
      "\n",
      "\n",
      "at random state 22, training accuracy is 1.0\n",
      "at random state 22, testing accuracy is 0.6680409110984795\n",
      "at random state 22, mean squared error is 0.048508892664967426\n",
      "at random state 22, mean absolute error is 0.1472868875672614\n",
      "\n",
      "\n",
      "at random state 23, training accuracy is 1.0\n",
      "at random state 23, testing accuracy is 0.6980989614400748\n",
      "at random state 23, mean squared error is 0.04461639762107051\n",
      "at random state 23, mean absolute error is 0.1392948173322005\n",
      "\n",
      "\n",
      "at random state 24, training accuracy is 1.0\n",
      "at random state 24, testing accuracy is 0.6914067340194107\n",
      "at random state 24, mean squared error is 0.044943103936561875\n",
      "at random state 24, mean absolute error is 0.1403823279524214\n",
      "\n",
      "\n",
      "at random state 25, training accuracy is 1.0\n",
      "at random state 25, testing accuracy is 0.679121602614114\n",
      "at random state 25, mean squared error is 0.04691277258566978\n",
      "at random state 25, mean absolute error is 0.14471254602095723\n",
      "\n",
      "\n",
      "at random state 26, training accuracy is 1.0\n",
      "at random state 26, testing accuracy is 0.6933195139674619\n",
      "at random state 26, mean squared error is 0.047330671197960916\n",
      "at random state 26, mean absolute error is 0.14249221183800623\n",
      "\n",
      "\n",
      "at random state 27, training accuracy is 1.0\n",
      "at random state 27, testing accuracy is 0.6883590657786687\n",
      "at random state 27, mean squared error is 0.046515151515151516\n",
      "at random state 27, mean absolute error is 0.14133673180402154\n",
      "\n",
      "\n",
      "at random state 28, training accuracy is 1.0\n",
      "at random state 28, testing accuracy is 0.7036564120178272\n",
      "at random state 28, mean squared error is 0.04320586236193713\n",
      "at random state 28, mean absolute error is 0.13890399320305863\n",
      "\n",
      "\n",
      "at random state 29, training accuracy is 1.0\n",
      "at random state 29, testing accuracy is 0.7047718542741992\n",
      "at random state 29, mean squared error is 0.0450719909374115\n",
      "at random state 29, mean absolute error is 0.1428490512602662\n",
      "\n",
      "\n",
      "at random state 30, training accuracy is 1.0\n",
      "at random state 30, testing accuracy is 0.6715438699376608\n",
      "at random state 30, mean squared error is 0.04750283205890682\n",
      "at random state 30, mean absolute error is 0.14415463041631266\n",
      "\n",
      "\n",
      "at random state 31, training accuracy is 1.0\n",
      "at random state 31, testing accuracy is 0.6788001331031671\n",
      "at random state 31, mean squared error is 0.048161653922401584\n",
      "at random state 31, mean absolute error is 0.14360237892948172\n",
      "\n",
      "\n",
      "at random state 32, training accuracy is 1.0\n",
      "at random state 32, testing accuracy is 0.696839933983816\n",
      "at random state 32, mean squared error is 0.04482010761823846\n",
      "at random state 32, mean absolute error is 0.14137638062871707\n",
      "\n",
      "\n",
      "at random state 33, training accuracy is 1.0\n",
      "at random state 33, testing accuracy is 0.6887470321858592\n",
      "at random state 33, mean squared error is 0.044784451996601526\n",
      "at random state 33, mean absolute error is 0.13982724440668365\n",
      "\n",
      "\n",
      "at random state 34, training accuracy is 1.0\n",
      "at random state 34, testing accuracy is 0.7021064180982326\n",
      "at random state 34, mean squared error is 0.04491107335032568\n",
      "at random state 34, mean absolute error is 0.13988671764372698\n",
      "\n",
      "\n",
      "at random state 35, training accuracy is 1.0\n",
      "at random state 35, testing accuracy is 0.6842269651513337\n",
      "at random state 35, mean squared error is 0.04675117530444633\n",
      "at random state 35, mean absolute error is 0.14324553950722174\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 36, training accuracy is 1.0\n",
      "at random state 36, testing accuracy is 0.672927472704951\n",
      "at random state 36, mean squared error is 0.04693446615689606\n",
      "at random state 36, mean absolute error is 0.14231662418578306\n",
      "\n",
      "\n",
      "at random state 37, training accuracy is 1.0\n",
      "at random state 37, testing accuracy is 0.7037104395496406\n",
      "at random state 37, mean squared error is 0.04409991503823279\n",
      "at random state 37, mean absolute error is 0.13932030586236194\n",
      "\n",
      "\n",
      "at random state 38, training accuracy is 1.0\n",
      "at random state 38, testing accuracy is 0.6861344185516519\n",
      "at random state 38, mean squared error is 0.04634927782497875\n",
      "at random state 38, mean absolute error is 0.1410960067969414\n",
      "\n",
      "\n",
      "at random state 39, training accuracy is 1.0\n",
      "at random state 39, testing accuracy is 0.6891929781949162\n",
      "at random state 39, mean squared error is 0.04499569527046162\n",
      "at random state 39, mean absolute error is 0.1411554800339847\n",
      "\n",
      "\n",
      "at random state 40, training accuracy is 1.0\n",
      "at random state 40, testing accuracy is 0.6840673202003913\n",
      "at random state 40, mean squared error is 0.046731124327386\n",
      "at random state 40, mean absolute error is 0.1425205324270745\n",
      "\n",
      "\n",
      "at random state 41, training accuracy is 1.0\n",
      "at random state 41, testing accuracy is 0.7066495526828158\n",
      "at random state 41, mean squared error is 0.04328351741716227\n",
      "at random state 41, mean absolute error is 0.14165109034267914\n",
      "\n",
      "\n",
      "at random state 42, training accuracy is 1.0\n",
      "at random state 42, testing accuracy is 0.6881722703850403\n",
      "at random state 42, mean squared error is 0.04606182384593599\n",
      "at random state 42, mean absolute error is 0.1414358538657604\n",
      "\n",
      "\n",
      "at random state 43, training accuracy is 1.0\n",
      "at random state 43, testing accuracy is 0.7014556384922284\n",
      "at random state 43, mean squared error is 0.04489889549702634\n",
      "at random state 43, mean absolute error is 0.1394052676295667\n",
      "\n",
      "\n",
      "at random state 44, training accuracy is 1.0\n",
      "at random state 44, testing accuracy is 0.6923815752316538\n",
      "at random state 44, mean squared error is 0.0463743981874823\n",
      "at random state 44, mean absolute error is 0.1433418295100538\n",
      "\n",
      "\n",
      "at random state 45, training accuracy is 1.0\n",
      "at random state 45, testing accuracy is 0.6687276279584237\n",
      "at random state 45, mean squared error is 0.048563098272444066\n",
      "at random state 45, mean absolute error is 0.14506088926649674\n",
      "\n",
      "\n",
      "at random state 46, training accuracy is 1.0\n",
      "at random state 46, testing accuracy is 0.7196612832425973\n",
      "at random state 46, mean squared error is 0.042137553101104494\n",
      "at random state 46, mean absolute error is 0.13636080430472955\n",
      "\n",
      "\n",
      "at random state 47, training accuracy is 1.0\n",
      "at random state 47, testing accuracy is 0.6605198612363196\n",
      "at random state 47, mean squared error is 0.0493511186632682\n",
      "at random state 47, mean absolute error is 0.14567544604927782\n",
      "\n",
      "\n",
      "at random state 48, training accuracy is 1.0\n",
      "at random state 48, testing accuracy is 0.6935922695799495\n",
      "at random state 48, mean squared error is 0.045291305579156046\n",
      "at random state 48, mean absolute error is 0.14006230529595015\n",
      "\n",
      "\n",
      "at random state 49, training accuracy is 1.0\n",
      "at random state 49, testing accuracy is 0.6961260292712719\n",
      "at random state 49, mean squared error is 0.045309232512036245\n",
      "at random state 49, mean absolute error is 0.13864910790144436\n",
      "\n",
      "\n",
      "at random state 50, training accuracy is 1.0\n",
      "at random state 50, testing accuracy is 0.6677012647569176\n",
      "at random state 50, mean squared error is 0.04836910223732653\n",
      "at random state 50, mean absolute error is 0.14327102803738317\n",
      "\n",
      "\n",
      "at random state 51, training accuracy is 1.0\n",
      "at random state 51, testing accuracy is 0.6415393824008886\n",
      "at random state 51, mean squared error is 0.05303180402152364\n",
      "at random state 51, mean absolute error is 0.15175587652223166\n",
      "\n",
      "\n",
      "at random state 52, training accuracy is 1.0\n",
      "at random state 52, testing accuracy is 0.6875186525214362\n",
      "at random state 52, mean squared error is 0.0475762673463608\n",
      "at random state 52, mean absolute error is 0.14551685075049559\n",
      "\n",
      "\n",
      "at random state 53, training accuracy is 1.0\n",
      "at random state 53, testing accuracy is 0.687704458403469\n",
      "at random state 53, mean squared error is 0.047030671197960915\n",
      "at random state 53, mean absolute error is 0.1456952704616256\n",
      "\n",
      "\n",
      "at random state 54, training accuracy is 1.0\n",
      "at random state 54, testing accuracy is 0.6846396450281818\n",
      "at random state 54, mean squared error is 0.04678793542905692\n",
      "at random state 54, mean absolute error is 0.14360804304729538\n",
      "\n",
      "\n",
      "at random state 55, training accuracy is 1.0\n",
      "at random state 55, testing accuracy is 0.676244001003607\n",
      "at random state 55, mean squared error is 0.04956355140186915\n",
      "at random state 55, mean absolute error is 0.14880487114131974\n",
      "\n",
      "\n",
      "at random state 56, training accuracy is 1.0\n",
      "at random state 56, testing accuracy is 0.6735182133231772\n",
      "at random state 56, mean squared error is 0.04705267629566695\n",
      "at random state 56, mean absolute error is 0.14259416595865193\n",
      "\n",
      "\n",
      "at random state 57, training accuracy is 1.0\n",
      "at random state 57, testing accuracy is 0.6911495880045708\n",
      "at random state 57, mean squared error is 0.047241744548286606\n",
      "at random state 57, mean absolute error is 0.14188615123194562\n",
      "\n",
      "\n",
      "at random state 58, training accuracy is 1.0\n",
      "at random state 58, testing accuracy is 0.6990767826059394\n",
      "at random state 58, mean squared error is 0.04541588785046729\n",
      "at random state 58, mean absolute error is 0.1419739450580572\n",
      "\n",
      "\n",
      "at random state 59, training accuracy is 1.0\n",
      "at random state 59, testing accuracy is 0.6583544074745437\n",
      "at random state 59, mean squared error is 0.048990116114415186\n",
      "at random state 59, mean absolute error is 0.1473322005097706\n",
      "\n",
      "\n",
      "at random state 60, training accuracy is 1.0\n",
      "at random state 60, testing accuracy is 0.695614454699358\n",
      "at random state 60, mean squared error is 0.04686573208722741\n",
      "at random state 60, mean absolute error is 0.1434409515717927\n",
      "\n",
      "\n",
      "at random state 61, training accuracy is 1.0\n",
      "at random state 61, testing accuracy is 0.6833939297145821\n",
      "at random state 61, mean squared error is 0.046917502124044184\n",
      "at random state 61, mean absolute error is 0.1412885868026055\n",
      "\n",
      "\n",
      "at random state 62, training accuracy is 1.0\n",
      "at random state 62, testing accuracy is 0.6928491455300037\n",
      "at random state 62, mean squared error is 0.04553299348626451\n",
      "at random state 62, mean absolute error is 0.1438685924667233\n",
      "\n",
      "\n",
      "at random state 63, training accuracy is 1.0\n",
      "at random state 63, testing accuracy is 0.6616074233384487\n",
      "at random state 63, mean squared error is 0.052139280657037664\n",
      "at random state 63, mean absolute error is 0.15133956386292832\n",
      "\n",
      "\n",
      "at random state 64, training accuracy is 1.0\n",
      "at random state 64, testing accuracy is 0.6738574929120014\n",
      "at random state 64, mean squared error is 0.048446927216086094\n",
      "at random state 64, mean absolute error is 0.14311243273860097\n",
      "\n",
      "\n",
      "at random state 65, training accuracy is 1.0\n",
      "at random state 65, testing accuracy is 0.6621364511380694\n",
      "at random state 65, mean squared error is 0.049822146700651375\n",
      "at random state 65, mean absolute error is 0.14497309544038517\n",
      "\n",
      "\n",
      "at random state 66, training accuracy is 1.0\n",
      "at random state 66, testing accuracy is 0.6882746392775604\n",
      "at random state 66, mean squared error is 0.045313140753327666\n",
      "at random state 66, mean absolute error is 0.1408184650240725\n",
      "\n",
      "\n",
      "at random state 67, training accuracy is 1.0\n",
      "at random state 67, testing accuracy is 0.6499178891873374\n",
      "at random state 67, mean squared error is 0.0522672613990371\n",
      "at random state 67, mean absolute error is 0.1508496176720476\n",
      "\n",
      "\n",
      "at random state 68, training accuracy is 1.0\n",
      "at random state 68, testing accuracy is 0.6917729040643021\n",
      "at random state 68, mean squared error is 0.04688986122911356\n",
      "at random state 68, mean absolute error is 0.14498725573491927\n",
      "\n",
      "\n",
      "at random state 69, training accuracy is 1.0\n",
      "at random state 69, testing accuracy is 0.6820454833349479\n",
      "at random state 69, mean squared error is 0.04833809119229679\n",
      "at random state 69, mean absolute error is 0.14593599546870573\n",
      "\n",
      "\n",
      "at random state 70, training accuracy is 1.0\n",
      "at random state 70, testing accuracy is 0.6612051443786292\n",
      "at random state 70, mean squared error is 0.05228937977909941\n",
      "at random state 70, mean absolute error is 0.1499801755876522\n",
      "\n",
      "\n",
      "at random state 71, training accuracy is 1.0\n",
      "at random state 71, testing accuracy is 0.6580670169984632\n",
      "at random state 71, mean squared error is 0.05103534409515718\n",
      "at random state 71, mean absolute error is 0.1465024072500708\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 72, training accuracy is 1.0\n",
      "at random state 72, testing accuracy is 0.700376433141922\n",
      "at random state 72, mean squared error is 0.04588006230529595\n",
      "at random state 72, mean absolute error is 0.14085811384876806\n",
      "\n",
      "\n",
      "at random state 73, training accuracy is 1.0\n",
      "at random state 73, testing accuracy is 0.6906273324002921\n",
      "at random state 73, mean squared error is 0.046040583404134805\n",
      "at random state 73, mean absolute error is 0.14160011328235625\n",
      "\n",
      "\n",
      "at random state 74, training accuracy is 1.0\n",
      "at random state 74, testing accuracy is 0.679192877159186\n",
      "at random state 74, mean squared error is 0.046764061172472386\n",
      "at random state 74, mean absolute error is 0.14340130274709714\n",
      "\n",
      "\n",
      "at random state 75, training accuracy is 1.0\n",
      "at random state 75, testing accuracy is 0.6904097590402716\n",
      "at random state 75, mean squared error is 0.04631030869442085\n",
      "at random state 75, mean absolute error is 0.14267629566694986\n",
      "\n",
      "\n",
      "at random state 76, training accuracy is 1.0\n",
      "at random state 76, testing accuracy is 0.6691338265186637\n",
      "at random state 76, mean squared error is 0.0474947323704333\n",
      "at random state 76, mean absolute error is 0.1424553950722175\n",
      "\n",
      "\n",
      "at random state 77, training accuracy is 1.0\n",
      "at random state 77, testing accuracy is 0.6866694198366585\n",
      "at random state 77, mean squared error is 0.04615879354290569\n",
      "at random state 77, mean absolute error is 0.14145284621920134\n",
      "\n",
      "\n",
      "at random state 78, training accuracy is 1.0\n",
      "at random state 78, testing accuracy is 0.6889854170170027\n",
      "at random state 78, mean squared error is 0.04694760691022373\n",
      "at random state 78, mean absolute error is 0.1432398753894081\n",
      "\n",
      "\n",
      "at random state 79, training accuracy is 1.0\n",
      "at random state 79, testing accuracy is 0.677066145785209\n",
      "at random state 79, mean squared error is 0.04773412630982724\n",
      "at random state 79, mean absolute error is 0.14434437836306993\n",
      "\n",
      "\n",
      "at random state 80, training accuracy is 1.0\n",
      "at random state 80, testing accuracy is 0.6697921768348983\n",
      "at random state 80, mean squared error is 0.04664868309260833\n",
      "at random state 80, mean absolute error is 0.14329934862645144\n",
      "\n",
      "\n",
      "at random state 81, training accuracy is 1.0\n",
      "at random state 81, testing accuracy is 0.683013729931554\n",
      "at random state 81, mean squared error is 0.04752537524780515\n",
      "at random state 81, mean absolute error is 0.14379779099405268\n",
      "\n",
      "\n",
      "at random state 82, training accuracy is 1.0\n",
      "at random state 82, testing accuracy is 0.6854695968291955\n",
      "at random state 82, mean squared error is 0.04713307844803172\n",
      "at random state 82, mean absolute error is 0.14394505805720756\n",
      "\n",
      "\n",
      "at random state 83, training accuracy is 1.0\n",
      "at random state 83, testing accuracy is 0.6833437941734863\n",
      "at random state 83, mean squared error is 0.046677909940526764\n",
      "at random state 83, mean absolute error is 0.14335032568677428\n",
      "\n",
      "\n",
      "at random state 84, training accuracy is 1.0\n",
      "at random state 84, testing accuracy is 0.6757350822263233\n",
      "at random state 84, mean squared error is 0.04681600113282356\n",
      "at random state 84, mean absolute error is 0.14222316624185782\n",
      "\n",
      "\n",
      "at random state 85, training accuracy is 1.0\n",
      "at random state 85, testing accuracy is 0.6791124334749934\n",
      "at random state 85, mean squared error is 0.051349815916171056\n",
      "at random state 85, mean absolute error is 0.14824695553667516\n",
      "\n",
      "\n",
      "at random state 86, training accuracy is 1.0\n",
      "at random state 86, testing accuracy is 0.7014071920121372\n",
      "at random state 86, mean squared error is 0.04606805437553101\n",
      "at random state 86, mean absolute error is 0.142398753894081\n",
      "\n",
      "\n",
      "at random state 87, training accuracy is 1.0\n",
      "at random state 87, testing accuracy is 0.6997014750663995\n",
      "at random state 87, mean squared error is 0.04528745397904276\n",
      "at random state 87, mean absolute error is 0.14200509770603228\n",
      "\n",
      "\n",
      "at random state 88, training accuracy is 1.0\n",
      "at random state 88, testing accuracy is 0.6678974370990074\n",
      "at random state 88, mean squared error is 0.04896884735202492\n",
      "at random state 88, mean absolute error is 0.14424242424242423\n",
      "\n",
      "\n",
      "at random state 89, training accuracy is 1.0\n",
      "at random state 89, testing accuracy is 0.6637114442967285\n",
      "at random state 89, mean squared error is 0.04946887567261399\n",
      "at random state 89, mean absolute error is 0.14534126309827244\n",
      "\n",
      "\n",
      "at random state 90, training accuracy is 1.0\n",
      "at random state 90, testing accuracy is 0.6567232492356982\n",
      "at random state 90, mean squared error is 0.05007017841971113\n",
      "at random state 90, mean absolute error is 0.14884168790710844\n",
      "\n",
      "\n",
      "at random state 91, training accuracy is 1.0\n",
      "at random state 91, testing accuracy is 0.6942974030773469\n",
      "at random state 91, mean squared error is 0.04572656471254602\n",
      "at random state 91, mean absolute error is 0.1410563579722458\n",
      "\n",
      "\n",
      "at random state 92, training accuracy is 1.0\n",
      "at random state 92, testing accuracy is 0.6849114599209831\n",
      "at random state 92, mean squared error is 0.045909459076748795\n",
      "at random state 92, mean absolute error is 0.14257434154630416\n",
      "\n",
      "\n",
      "at random state 93, training accuracy is 1.0\n",
      "at random state 93, testing accuracy is 0.6794043429860941\n",
      "at random state 93, mean squared error is 0.048041971112999154\n",
      "at random state 93, mean absolute error is 0.14243557065986973\n",
      "\n",
      "\n",
      "at random state 94, training accuracy is 1.0\n",
      "at random state 94, testing accuracy is 0.6597014636910362\n",
      "at random state 94, mean squared error is 0.050369385443217216\n",
      "at random state 94, mean absolute error is 0.148719909374115\n",
      "\n",
      "\n",
      "at random state 95, training accuracy is 1.0\n",
      "at random state 95, testing accuracy is 0.6788541020649483\n",
      "at random state 95, mean squared error is 0.048623336165392245\n",
      "at random state 95, mean absolute error is 0.14562446898895495\n",
      "\n",
      "\n",
      "at random state 96, training accuracy is 1.0\n",
      "at random state 96, testing accuracy is 0.6873701432359134\n",
      "at random state 96, mean squared error is 0.04785123194562446\n",
      "at random state 96, mean absolute error is 0.14337298215802888\n",
      "\n",
      "\n",
      "at random state 97, training accuracy is 1.0\n",
      "at random state 97, testing accuracy is 0.6709576645293815\n",
      "at random state 97, mean squared error is 0.04887816482582837\n",
      "at random state 97, mean absolute error is 0.14738034551118667\n",
      "\n",
      "\n",
      "at random state 98, training accuracy is 1.0\n",
      "at random state 98, testing accuracy is 0.6760431898534973\n",
      "at random state 98, mean squared error is 0.047294902293967704\n",
      "at random state 98, mean absolute error is 0.1431067686207873\n",
      "\n",
      "\n",
      "at random state 99, training accuracy is 1.0\n",
      "at random state 99, testing accuracy is 0.702021251182106\n",
      "at random state 99, mean squared error is 0.043382469555366746\n",
      "at random state 99, mean absolute error is 0.1408297932596998\n",
      "\n",
      "\n",
      "Max accuracy at random state 46 = 0.7196612832425973\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dtc=DecisionTreeRegressor()\n",
    "model_selection(dtc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c81c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "score=cross_val_score(dtc,dfx,y,cv=8)\n",
    "print(score)\n",
    "print(score.mean())\n",
    "print(score.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167bfb33",
   "metadata": {},
   "source": [
    "# GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0a34d8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 0, training accuracy is 0.6899889461179713\n",
      "at random state 0, testing accuracy is 0.6662309013846404\n",
      "at random state 0, mean squared error is 0.04959615671637024\n",
      "at random state 0, mean absolute error is 0.17004839169196004\n",
      "\n",
      "\n",
      "at random state 1, training accuracy is 0.685905726763429\n",
      "at random state 1, testing accuracy is 0.6624408016861383\n",
      "at random state 1, mean squared error is 0.05081795314398608\n",
      "at random state 1, mean absolute error is 0.17243815052423814\n",
      "\n",
      "\n",
      "at random state 2, training accuracy is 0.6933509034470868\n",
      "at random state 2, testing accuracy is 0.6676168185350171\n",
      "at random state 2, mean squared error is 0.04984565536085859\n",
      "at random state 2, mean absolute error is 0.16991647357616407\n",
      "\n",
      "\n",
      "at random state 3, training accuracy is 0.6860845281827517\n",
      "at random state 3, testing accuracy is 0.6737041713341962\n",
      "at random state 3, mean squared error is 0.049108102377944586\n",
      "at random state 3, mean absolute error is 0.16841133799487365\n",
      "\n",
      "\n",
      "at random state 4, training accuracy is 0.6880141400145305\n",
      "at random state 4, testing accuracy is 0.6774175604262826\n",
      "at random state 4, mean squared error is 0.047385671512494836\n",
      "at random state 4, mean absolute error is 0.1667047545505074\n",
      "\n",
      "\n",
      "at random state 5, training accuracy is 0.6824366320558863\n",
      "at random state 5, testing accuracy is 0.6791539431789069\n",
      "at random state 5, mean squared error is 0.04774970229368615\n",
      "at random state 5, mean absolute error is 0.1659102967257699\n",
      "\n",
      "\n",
      "at random state 6, training accuracy is 0.6844429159862697\n",
      "at random state 6, testing accuracy is 0.6828760045118929\n",
      "at random state 6, mean squared error is 0.047395971462599724\n",
      "at random state 6, mean absolute error is 0.16519533569201963\n",
      "\n",
      "\n",
      "at random state 7, training accuracy is 0.6888476943720083\n",
      "at random state 7, testing accuracy is 0.6668192776164724\n",
      "at random state 7, mean squared error is 0.049646904274079244\n",
      "at random state 7, mean absolute error is 0.16868467730671774\n",
      "\n",
      "\n",
      "at random state 8, training accuracy is 0.6877818152882991\n",
      "at random state 8, testing accuracy is 0.6685806271242312\n",
      "at random state 8, mean squared error is 0.05048439363899133\n",
      "at random state 8, mean absolute error is 0.16958193834828975\n",
      "\n",
      "\n",
      "at random state 9, training accuracy is 0.6839698451194229\n",
      "at random state 9, testing accuracy is 0.6659645261562039\n",
      "at random state 9, mean squared error is 0.04890852208012764\n",
      "at random state 9, mean absolute error is 0.1689076320630527\n",
      "\n",
      "\n",
      "at random state 10, training accuracy is 0.6942435117330479\n",
      "at random state 10, testing accuracy is 0.6587538581051688\n",
      "at random state 10, mean squared error is 0.04927508544949368\n",
      "at random state 10, mean absolute error is 0.16886021131513743\n",
      "\n",
      "\n",
      "at random state 11, training accuracy is 0.6873185323609182\n",
      "at random state 11, testing accuracy is 0.6770830515940893\n",
      "at random state 11, mean squared error is 0.048617189441257605\n",
      "at random state 11, mean absolute error is 0.16750675375517501\n",
      "\n",
      "\n",
      "at random state 12, training accuracy is 0.6877534419398342\n",
      "at random state 12, testing accuracy is 0.6770945081866728\n",
      "at random state 12, mean squared error is 0.04748833386164216\n",
      "at random state 12, mean absolute error is 0.165586847184549\n",
      "\n",
      "\n",
      "at random state 13, training accuracy is 0.6853539136823306\n",
      "at random state 13, testing accuracy is 0.6710378598601423\n",
      "at random state 13, mean squared error is 0.0497085963029647\n",
      "at random state 13, mean absolute error is 0.1687047082352965\n",
      "\n",
      "\n",
      "at random state 14, training accuracy is 0.686076108117094\n",
      "at random state 14, testing accuracy is 0.6818691367063585\n",
      "at random state 14, mean squared error is 0.04648414395713164\n",
      "at random state 14, mean absolute error is 0.16432951408330396\n",
      "\n",
      "\n",
      "at random state 15, training accuracy is 0.6863520380583266\n",
      "at random state 15, testing accuracy is 0.6739398827951053\n",
      "at random state 15, mean squared error is 0.04722825233356735\n",
      "at random state 15, mean absolute error is 0.16560886755845372\n",
      "\n",
      "\n",
      "at random state 16, training accuracy is 0.6890550975461227\n",
      "at random state 16, testing accuracy is 0.66424097125918\n",
      "at random state 16, mean squared error is 0.04905769088292126\n",
      "at random state 16, mean absolute error is 0.16739007689354646\n",
      "\n",
      "\n",
      "at random state 17, training accuracy is 0.690311348670126\n",
      "at random state 17, testing accuracy is 0.6680499341435195\n",
      "at random state 17, mean squared error is 0.04827156705604965\n",
      "at random state 17, mean absolute error is 0.16741789711929342\n",
      "\n",
      "\n",
      "at random state 18, training accuracy is 0.6859189788563171\n",
      "at random state 18, testing accuracy is 0.6786302976261969\n",
      "at random state 18, mean squared error is 0.048101180143334296\n",
      "at random state 18, mean absolute error is 0.1673033385600331\n",
      "\n",
      "\n",
      "at random state 19, training accuracy is 0.6904605293803063\n",
      "at random state 19, testing accuracy is 0.6681656165658162\n",
      "at random state 19, mean squared error is 0.04873982877189438\n",
      "at random state 19, mean absolute error is 0.16853996450141243\n",
      "\n",
      "\n",
      "at random state 20, training accuracy is 0.6872221283414744\n",
      "at random state 20, testing accuracy is 0.6694499292218389\n",
      "at random state 20, mean squared error is 0.04968013572118008\n",
      "at random state 20, mean absolute error is 0.16886180875641055\n",
      "\n",
      "\n",
      "at random state 21, training accuracy is 0.6896518666528133\n",
      "at random state 21, testing accuracy is 0.6632812406664632\n",
      "at random state 21, mean squared error is 0.05070865288502501\n",
      "at random state 21, mean absolute error is 0.17192520959941493\n",
      "\n",
      "\n",
      "at random state 22, training accuracy is 0.6893251296295105\n",
      "at random state 22, testing accuracy is 0.6642450449695037\n",
      "at random state 22, mean squared error is 0.04957821188005273\n",
      "at random state 22, mean absolute error is 0.16967640027703504\n",
      "\n",
      "\n",
      "at random state 23, training accuracy is 0.6856194136909213\n",
      "at random state 23, testing accuracy is 0.6788026373247592\n",
      "at random state 23, mean squared error is 0.0483259422741606\n",
      "at random state 23, mean absolute error is 0.16746711971388423\n",
      "\n",
      "\n",
      "at random state 24, training accuracy is 0.6941148676776897\n",
      "at random state 24, testing accuracy is 0.6605578813392878\n",
      "at random state 24, mean squared error is 0.04908847669952679\n",
      "at random state 24, mean absolute error is 0.16658470527940802\n",
      "\n",
      "\n",
      "at random state 25, training accuracy is 0.6886242888042688\n",
      "at random state 25, testing accuracy is 0.6688089817487605\n",
      "at random state 25, mean squared error is 0.04974152151190438\n",
      "at random state 25, mean absolute error is 0.16997906745387487\n",
      "\n",
      "\n",
      "at random state 26, training accuracy is 0.6839505820632477\n",
      "at random state 26, testing accuracy is 0.6736548940920639\n",
      "at random state 26, mean squared error is 0.050046789658887945\n",
      "at random state 26, mean absolute error is 0.169382978673989\n",
      "\n",
      "\n",
      "at random state 27, training accuracy is 0.6889882636003729\n",
      "at random state 27, testing accuracy is 0.6646398164082477\n",
      "at random state 27, mean squared error is 0.04949734145331227\n",
      "at random state 27, mean absolute error is 0.1690756170062069\n",
      "\n",
      "\n",
      "at random state 28, training accuracy is 0.6819516225268547\n",
      "at random state 28, testing accuracy is 0.6773925869252895\n",
      "at random state 28, mean squared error is 0.04807288734739277\n",
      "at random state 28, mean absolute error is 0.16684888640177278\n",
      "\n",
      "\n",
      "at random state 29, training accuracy is 0.684851565299595\n",
      "at random state 29, testing accuracy is 0.6769438013494066\n",
      "at random state 29, mean squared error is 0.04909877682057462\n",
      "at random state 29, mean absolute error is 0.16887422867141863\n",
      "\n",
      "\n",
      "at random state 30, training accuracy is 0.693134807180594\n",
      "at random state 30, testing accuracy is 0.6640335648079524\n",
      "at random state 30, mean squared error is 0.04897192206548303\n",
      "at random state 30, mean absolute error is 0.16820369800504073\n",
      "\n",
      "\n",
      "at random state 31, training accuracy is 0.6879759081223806\n",
      "at random state 31, testing accuracy is 0.6789252462379107\n",
      "at random state 31, mean squared error is 0.048507508016336194\n",
      "at random state 31, mean absolute error is 0.16599422054953292\n",
      "\n",
      "\n",
      "at random state 32, training accuracy is 0.6863134946227529\n",
      "at random state 32, testing accuracy is 0.6743856154293708\n",
      "at random state 32, mean squared error is 0.0488411876460727\n",
      "at random state 32, mean absolute error is 0.16909196647985353\n",
      "\n",
      "\n",
      "at random state 33, training accuracy is 0.6898288194891309\n",
      "at random state 33, testing accuracy is 0.6715920783981641\n",
      "at random state 33, mean squared error is 0.04717453842589868\n",
      "at random state 33, mean absolute error is 0.165713973966992\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 34, training accuracy is 0.6859665543966134\n",
      "at random state 34, testing accuracy is 0.6649095163103902\n",
      "at random state 34, mean squared error is 0.05034776205399432\n",
      "at random state 34, mean absolute error is 0.16987539981125246\n",
      "\n",
      "\n",
      "at random state 35, training accuracy is 0.6852134226839426\n",
      "at random state 35, testing accuracy is 0.6705370056315824\n",
      "at random state 35, mean squared error is 0.04862171222928789\n",
      "at random state 35, mean absolute error is 0.1688893108497296\n",
      "\n",
      "\n",
      "at random state 36, training accuracy is 0.6940654721442732\n",
      "at random state 36, testing accuracy is 0.6650217865417465\n",
      "at random state 36, mean squared error is 0.04938677124466523\n",
      "at random state 36, mean absolute error is 0.16873669077657782\n",
      "\n",
      "\n",
      "at random state 37, training accuracy is 0.6867013887145501\n",
      "at random state 37, testing accuracy is 0.6738922501422111\n",
      "at random state 37, mean squared error is 0.04901068635056112\n",
      "at random state 37, mean absolute error is 0.16766588288543138\n",
      "\n",
      "\n",
      "at random state 38, training accuracy is 0.6844408598809022\n",
      "at random state 38, testing accuracy is 0.6717236694541737\n",
      "at random state 38, mean squared error is 0.049231956030007275\n",
      "at random state 38, mean absolute error is 0.1683295111734422\n",
      "\n",
      "\n",
      "at random state 39, training accuracy is 0.6865532577108598\n",
      "at random state 39, testing accuracy is 0.667902818652593\n",
      "at random state 39, mean squared error is 0.04858703523010831\n",
      "at random state 39, mean absolute error is 0.1694334795044126\n",
      "\n",
      "\n",
      "at random state 40, training accuracy is 0.6883080789691987\n",
      "at random state 40, testing accuracy is 0.6814537596279046\n",
      "at random state 40, mean squared error is 0.04703716530239996\n",
      "at random state 40, mean absolute error is 0.16420351171454006\n",
      "\n",
      "\n",
      "at random state 41, training accuracy is 0.6851796704801432\n",
      "at random state 41, testing accuracy is 0.6739666568076914\n",
      "at random state 41, mean squared error is 0.04814401896977813\n",
      "at random state 41, mean absolute error is 0.16699771087842016\n",
      "\n",
      "\n",
      "at random state 42, training accuracy is 0.6884346336640812\n",
      "at random state 42, testing accuracy is 0.666061098890826\n",
      "at random state 42, mean squared error is 0.04975933044744049\n",
      "at random state 42, mean absolute error is 0.17027464515168797\n",
      "\n",
      "\n",
      "at random state 43, training accuracy is 0.6858405118959507\n",
      "at random state 43, testing accuracy is 0.6734502633539748\n",
      "at random state 43, mean squared error is 0.049200623956978135\n",
      "at random state 43, mean absolute error is 0.1679685142665633\n",
      "\n",
      "\n",
      "at random state 44, training accuracy is 0.6890819246425437\n",
      "at random state 44, testing accuracy is 0.6571443120415302\n",
      "at random state 44, mean squared error is 0.05134832017297729\n",
      "at random state 44, mean absolute error is 0.17140218940673288\n",
      "\n",
      "\n",
      "at random state 45, training accuracy is 0.6868517999369017\n",
      "at random state 45, testing accuracy is 0.6749907888637093\n",
      "at random state 45, mean squared error is 0.04827256915358152\n",
      "at random state 45, mean absolute error is 0.16754775597111451\n",
      "\n",
      "\n",
      "at random state 46, training accuracy is 0.6821885990891028\n",
      "at random state 46, testing accuracy is 0.6773837921954544\n",
      "at random state 46, mean squared error is 0.04865887582881673\n",
      "at random state 46, mean absolute error is 0.16787916422267776\n",
      "\n",
      "\n",
      "at random state 47, training accuracy is 0.6895929225587196\n",
      "at random state 47, testing accuracy is 0.6678986519339623\n",
      "at random state 47, mean squared error is 0.04889992469957245\n",
      "at random state 47, mean absolute error is 0.16727677425221643\n",
      "\n",
      "\n",
      "at random state 48, training accuracy is 0.6915478429179203\n",
      "at random state 48, testing accuracy is 0.669066169575595\n",
      "at random state 48, mean squared error is 0.04966190703624407\n",
      "at random state 48, mean absolute error is 0.169372767451632\n",
      "\n",
      "\n",
      "at random state 49, training accuracy is 0.6912184546494333\n",
      "at random state 49, testing accuracy is 0.6720849027157897\n",
      "at random state 49, mean squared error is 0.049125254114905495\n",
      "at random state 49, mean absolute error is 0.1681484559267737\n",
      "\n",
      "\n",
      "at random state 50, training accuracy is 0.6899474782771681\n",
      "at random state 50, testing accuracy is 0.6688767932484696\n",
      "at random state 50, mean squared error is 0.048872784140328314\n",
      "at random state 50, mean absolute error is 0.16946482528723317\n",
      "\n",
      "\n",
      "at random state 51, training accuracy is 0.69033569468376\n",
      "at random state 51, testing accuracy is 0.6644204142128733\n",
      "at random state 51, mean squared error is 0.05064151202167995\n",
      "at random state 51, mean absolute error is 0.17163640515090864\n",
      "\n",
      "\n",
      "at random state 52, training accuracy is 0.6862365659888146\n",
      "at random state 52, testing accuracy is 0.6703494243927215\n",
      "at random state 52, mean squared error is 0.049783084569922045\n",
      "at random state 52, mean absolute error is 0.1693222203031981\n",
      "\n",
      "\n",
      "at random state 53, training accuracy is 0.6920783536916502\n",
      "at random state 53, testing accuracy is 0.6718489656685375\n",
      "at random state 53, mean squared error is 0.04885618411875009\n",
      "at random state 53, mean absolute error is 0.16762205082046955\n",
      "\n",
      "\n",
      "at random state 54, training accuracy is 0.6892018309506209\n",
      "at random state 54, testing accuracy is 0.6656589184373729\n",
      "at random state 54, mean squared error is 0.04967940992668656\n",
      "at random state 54, mean absolute error is 0.1682569333271583\n",
      "\n",
      "\n",
      "at random state 55, training accuracy is 0.6895644869540785\n",
      "at random state 55, testing accuracy is 0.6665848774808198\n",
      "at random state 55, mean squared error is 0.04969481755355765\n",
      "at random state 55, mean absolute error is 0.16925171888233764\n",
      "\n",
      "\n",
      "at random state 56, training accuracy is 0.6849246095120327\n",
      "at random state 56, testing accuracy is 0.6771185776448494\n",
      "at random state 56, mean squared error is 0.047370980749949526\n",
      "at random state 56, mean absolute error is 0.16699831512293178\n",
      "\n",
      "\n",
      "at random state 57, training accuracy is 0.6882099582875838\n",
      "at random state 57, testing accuracy is 0.6729908437024615\n",
      "at random state 57, mean squared error is 0.049441603901362985\n",
      "at random state 57, mean absolute error is 0.16927430613400737\n",
      "\n",
      "\n",
      "at random state 58, training accuracy is 0.6888749613577079\n",
      "at random state 58, testing accuracy is 0.6703749345412153\n",
      "at random state 58, mean squared error is 0.04913755577989426\n",
      "at random state 58, mean absolute error is 0.1689821348864909\n",
      "\n",
      "\n",
      "at random state 59, training accuracy is 0.6940570879668464\n",
      "at random state 59, testing accuracy is 0.6653193720937042\n",
      "at random state 59, mean squared error is 0.048462567324789735\n",
      "at random state 59, mean absolute error is 0.16761832884820788\n",
      "\n",
      "\n",
      "at random state 60, training accuracy is 0.6834750880898928\n",
      "at random state 60, testing accuracy is 0.6720973607228646\n",
      "at random state 60, mean squared error is 0.04992812895242311\n",
      "at random state 60, mean absolute error is 0.17009984489905142\n",
      "\n",
      "\n",
      "at random state 61, training accuracy is 0.6894984275188445\n",
      "at random state 61, testing accuracy is 0.6633067736727409\n",
      "at random state 61, mean squared error is 0.05097714724147289\n",
      "at random state 61, mean absolute error is 0.1704808461178463\n",
      "\n",
      "\n",
      "at random state 62, training accuracy is 0.6873191260546352\n",
      "at random state 62, testing accuracy is 0.6731575511795053\n",
      "at random state 62, mean squared error is 0.04875428379261679\n",
      "at random state 62, mean absolute error is 0.16802792662974894\n",
      "\n",
      "\n",
      "at random state 63, training accuracy is 0.6880543448400392\n",
      "at random state 63, testing accuracy is 0.6716408933166984\n",
      "at random state 63, mean squared error is 0.05004456982281495\n",
      "at random state 63, mean absolute error is 0.17043637763914612\n",
      "\n",
      "\n",
      "at random state 64, training accuracy is 0.6890837751638164\n",
      "at random state 64, testing accuracy is 0.6683953494295028\n",
      "at random state 64, mean squared error is 0.04947615982382299\n",
      "at random state 64, mean absolute error is 0.1688781689919797\n",
      "\n",
      "\n",
      "at random state 65, training accuracy is 0.687413419449938\n",
      "at random state 65, testing accuracy is 0.6744646697754793\n",
      "at random state 65, mean squared error is 0.04812431871890632\n",
      "at random state 65, mean absolute error is 0.16758772011747528\n",
      "\n",
      "\n",
      "at random state 66, training accuracy is 0.6899466667942016\n",
      "at random state 66, testing accuracy is 0.6584705230918471\n",
      "at random state 66, mean squared error is 0.04992627930073727\n",
      "at random state 66, mean absolute error is 0.1695011112447534\n",
      "\n",
      "\n",
      "at random state 67, training accuracy is 0.6856710662033945\n",
      "at random state 67, testing accuracy is 0.6645895510262696\n",
      "at random state 67, mean squared error is 0.049337312049689254\n",
      "at random state 67, mean absolute error is 0.16839771946054727\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 68, training accuracy is 0.692370914648881\n",
      "at random state 68, testing accuracy is 0.6655601165517253\n",
      "at random state 68, mean squared error is 0.04913998228460063\n",
      "at random state 68, mean absolute error is 0.16809215224026894\n",
      "\n",
      "\n",
      "at random state 69, training accuracy is 0.6875953239869408\n",
      "at random state 69, testing accuracy is 0.668284711289578\n",
      "at random state 69, mean squared error is 0.050200433587831446\n",
      "at random state 69, mean absolute error is 0.17110341925379957\n",
      "\n",
      "\n",
      "at random state 70, training accuracy is 0.6903212949360296\n",
      "at random state 70, testing accuracy is 0.6624763037336019\n",
      "at random state 70, mean squared error is 0.05184383972073484\n",
      "at random state 70, mean absolute error is 0.17334747472475906\n",
      "\n",
      "\n",
      "at random state 71, training accuracy is 0.6912558627785215\n",
      "at random state 71, testing accuracy is 0.6611977612493634\n",
      "at random state 71, mean squared error is 0.05044865131306787\n",
      "at random state 71, mean absolute error is 0.17176193894413463\n",
      "\n",
      "\n",
      "at random state 72, training accuracy is 0.6911639117684603\n",
      "at random state 72, testing accuracy is 0.6660180742050414\n",
      "at random state 72, mean squared error is 0.05013920907122217\n",
      "at random state 72, mean absolute error is 0.16907913136959238\n",
      "\n",
      "\n",
      "at random state 73, training accuracy is 0.6904131775517501\n",
      "at random state 73, testing accuracy is 0.6705876255347623\n",
      "at random state 73, mean squared error is 0.048555802443000284\n",
      "at random state 73, mean absolute error is 0.16814700634356425\n",
      "\n",
      "\n",
      "at random state 74, training accuracy is 0.6870194812600392\n",
      "at random state 74, testing accuracy is 0.6636048371703825\n",
      "at random state 74, mean squared error is 0.049774803215311574\n",
      "at random state 74, mean absolute error is 0.17133257424896126\n",
      "\n",
      "\n",
      "at random state 75, training accuracy is 0.6833928899265014\n",
      "at random state 75, testing accuracy is 0.6817010482843993\n",
      "at random state 75, mean squared error is 0.047833607286770774\n",
      "at random state 75, mean absolute error is 0.1669053556832066\n",
      "\n",
      "\n",
      "at random state 76, training accuracy is 0.6876371419234157\n",
      "at random state 76, testing accuracy is 0.6793449739146331\n",
      "at random state 76, mean squared error is 0.04689190534867615\n",
      "at random state 76, mean absolute error is 0.16559425956407742\n",
      "\n",
      "\n",
      "at random state 77, training accuracy is 0.6852227106575424\n",
      "at random state 77, testing accuracy is 0.6787577608025757\n",
      "at random state 77, mean squared error is 0.04749779657709401\n",
      "at random state 77, mean absolute error is 0.16452147659383223\n",
      "\n",
      "\n",
      "at random state 78, training accuracy is 0.6882387180257693\n",
      "at random state 78, testing accuracy is 0.674160780021592\n",
      "at random state 78, mean squared error is 0.04931659952640514\n",
      "at random state 78, mean absolute error is 0.1687409009766852\n",
      "\n",
      "\n",
      "at random state 79, training accuracy is 0.689535643619124\n",
      "at random state 79, testing accuracy is 0.6824638637880793\n",
      "at random state 79, mean squared error is 0.047687382407890375\n",
      "at random state 79, mean absolute error is 0.16635151695975514\n",
      "\n",
      "\n",
      "at random state 80, training accuracy is 0.6890745006538384\n",
      "at random state 80, testing accuracy is 0.67164849663222\n",
      "at random state 80, mean squared error is 0.047438922902058665\n",
      "at random state 80, mean absolute error is 0.16621878378045896\n",
      "\n",
      "\n",
      "at random state 81, training accuracy is 0.6865379616663463\n",
      "at random state 81, testing accuracy is 0.6658865077867391\n",
      "at random state 81, mean squared error is 0.050396748295694564\n",
      "at random state 81, mean absolute error is 0.17152268698950948\n",
      "\n",
      "\n",
      "at random state 82, training accuracy is 0.6910093253422076\n",
      "at random state 82, testing accuracy is 0.6707283020817705\n",
      "at random state 82, mean squared error is 0.04905702910062986\n",
      "at random state 82, mean absolute error is 0.1676427200946279\n",
      "\n",
      "\n",
      "at random state 83, training accuracy is 0.6885561404436833\n",
      "at random state 83, testing accuracy is 0.6751423909314063\n",
      "at random state 83, mean squared error is 0.04771535154428204\n",
      "at random state 83, mean absolute error is 0.16632444365711607\n",
      "\n",
      "\n",
      "at random state 84, training accuracy is 0.6883838959208788\n",
      "at random state 84, testing accuracy is 0.6790238118773435\n",
      "at random state 84, mean squared error is 0.04690882559701\n",
      "at random state 84, mean absolute error is 0.16520477657172158\n",
      "\n",
      "\n",
      "at random state 85, training accuracy is 0.6870078284107695\n",
      "at random state 85, testing accuracy is 0.6695535616227667\n",
      "at random state 85, mean squared error is 0.05168434513317648\n",
      "at random state 85, mean absolute error is 0.17233477032277483\n",
      "\n",
      "\n",
      "at random state 86, training accuracy is 0.6858344614605315\n",
      "at random state 86, testing accuracy is 0.6728066541276602\n",
      "at random state 86, mean squared error is 0.049673356487018976\n",
      "at random state 86, mean absolute error is 0.16997812503257648\n",
      "\n",
      "\n",
      "at random state 87, training accuracy is 0.6841380253319518\n",
      "at random state 87, testing accuracy is 0.6629612665134936\n",
      "at random state 87, mean squared error is 0.05052834824698577\n",
      "at random state 87, mean absolute error is 0.17080845125867458\n",
      "\n",
      "\n",
      "at random state 88, training accuracy is 0.6902689323681526\n",
      "at random state 88, testing accuracy is 0.6679274139984048\n",
      "at random state 88, mean squared error is 0.04980237725278476\n",
      "at random state 88, mean absolute error is 0.17015541042594942\n",
      "\n",
      "\n",
      "at random state 89, training accuracy is 0.6857661732914755\n",
      "at random state 89, testing accuracy is 0.6717460052869132\n",
      "at random state 89, mean squared error is 0.049110518727083705\n",
      "at random state 89, mean absolute error is 0.16941881544282844\n",
      "\n",
      "\n",
      "at random state 90, training accuracy is 0.6960889951944604\n",
      "at random state 90, testing accuracy is 0.6576909553226036\n",
      "at random state 90, mean squared error is 0.050053610769014496\n",
      "at random state 90, mean absolute error is 0.17058572188958102\n",
      "\n",
      "\n",
      "at random state 91, training accuracy is 0.6910261947975245\n",
      "at random state 91, testing accuracy is 0.6670367932151596\n",
      "at random state 91, mean squared error is 0.0492068437135596\n",
      "at random state 91, mean absolute error is 0.16836568980633448\n",
      "\n",
      "\n",
      "at random state 92, training accuracy is 0.6883747949280352\n",
      "at random state 92, testing accuracy is 0.6701263306728984\n",
      "at random state 92, mean squared error is 0.047857670168271055\n",
      "at random state 92, mean absolute error is 0.16681345503873923\n",
      "\n",
      "\n",
      "at random state 93, training accuracy is 0.6938524887098566\n",
      "at random state 93, testing accuracy is 0.6611643009327142\n",
      "at random state 93, mean squared error is 0.04999592122239271\n",
      "at random state 93, mean absolute error is 0.16926550866681875\n",
      "\n",
      "\n",
      "at random state 94, training accuracy is 0.6953786377380484\n",
      "at random state 94, testing accuracy is 0.6576497703283105\n",
      "at random state 94, mean squared error is 0.05011523776308739\n",
      "at random state 94, mean absolute error is 0.16988715745846675\n",
      "\n",
      "\n",
      "at random state 95, training accuracy is 0.6868587343034527\n",
      "at random state 95, testing accuracy is 0.6771309224199429\n",
      "at random state 95, mean squared error is 0.048441228084562526\n",
      "at random state 95, mean absolute error is 0.1680123298255816\n",
      "\n",
      "\n",
      "at random state 96, training accuracy is 0.6850588553704882\n",
      "at random state 96, testing accuracy is 0.6720782547860837\n",
      "at random state 96, mean squared error is 0.05008467348762077\n",
      "at random state 96, mean absolute error is 0.17008431881228037\n",
      "\n",
      "\n",
      "at random state 97, training accuracy is 0.6840271293096014\n",
      "at random state 97, testing accuracy is 0.6681749608438841\n",
      "at random state 97, mean squared error is 0.05024935116155131\n",
      "at random state 97, mean absolute error is 0.17034044756024302\n",
      "\n",
      "\n",
      "at random state 98, training accuracy is 0.6928810115355437\n",
      "at random state 98, testing accuracy is 0.6566684567421239\n",
      "at random state 98, mean squared error is 0.05022541835203133\n",
      "at random state 98, mean absolute error is 0.16941376770430594\n",
      "\n",
      "\n",
      "at random state 99, training accuracy is 0.6885921476429232\n",
      "at random state 99, testing accuracy is 0.6633805269068551\n",
      "at random state 99, mean squared error is 0.048479874475373316\n",
      "at random state 99, mean absolute error is 0.16630242013821359\n",
      "\n",
      "\n",
      "Max accuracy at random state 6 = 0.6828760045118929\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gb = GradientBoostingRegressor()\n",
    "model_selection(gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8f87ba3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.21251605 0.3942506  0.41396247 0.24574424 0.64598232 0.29593615\n",
      " 0.4400672  0.30594594 0.17623748]\n",
      "0.3478491617571897\n",
      "0.1360365488820587\n"
     ]
    }
   ],
   "source": [
    "score=cross_val_score(gb,dfx,y,cv=9)\n",
    "print(score)\n",
    "print(score.mean())\n",
    "print(score.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f856b2c",
   "metadata": {},
   "source": [
    "# RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "8cffc7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 0, training accuracy is 0.973789916911575\n",
      "at random state 0, testing accuracy is 0.8227015063439409\n",
      "at random state 0, mean squared error is 0.024096888521346758\n",
      "at random state 0, mean absolute error is 0.11201558486636586\n",
      "\n",
      "\n",
      "at random state 1, training accuracy is 0.9746853816446602\n",
      "at random state 1, testing accuracy is 0.8286791558418365\n",
      "at random state 1, mean squared error is 0.023042384585213474\n",
      "at random state 1, mean absolute error is 0.10951166261714683\n",
      "\n",
      "\n",
      "at random state 2, training accuracy is 0.9744212457988305\n",
      "at random state 2, testing accuracy is 0.8197435402603348\n",
      "at random state 2, mean squared error is 0.024752525043387708\n",
      "at random state 2, mean absolute error is 0.11185328011107253\n",
      "\n",
      "\n",
      "at random state 3, training accuracy is 0.9742985025495625\n",
      "at random state 3, testing accuracy is 0.8243541674292205\n",
      "at random state 3, mean squared error is 0.02424778710517182\n",
      "at random state 3, mean absolute error is 0.11244828184658105\n",
      "\n",
      "\n",
      "at random state 4, training accuracy is 0.9738033052838956\n",
      "at random state 4, testing accuracy is 0.8307147510134614\n",
      "at random state 4, mean squared error is 0.02324638357861853\n",
      "at random state 4, mean absolute error is 0.11029187782020132\n",
      "\n",
      "\n",
      "at random state 5, training accuracy is 0.9750274703250642\n",
      "at random state 5, testing accuracy is 0.8103226545438653\n",
      "at random state 5, mean squared error is 0.02597339572370705\n",
      "at random state 5, mean absolute error is 0.11471961124609509\n",
      "\n",
      "\n",
      "at random state 6, training accuracy is 0.9745513081602143\n",
      "at random state 6, testing accuracy is 0.8187608303361777\n",
      "at random state 6, mean squared error is 0.02488679487677889\n",
      "at random state 6, mean absolute error is 0.11218493578618534\n",
      "\n",
      "\n",
      "at random state 7, training accuracy is 0.9736827480636462\n",
      "at random state 7, testing accuracy is 0.8279267785158387\n",
      "at random state 7, mean squared error is 0.023222367511280807\n",
      "at random state 7, mean absolute error is 0.10987094758764318\n",
      "\n",
      "\n",
      "at random state 8, training accuracy is 0.9746185122162236\n",
      "at random state 8, testing accuracy is 0.8156106877722388\n",
      "at random state 8, mean squared error is 0.024940757625824365\n",
      "at random state 8, mean absolute error is 0.11247511280805275\n",
      "\n",
      "\n",
      "at random state 9, training accuracy is 0.9749789204437493\n",
      "at random state 9, testing accuracy is 0.7993956110730827\n",
      "at random state 9, mean squared error is 0.02628654466851787\n",
      "at random state 9, mean absolute error is 0.11563641096841375\n",
      "\n",
      "\n",
      "at random state 10, training accuracy is 0.9746703513019931\n",
      "at random state 10, testing accuracy is 0.8207245434500192\n",
      "at random state 10, mean squared error is 0.02387731423464076\n",
      "at random state 10, mean absolute error is 0.11061961124609511\n",
      "\n",
      "\n",
      "at random state 11, training accuracy is 0.9737815493897833\n",
      "at random state 11, testing accuracy is 0.8201474170139719\n",
      "at random state 11, mean squared error is 0.024238729927108647\n",
      "at random state 11, mean absolute error is 0.11120881638320028\n",
      "\n",
      "\n",
      "at random state 12, training accuracy is 0.9747483623360638\n",
      "at random state 12, testing accuracy is 0.8002296910210468\n",
      "at random state 12, mean squared error is 0.025998299951405766\n",
      "at random state 12, mean absolute error is 0.1149590419993058\n",
      "\n",
      "\n",
      "at random state 13, training accuracy is 0.9738765300626854\n",
      "at random state 13, testing accuracy is 0.8259220434596288\n",
      "at random state 13, mean squared error is 0.02398685233946547\n",
      "at random state 13, mean absolute error is 0.11240308920513709\n",
      "\n",
      "\n",
      "at random state 14, training accuracy is 0.9748706899813544\n",
      "at random state 14, testing accuracy is 0.8231271397447334\n",
      "at random state 14, mean squared error is 0.02320134686567164\n",
      "at random state 14, mean absolute error is 0.10985157931273863\n",
      "\n",
      "\n",
      "at random state 15, training accuracy is 0.9744374353919958\n",
      "at random state 15, testing accuracy is 0.8253570747605795\n",
      "at random state 15, mean squared error is 0.02309464841374522\n",
      "at random state 15, mean absolute error is 0.10890301978479694\n",
      "\n",
      "\n",
      "at random state 16, training accuracy is 0.9740994113348721\n",
      "at random state 16, testing accuracy is 0.8301575772105196\n",
      "at random state 16, mean squared error is 0.02332854960430406\n",
      "at random state 16, mean absolute error is 0.11076664352655327\n",
      "\n",
      "\n",
      "at random state 17, training accuracy is 0.9742402670452438\n",
      "at random state 17, testing accuracy is 0.8324628065958806\n",
      "at random state 17, mean squared error is 0.023950982860118018\n",
      "at random state 17, mean absolute error is 0.11096299895869491\n",
      "\n",
      "\n",
      "at random state 18, training accuracy is 0.974589535139887\n",
      "at random state 18, testing accuracy is 0.825287526934378\n",
      "at random state 18, mean squared error is 0.024576827500867753\n",
      "at random state 18, mean absolute error is 0.11245182228392919\n",
      "\n",
      "\n",
      "at random state 19, training accuracy is 0.9745631900712669\n",
      "at random state 19, testing accuracy is 0.8302933202638958\n",
      "at random state 19, mean squared error is 0.022923373797292604\n",
      "at random state 19, mean absolute error is 0.10910513710517182\n",
      "\n",
      "\n",
      "at random state 20, training accuracy is 0.9747301350069468\n",
      "at random state 20, testing accuracy is 0.8188855667128602\n",
      "at random state 20, mean squared error is 0.02506363896563693\n",
      "at random state 20, mean absolute error is 0.11289461992363761\n",
      "\n",
      "\n",
      "at random state 21, training accuracy is 0.9742537006519024\n",
      "at random state 21, testing accuracy is 0.8152770033125454\n",
      "at random state 21, mean squared error is 0.02503797131204443\n",
      "at random state 21, mean absolute error is 0.114671745921555\n",
      "\n",
      "\n",
      "at random state 22, training accuracy is 0.9745006143786795\n",
      "at random state 22, testing accuracy is 0.8296603601031389\n",
      "at random state 22, mean squared error is 0.02342533709475877\n",
      "at random state 22, mean absolute error is 0.1101196459562652\n",
      "\n",
      "\n",
      "at random state 23, training accuracy is 0.9741798998990098\n",
      "at random state 23, testing accuracy is 0.8269235240812791\n",
      "at random state 23, mean squared error is 0.023645882922596322\n",
      "at random state 23, mean absolute error is 0.10994786532454008\n",
      "\n",
      "\n",
      "at random state 24, training accuracy is 0.9742726325437089\n",
      "at random state 24, testing accuracy is 0.8232046018986829\n",
      "at random state 24, mean squared error is 0.0239183615515446\n",
      "at random state 24, mean absolute error is 0.11193727872266575\n",
      "\n",
      "\n",
      "at random state 25, training accuracy is 0.9740124188126031\n",
      "at random state 25, testing accuracy is 0.8205725715665397\n",
      "at random state 25, mean squared error is 0.02442146972578966\n",
      "at random state 25, mean absolute error is 0.1126850399166956\n",
      "\n",
      "\n",
      "at random state 26, training accuracy is 0.9740526515668719\n",
      "at random state 26, testing accuracy is 0.8268174573546342\n",
      "at random state 26, mean squared error is 0.023033771853523084\n",
      "at random state 26, mean absolute error is 0.109129573064908\n",
      "\n",
      "\n",
      "at random state 27, training accuracy is 0.974335222940068\n",
      "at random state 27, testing accuracy is 0.8298168255201144\n",
      "at random state 27, mean squared error is 0.02335448919472406\n",
      "at random state 27, mean absolute error is 0.11254331829225964\n",
      "\n",
      "\n",
      "at random state 28, training accuracy is 0.9745447861555924\n",
      "at random state 28, testing accuracy is 0.8134909236652674\n",
      "at random state 28, mean squared error is 0.025369979902811524\n",
      "at random state 28, mean absolute error is 0.11533099618188129\n",
      "\n",
      "\n",
      "at random state 29, training accuracy is 0.9743030469475239\n",
      "at random state 29, testing accuracy is 0.8338781703187376\n",
      "at random state 29, mean squared error is 0.02272598637972926\n",
      "at random state 29, mean absolute error is 0.11066768483165568\n",
      "\n",
      "\n",
      "at random state 30, training accuracy is 0.9746566844600733\n",
      "at random state 30, testing accuracy is 0.8136725414646819\n",
      "at random state 30, mean squared error is 0.024835740562304753\n",
      "at random state 30, mean absolute error is 0.11343346060395698\n",
      "\n",
      "\n",
      "at random state 31, training accuracy is 0.9750136476149704\n",
      "at random state 31, testing accuracy is 0.8078994959678152\n",
      "at random state 31, mean squared error is 0.02630612308573412\n",
      "at random state 31, mean absolute error is 0.11342731690385281\n",
      "\n",
      "\n",
      "at random state 32, training accuracy is 0.9747218271769522\n",
      "at random state 32, testing accuracy is 0.8207282272892109\n",
      "at random state 32, mean squared error is 0.02341634009371747\n",
      "at random state 32, mean absolute error is 0.11009125303713989\n",
      "\n",
      "\n",
      "at random state 33, training accuracy is 0.9748760288169168\n",
      "at random state 33, testing accuracy is 0.8219676783987022\n",
      "at random state 33, mean squared error is 0.024623784404720583\n",
      "at random state 33, mean absolute error is 0.11195248177716069\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 34, training accuracy is 0.9743425497735778\n",
      "at random state 34, testing accuracy is 0.8118352454875946\n",
      "at random state 34, mean squared error is 0.02546485263450191\n",
      "at random state 34, mean absolute error is 0.11421242624088856\n",
      "\n",
      "\n",
      "at random state 35, training accuracy is 0.97471722036956\n",
      "at random state 35, testing accuracy is 0.8159688245882989\n",
      "at random state 35, mean squared error is 0.025117819448108297\n",
      "at random state 35, mean absolute error is 0.11363391183616797\n",
      "\n",
      "\n",
      "at random state 36, training accuracy is 0.9743996641785475\n",
      "at random state 36, testing accuracy is 0.8201349480527644\n",
      "at random state 36, mean squared error is 0.02439644949323153\n",
      "at random state 36, mean absolute error is 0.11234213814647695\n",
      "\n",
      "\n",
      "at random state 37, training accuracy is 0.97420934994658\n",
      "at random state 37, testing accuracy is 0.8088511597266935\n",
      "at random state 37, mean squared error is 0.025301509333564738\n",
      "at random state 37, mean absolute error is 0.11378642832349879\n",
      "\n",
      "\n",
      "at random state 38, training accuracy is 0.9741696529980801\n",
      "at random state 38, testing accuracy is 0.8270715315188505\n",
      "at random state 38, mean squared error is 0.024220131041305097\n",
      "at random state 38, mean absolute error is 0.1111980909406456\n",
      "\n",
      "\n",
      "at random state 39, training accuracy is 0.9740839933511813\n",
      "at random state 39, testing accuracy is 0.8166304308507444\n",
      "at random state 39, mean squared error is 0.024902068351266918\n",
      "at random state 39, mean absolute error is 0.11297480041652203\n",
      "\n",
      "\n",
      "at random state 40, training accuracy is 0.9745897827565316\n",
      "at random state 40, testing accuracy is 0.8165814191884724\n",
      "at random state 40, mean squared error is 0.025146034814300586\n",
      "at random state 40, mean absolute error is 0.11343832002776813\n",
      "\n",
      "\n",
      "at random state 41, training accuracy is 0.9743079426874045\n",
      "at random state 41, testing accuracy is 0.8242308784879288\n",
      "at random state 41, mean squared error is 0.023676052776813603\n",
      "at random state 41, mean absolute error is 0.1112432141617494\n",
      "\n",
      "\n",
      "at random state 42, training accuracy is 0.9744101589711679\n",
      "at random state 42, testing accuracy is 0.8284654652005525\n",
      "at random state 42, mean squared error is 0.02204073789656369\n",
      "at random state 42, mean absolute error is 0.10817827143353004\n",
      "\n",
      "\n",
      "at random state 43, training accuracy is 0.9743371129712559\n",
      "at random state 43, testing accuracy is 0.8225165831391164\n",
      "at random state 43, mean squared error is 0.023335850555362712\n",
      "at random state 43, mean absolute error is 0.11039642485248176\n",
      "\n",
      "\n",
      "at random state 44, training accuracy is 0.9741194992723737\n",
      "at random state 44, testing accuracy is 0.8218029118177048\n",
      "at random state 44, mean squared error is 0.024878521655675112\n",
      "at random state 44, mean absolute error is 0.1131676848316557\n",
      "\n",
      "\n",
      "at random state 45, training accuracy is 0.9744908376531843\n",
      "at random state 45, testing accuracy is 0.8124134836931769\n",
      "at random state 45, mean squared error is 0.02574811920166608\n",
      "at random state 45, mean absolute error is 0.1150905241235682\n",
      "\n",
      "\n",
      "at random state 46, training accuracy is 0.9745684736807276\n",
      "at random state 46, testing accuracy is 0.8242564389911681\n",
      "at random state 46, mean squared error is 0.024292409076709473\n",
      "at random state 46, mean absolute error is 0.1117227004512322\n",
      "\n",
      "\n",
      "at random state 47, training accuracy is 0.974380535102148\n",
      "at random state 47, testing accuracy is 0.8206221306022763\n",
      "at random state 47, mean squared error is 0.024012349986115945\n",
      "at random state 47, mean absolute error is 0.11176108989934054\n",
      "\n",
      "\n",
      "at random state 48, training accuracy is 0.9743402313383421\n",
      "at random state 48, testing accuracy is 0.8159056511859781\n",
      "at random state 48, mean squared error is 0.02478893680666435\n",
      "at random state 48, mean absolute error is 0.11399520999652898\n",
      "\n",
      "\n",
      "at random state 49, training accuracy is 0.9744024470856881\n",
      "at random state 49, testing accuracy is 0.8203250049715097\n",
      "at random state 49, mean squared error is 0.023562625109337033\n",
      "at random state 49, mean absolute error is 0.10938472752516484\n",
      "\n",
      "\n",
      "at random state 50, training accuracy is 0.9747122226976348\n",
      "at random state 50, testing accuracy is 0.8221861043499505\n",
      "at random state 50, mean squared error is 0.02353686044081916\n",
      "at random state 50, mean absolute error is 0.11194623394654633\n",
      "\n",
      "\n",
      "at random state 51, training accuracy is 0.9739311590877524\n",
      "at random state 51, testing accuracy is 0.8219104288125381\n",
      "at random state 51, mean squared error is 0.025845117729954875\n",
      "at random state 51, mean absolute error is 0.11500159666782367\n",
      "\n",
      "\n",
      "at random state 52, training accuracy is 0.9748879343882793\n",
      "at random state 52, testing accuracy is 0.818776243622396\n",
      "at random state 52, mean squared error is 0.02479446833738286\n",
      "at random state 52, mean absolute error is 0.11316827490454703\n",
      "\n",
      "\n",
      "at random state 53, training accuracy is 0.9749092082559347\n",
      "at random state 53, testing accuracy is 0.8153674763924774\n",
      "at random state 53, mean squared error is 0.02509397985421729\n",
      "at random state 53, mean absolute error is 0.1150812217979868\n",
      "\n",
      "\n",
      "at random state 54, training accuracy is 0.9742073345182265\n",
      "at random state 54, testing accuracy is 0.8301024383057145\n",
      "at random state 54, mean squared error is 0.022891289659840344\n",
      "at random state 54, mean absolute error is 0.10963606386671296\n",
      "\n",
      "\n",
      "at random state 55, training accuracy is 0.9742688369326177\n",
      "at random state 55, testing accuracy is 0.8185432169503898\n",
      "at random state 55, mean squared error is 0.025948956299895874\n",
      "at random state 55, mean absolute error is 0.11614245053800763\n",
      "\n",
      "\n",
      "at random state 56, training accuracy is 0.9742865552858714\n",
      "at random state 56, testing accuracy is 0.824030930751007\n",
      "at random state 56, mean squared error is 0.024079041669559178\n",
      "at random state 56, mean absolute error is 0.11229944463727874\n",
      "\n",
      "\n",
      "at random state 57, training accuracy is 0.9743266258077614\n",
      "at random state 57, testing accuracy is 0.8211074597139065\n",
      "at random state 57, mean squared error is 0.024407287073932665\n",
      "at random state 57, mean absolute error is 0.1119970843457133\n",
      "\n",
      "\n",
      "at random state 58, training accuracy is 0.9739013208678492\n",
      "at random state 58, testing accuracy is 0.8250791013365182\n",
      "at random state 58, mean squared error is 0.023220591055189168\n",
      "at random state 58, mean absolute error is 0.11078601180145782\n",
      "\n",
      "\n",
      "at random state 59, training accuracy is 0.9748341465257427\n",
      "at random state 59, testing accuracy is 0.8360764479352745\n",
      "at random state 59, mean squared error is 0.022793765654286702\n",
      "at random state 59, mean absolute error is 0.10912547726483861\n",
      "\n",
      "\n",
      "at random state 60, training accuracy is 0.9741306522678793\n",
      "at random state 60, testing accuracy is 0.8223387638074132\n",
      "at random state 60, mean squared error is 0.02365658576535925\n",
      "at random state 60, mean absolute error is 0.11043502256161054\n",
      "\n",
      "\n",
      "at random state 61, training accuracy is 0.9745423508677687\n",
      "at random state 61, testing accuracy is 0.7996665591215042\n",
      "at random state 61, mean squared error is 0.026610580753210676\n",
      "at random state 61, mean absolute error is 0.1142248871919472\n",
      "\n",
      "\n",
      "at random state 62, training accuracy is 0.9745418253051903\n",
      "at random state 62, testing accuracy is 0.8159452136712836\n",
      "at random state 62, mean squared error is 0.025028426424852483\n",
      "at random state 62, mean absolute error is 0.11330340159666782\n",
      "\n",
      "\n",
      "at random state 63, training accuracy is 0.9743538593368516\n",
      "at random state 63, testing accuracy is 0.8173070851515479\n",
      "at random state 63, mean squared error is 0.02455391211037834\n",
      "at random state 63, mean absolute error is 0.11437296077750782\n",
      "\n",
      "\n",
      "at random state 64, training accuracy is 0.9742568076733521\n",
      "at random state 64, testing accuracy is 0.8119521145065214\n",
      "at random state 64, mean squared error is 0.025231859993057968\n",
      "at random state 64, mean absolute error is 0.11293266227004514\n",
      "\n",
      "\n",
      "at random state 65, training accuracy is 0.9739301414823697\n",
      "at random state 65, testing accuracy is 0.8082039784780732\n",
      "at random state 65, mean squared error is 0.02628634665741062\n",
      "at random state 65, mean absolute error is 0.1162742103436307\n",
      "\n",
      "\n",
      "at random state 66, training accuracy is 0.9742966329616297\n",
      "at random state 66, testing accuracy is 0.8134725151023853\n",
      "at random state 66, mean squared error is 0.025522896494272824\n",
      "at random state 66, mean absolute error is 0.11452335994446373\n",
      "\n",
      "\n",
      "at random state 67, training accuracy is 0.9743743148709095\n",
      "at random state 67, testing accuracy is 0.8214939909754326\n",
      "at random state 67, mean squared error is 0.023182993370357504\n",
      "at random state 67, mean absolute error is 0.11055807011454355\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 68, training accuracy is 0.9744878502889164\n",
      "at random state 68, testing accuracy is 0.8182417526650738\n",
      "at random state 68, mean squared error is 0.025800015706351962\n",
      "at random state 68, mean absolute error is 0.11365362721277335\n",
      "\n",
      "\n",
      "at random state 69, training accuracy is 0.97421933347641\n",
      "at random state 69, testing accuracy is 0.8155113933868136\n",
      "at random state 69, mean squared error is 0.0253245965498091\n",
      "at random state 69, mean absolute error is 0.11265623047552933\n",
      "\n",
      "\n",
      "at random state 70, training accuracy is 0.9745696754969019\n",
      "at random state 70, testing accuracy is 0.8251525650987632\n",
      "at random state 70, mean squared error is 0.02430435199583478\n",
      "at random state 70, mean absolute error is 0.11212113849357862\n",
      "\n",
      "\n",
      "at random state 71, training accuracy is 0.9753039037806454\n",
      "at random state 71, testing accuracy is 0.8055874362477364\n",
      "at random state 71, mean squared error is 0.026549530881638316\n",
      "at random state 71, mean absolute error is 0.11552735161402289\n",
      "\n",
      "\n",
      "at random state 72, training accuracy is 0.9746825116697327\n",
      "at random state 72, testing accuracy is 0.8158498614438854\n",
      "at random state 72, mean squared error is 0.02534208273863242\n",
      "at random state 72, mean absolute error is 0.11312193682749047\n",
      "\n",
      "\n",
      "at random state 73, training accuracy is 0.9740166980886665\n",
      "at random state 73, testing accuracy is 0.8287706189543289\n",
      "at random state 73, mean squared error is 0.02287526328705311\n",
      "at random state 73, mean absolute error is 0.11144626865671639\n",
      "\n",
      "\n",
      "at random state 74, training accuracy is 0.9747908245525223\n",
      "at random state 74, testing accuracy is 0.8224680486679639\n",
      "at random state 74, mean squared error is 0.024062721058660183\n",
      "at random state 74, mean absolute error is 0.11098469281499479\n",
      "\n",
      "\n",
      "at random state 75, training accuracy is 0.9745630613457433\n",
      "at random state 75, testing accuracy is 0.8203088776715869\n",
      "at random state 75, mean squared error is 0.024022700072891357\n",
      "at random state 75, mean absolute error is 0.11117393266227005\n",
      "\n",
      "\n",
      "at random state 76, training accuracy is 0.9745266466683112\n",
      "at random state 76, testing accuracy is 0.8242614960515297\n",
      "at random state 76, mean squared error is 0.024284749389101\n",
      "at random state 76, mean absolute error is 0.1114082263103089\n",
      "\n",
      "\n",
      "at random state 77, training accuracy is 0.9746746641770199\n",
      "at random state 77, testing accuracy is 0.8090207381789158\n",
      "at random state 77, mean squared error is 0.026148734307532118\n",
      "at random state 77, mean absolute error is 0.1148581742450538\n",
      "\n",
      "\n",
      "at random state 78, training accuracy is 0.9746428904293777\n",
      "at random state 78, testing accuracy is 0.819744919362926\n",
      "at random state 78, mean squared error is 0.024478704949670242\n",
      "at random state 78, mean absolute error is 0.112077403679278\n",
      "\n",
      "\n",
      "at random state 79, training accuracy is 0.9742013368730721\n",
      "at random state 79, testing accuracy is 0.8117938784293959\n",
      "at random state 79, mean squared error is 0.025752764880249912\n",
      "at random state 79, mean absolute error is 0.11517896563693163\n",
      "\n",
      "\n",
      "at random state 80, training accuracy is 0.9744090409882141\n",
      "at random state 80, testing accuracy is 0.8130969183721425\n",
      "at random state 80, mean squared error is 0.024629573380770566\n",
      "at random state 80, mean absolute error is 0.11055636931620964\n",
      "\n",
      "\n",
      "at random state 81, training accuracy is 0.9743393470185558\n",
      "at random state 81, testing accuracy is 0.8121925411912164\n",
      "at random state 81, mean squared error is 0.02609692663658451\n",
      "at random state 81, mean absolute error is 0.11481402290871226\n",
      "\n",
      "\n",
      "at random state 82, training accuracy is 0.9748116857553555\n",
      "at random state 82, testing accuracy is 0.8129217987039652\n",
      "at random state 82, mean squared error is 0.02458097213120445\n",
      "at random state 82, mean absolute error is 0.11136657410621315\n",
      "\n",
      "\n",
      "at random state 83, training accuracy is 0.9747611586940403\n",
      "at random state 83, testing accuracy is 0.8196955364380375\n",
      "at random state 83, mean squared error is 0.025164643575147524\n",
      "at random state 83, mean absolute error is 0.1126602568552586\n",
      "\n",
      "\n",
      "at random state 84, training accuracy is 0.9746459864940998\n",
      "at random state 84, testing accuracy is 0.8168110915563958\n",
      "at random state 84, mean squared error is 0.0251240567545991\n",
      "at random state 84, mean absolute error is 0.11235067684831658\n",
      "\n",
      "\n",
      "at random state 85, training accuracy is 0.9747049471852258\n",
      "at random state 85, testing accuracy is 0.8148378154655893\n",
      "at random state 85, mean squared error is 0.024996264529677188\n",
      "at random state 85, mean absolute error is 0.1138271780631725\n",
      "\n",
      "\n",
      "at random state 86, training accuracy is 0.9747435371495513\n",
      "at random state 86, testing accuracy is 0.8116447720821305\n",
      "at random state 86, mean squared error is 0.025792642318639356\n",
      "at random state 86, mean absolute error is 0.11261256508156889\n",
      "\n",
      "\n",
      "at random state 87, training accuracy is 0.9747374256417436\n",
      "at random state 87, testing accuracy is 0.8128084954103425\n",
      "at random state 87, mean squared error is 0.024998071037834073\n",
      "at random state 87, mean absolute error is 0.11324901076015269\n",
      "\n",
      "\n",
      "at random state 88, training accuracy is 0.974984668026751\n",
      "at random state 88, testing accuracy is 0.8063175728135422\n",
      "at random state 88, mean squared error is 0.02704636157931275\n",
      "at random state 88, mean absolute error is 0.11617955570982298\n",
      "\n",
      "\n",
      "at random state 89, training accuracy is 0.9743173373521988\n",
      "at random state 89, testing accuracy is 0.8271136360677926\n",
      "at random state 89, mean squared error is 0.02416218090940645\n",
      "at random state 89, mean absolute error is 0.11140229087122525\n",
      "\n",
      "\n",
      "at random state 90, training accuracy is 0.9742674545824097\n",
      "at random state 90, testing accuracy is 0.8305513854298788\n",
      "at random state 90, mean squared error is 0.02360456123915306\n",
      "at random state 90, mean absolute error is 0.11067424505380073\n",
      "\n",
      "\n",
      "at random state 91, training accuracy is 0.9747401592380338\n",
      "at random state 91, testing accuracy is 0.8185624740151676\n",
      "at random state 91, mean squared error is 0.024192092301284274\n",
      "at random state 91, mean absolute error is 0.11127053106560221\n",
      "\n",
      "\n",
      "at random state 92, training accuracy is 0.97424198845454\n",
      "at random state 92, testing accuracy is 0.8255299535945667\n",
      "at random state 92, mean squared error is 0.024482261697327316\n",
      "at random state 92, mean absolute error is 0.11189201666088164\n",
      "\n",
      "\n",
      "at random state 93, training accuracy is 0.9749756911797545\n",
      "at random state 93, testing accuracy is 0.8123044727238725\n",
      "at random state 93, mean squared error is 0.024890531187087814\n",
      "at random state 93, mean absolute error is 0.11313762582436653\n",
      "\n",
      "\n",
      "at random state 94, training accuracy is 0.9742442268777757\n",
      "at random state 94, testing accuracy is 0.8184535030415879\n",
      "at random state 94, mean squared error is 0.024213787396737244\n",
      "at random state 94, mean absolute error is 0.11292943422422769\n",
      "\n",
      "\n",
      "at random state 95, training accuracy is 0.9745632761322688\n",
      "at random state 95, testing accuracy is 0.8049511341524567\n",
      "at random state 95, mean squared error is 0.025665378816383207\n",
      "at random state 95, mean absolute error is 0.11483897952099967\n",
      "\n",
      "\n",
      "at random state 96, training accuracy is 0.974783225270255\n",
      "at random state 96, testing accuracy is 0.8056086659814707\n",
      "at random state 96, mean squared error is 0.026416204304061087\n",
      "at random state 96, mean absolute error is 0.11613141270392226\n",
      "\n",
      "\n",
      "at random state 97, training accuracy is 0.9743117658739762\n",
      "at random state 97, testing accuracy is 0.8196472313412374\n",
      "at random state 97, mean squared error is 0.025073882058313082\n",
      "at random state 97, mean absolute error is 0.1131533148212426\n",
      "\n",
      "\n",
      "at random state 98, training accuracy is 0.9745823565129339\n",
      "at random state 98, testing accuracy is 0.81291483037053\n",
      "at random state 98, mean squared error is 0.025132419399514057\n",
      "at random state 98, mean absolute error is 0.1137946893439778\n",
      "\n",
      "\n",
      "at random state 99, training accuracy is 0.9746331821109173\n",
      "at random state 99, testing accuracy is 0.8115577916537836\n",
      "at random state 99, mean squared error is 0.025863216095105862\n",
      "at random state 99, mean absolute error is 0.11367778549114889\n",
      "\n",
      "\n",
      "Max accuracy at random state 59 = 0.8360764479352745\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor()\n",
    "model_selection(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "f96f4eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.04241865  0.22460506  0.02435112  0.5932798   0.31536063  0.54867476\n",
      "  0.3767961   0.13540964]\n",
      "0.2720073085867303\n",
      "0.21613776316670033\n"
     ]
    }
   ],
   "source": [
    "score=cross_val_score(rf,dfx,y,cv=8)\n",
    "print(score)\n",
    "print(score.mean())\n",
    "print(score.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d300e394",
   "metadata": {},
   "source": [
    "we will select the Gradient Boost Regressor algorithm for building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3b685a",
   "metadata": {},
   "source": [
    "# hyper parameter tuning using Grid Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1ee4e789",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2832b899",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict={'max_features':['auto','sqrt','log2'],'loss':['squared_error', 'absolute_error'],'learning_rate':[0.01,0.1],'n_estimators':[200,300],'criterion':['friedman_mse', 'squared_error']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "42eb7aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gd=GridSearchCV(estimator=gb,param_grid=dict,cv=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887ac78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gd.fit(dfx,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "a4df7401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'squared_error',\n",
       " 'learning_rate': 0.1,\n",
       " 'loss': 'squared_error',\n",
       " 'n_estimators': 200}"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gd.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "5e09a0ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36484154175663014"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gd.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bbc692",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2d8af1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17cf7eb3",
   "metadata": {},
   "source": [
    "# prediction of the region "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a608d2",
   "metadata": {},
   "source": [
    "## visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d70436",
   "metadata": {},
   "source": [
    "descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c1d7a5ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17651, 10)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfnew.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2dcabe9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2494</th>\n",
       "      <td>1.07</td>\n",
       "      <td>9.997641</td>\n",
       "      <td>10.491210</td>\n",
       "      <td>8.326698</td>\n",
       "      <td>8.839291</td>\n",
       "      <td>7.436588</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4521</th>\n",
       "      <td>1.14</td>\n",
       "      <td>9.628807</td>\n",
       "      <td>12.736927</td>\n",
       "      <td>7.927537</td>\n",
       "      <td>11.729727</td>\n",
       "      <td>9.241386</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16511</th>\n",
       "      <td>1.84</td>\n",
       "      <td>7.959629</td>\n",
       "      <td>7.917172</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.536315</td>\n",
       "      <td>3.643359</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5096</th>\n",
       "      <td>1.02</td>\n",
       "      <td>11.705366</td>\n",
       "      <td>11.043127</td>\n",
       "      <td>7.825157</td>\n",
       "      <td>11.191998</td>\n",
       "      <td>10.811018</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9923</th>\n",
       "      <td>1.56</td>\n",
       "      <td>9.398777</td>\n",
       "      <td>11.129704</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.654807</td>\n",
       "      <td>9.236310</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18003</th>\n",
       "      <td>1.43</td>\n",
       "      <td>7.885401</td>\n",
       "      <td>7.969206</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.506273</td>\n",
       "      <td>3.635479</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5217</th>\n",
       "      <td>1.11</td>\n",
       "      <td>14.467545</td>\n",
       "      <td>13.322496</td>\n",
       "      <td>9.240307</td>\n",
       "      <td>13.512978</td>\n",
       "      <td>12.580586</td>\n",
       "      <td>9.179670</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13904</th>\n",
       "      <td>1.56</td>\n",
       "      <td>5.242329</td>\n",
       "      <td>7.875716</td>\n",
       "      <td>6.106132</td>\n",
       "      <td>7.508156</td>\n",
       "      <td>6.464760</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>845</th>\n",
       "      <td>1.15</td>\n",
       "      <td>10.322569</td>\n",
       "      <td>11.593462</td>\n",
       "      <td>5.689007</td>\n",
       "      <td>10.853835</td>\n",
       "      <td>7.412993</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4490</th>\n",
       "      <td>1.45</td>\n",
       "      <td>9.600546</td>\n",
       "      <td>12.278661</td>\n",
       "      <td>7.847567</td>\n",
       "      <td>11.962629</td>\n",
       "      <td>9.757243</td>\n",
       "      <td>7.622297</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       AveragePrice       4046       4225      4770  Small Bags  Large Bags  \\\n",
       "2494           1.07   9.997641  10.491210  8.326698    8.839291    7.436588   \n",
       "4521           1.14   9.628807  12.736927  7.927537   11.729727    9.241386   \n",
       "16511          1.84   7.959629   7.917172  0.000000    8.536315    3.643359   \n",
       "5096           1.02  11.705366  11.043127  7.825157   11.191998   10.811018   \n",
       "9923           1.56   9.398777  11.129704  0.000000    8.654807    9.236310   \n",
       "18003          1.43   7.885401   7.969206  0.000000    9.506273    3.635479   \n",
       "5217           1.11  14.467545  13.322496  9.240307   13.512978   12.580586   \n",
       "13904          1.56   5.242329   7.875716  6.106132    7.508156    6.464760   \n",
       "845            1.15  10.322569  11.593462  5.689007   10.853835    7.412993   \n",
       "4490           1.45   9.600546  12.278661  7.847567   11.962629    9.757243   \n",
       "\n",
       "       XLarge Bags  type  year  region  \n",
       "2494      0.000000     0     0      47  \n",
       "4521      0.000000     0     1      32  \n",
       "16511     0.000000     1     2      33  \n",
       "5096      0.000000     0     1      44  \n",
       "9923      0.000000     1     0      15  \n",
       "18003     0.000000     1     3      33  \n",
       "5217      9.179670     0     1      46  \n",
       "13904     0.000000     1     1      37  \n",
       "845       0.000000     0     0      16  \n",
       "4490      7.622297     0     1      32  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfnew.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b2102774",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx=dfnew.drop([\"region\"],axis=1)\n",
    "yy=dfnew[\"region\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb5e497",
   "metadata": {},
   "source": [
    "## standard scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "80a92201",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3c46a49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx=sc.fit_transform(dfxx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "81dfccca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3d6736",
   "metadata": {},
   "source": [
    "# splitting of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d40aacab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "cf8168f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_selection(instance):\n",
    "    maxacc=0\n",
    "    rs=0\n",
    "    for i in range(0,100):\n",
    "        x_train,x_test,y_train,y_test=train_test_split(xx,yy,random_state=i,test_size=0.30)\n",
    "        instance.fit(x_train,y_train)\n",
    "        pred_train=instance.predict(x_train)\n",
    "        pred_test=instance.predict(x_test)\n",
    "        if((accuracy_score(y_test,pred_test))>maxacc):\n",
    "            maxacc=accuracy_score(y_test,pred_test)\n",
    "            rs=i\n",
    "        print(f\"at random state {i},  accuracy score is {accuracy_score(y_test,pred_test)}\")\n",
    "        print(f\"at random state {i}, confusion matrix is {confusion_matrix(y_test,pred_test)}\")\n",
    "        print(f\"at random state {i}, classification report is {classification_report(y_test,pred_test)}\")\n",
    "        print(\"\\n\")\n",
    "    print(\"Max accuracy at random state\",rs, \"=\",maxacc)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bf76d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bec059a7",
   "metadata": {},
   "source": [
    "## KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2473d17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 0,  accuracy score is 0.8155211480362538\n",
      "at random state 0, confusion matrix is [[86  0  0 ...  0  0  0]\n",
      " [ 0 70  0 ...  0  0  2]\n",
      " [ 0  0 91 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 49  0  0]\n",
      " [ 0  0  0 ...  0 74  0]\n",
      " [ 0  0  1 ...  0  0 61]]\n",
      "at random state 0, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.92      0.83        93\n",
      "           1       0.65      0.69      0.67       102\n",
      "           2       0.86      0.91      0.88       100\n",
      "           3       0.90      0.74      0.81       104\n",
      "           4       0.87      0.91      0.89       104\n",
      "           5       0.78      0.73      0.76        97\n",
      "           6       0.95      0.93      0.94        84\n",
      "           7       0.76      0.85      0.80       106\n",
      "           8       0.94      0.89      0.91       119\n",
      "           9       0.78      0.81      0.79        93\n",
      "          10       0.76      0.80      0.78       113\n",
      "          11       0.80      0.88      0.84       104\n",
      "          12       0.89      0.90      0.89        97\n",
      "          13       0.63      0.66      0.65        89\n",
      "          14       0.88      0.91      0.89        93\n",
      "          15       0.94      0.96      0.95        79\n",
      "          16       0.94      0.90      0.92        98\n",
      "          17       0.89      0.87      0.88        95\n",
      "          18       0.76      0.87      0.81        97\n",
      "          19       0.75      0.82      0.78        87\n",
      "          20       0.92      0.82      0.87       100\n",
      "          21       0.72      0.77      0.75       105\n",
      "          22       0.95      0.96      0.96       102\n",
      "          23       0.73      0.86      0.79       100\n",
      "          24       0.57      0.58      0.57       112\n",
      "          25       0.83      0.98      0.90        97\n",
      "          26       0.76      0.78      0.77       104\n",
      "          27       0.82      0.78      0.80        96\n",
      "          28       0.97      0.89      0.93        94\n",
      "          29       0.98      0.86      0.91       100\n",
      "          30       0.93      0.85      0.89       105\n",
      "          31       0.52      0.52      0.52       105\n",
      "          32       0.89      0.78      0.83       109\n",
      "          33       0.74      0.82      0.78       100\n",
      "          34       0.90      0.89      0.90       111\n",
      "          35       0.91      0.92      0.92        91\n",
      "          36       0.69      0.73      0.71       104\n",
      "          37       0.75      0.73      0.74        99\n",
      "          38       0.84      0.79      0.82       116\n",
      "          39       0.81      0.83      0.82       105\n",
      "          40       0.81      0.86      0.83       112\n",
      "          41       0.71      0.66      0.68       102\n",
      "          42       0.89      0.90      0.89        78\n",
      "          43       0.78      0.71      0.74        93\n",
      "          44       0.86      0.78      0.82       100\n",
      "          45       0.99      0.93      0.96        85\n",
      "          46       0.94      0.95      0.94        96\n",
      "          47       0.89      0.83      0.86       102\n",
      "          48       0.80      0.66      0.73        98\n",
      "          49       0.81      0.86      0.83        97\n",
      "          50       0.50      0.43      0.46       109\n",
      "          51       1.00      1.00      1.00        49\n",
      "          52       0.96      0.95      0.95        78\n",
      "          53       0.79      0.69      0.74        88\n",
      "\n",
      "    accuracy                           0.82      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.82      0.82      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 1,  accuracy score is 0.8125\n",
      "at random state 1, confusion matrix is [[ 84   0   0 ...   0   0   0]\n",
      " [  0  79   0 ...   0   0   1]\n",
      " [  0   0 119 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  46   0   0]\n",
      " [  0   0   0 ...   0  65   0]\n",
      " [  0   0   1 ...   0   0  74]]\n",
      "at random state 1, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.86      0.80        98\n",
      "           1       0.72      0.73      0.73       108\n",
      "           2       0.86      0.98      0.92       122\n",
      "           3       0.85      0.85      0.85        92\n",
      "           4       0.85      0.89      0.87       114\n",
      "           5       0.73      0.73      0.73       107\n",
      "           6       0.93      0.92      0.93        91\n",
      "           7       0.81      0.77      0.79       102\n",
      "           8       0.94      0.90      0.92        98\n",
      "           9       0.70      0.81      0.75       102\n",
      "          10       0.67      0.70      0.68       114\n",
      "          11       0.85      0.91      0.88       104\n",
      "          12       0.93      0.88      0.90       118\n",
      "          13       0.67      0.68      0.68       101\n",
      "          14       0.93      0.92      0.92        96\n",
      "          15       0.98      0.83      0.90        78\n",
      "          16       0.90      0.90      0.90        96\n",
      "          17       0.93      0.85      0.89       106\n",
      "          18       0.88      0.87      0.87        97\n",
      "          19       0.79      0.83      0.81       106\n",
      "          20       0.90      0.91      0.91        90\n",
      "          21       0.63      0.76      0.69       101\n",
      "          22       0.92      0.94      0.93        97\n",
      "          23       0.83      0.81      0.82        99\n",
      "          24       0.52      0.57      0.55        94\n",
      "          25       0.86      0.99      0.92        96\n",
      "          26       0.67      0.70      0.69        97\n",
      "          27       0.88      0.90      0.89        98\n",
      "          28       0.94      0.92      0.93       102\n",
      "          29       0.96      0.96      0.96        84\n",
      "          30       0.87      0.82      0.84       104\n",
      "          31       0.50      0.68      0.58        96\n",
      "          32       0.86      0.79      0.82        94\n",
      "          33       0.68      0.73      0.70        96\n",
      "          34       0.96      0.86      0.90        99\n",
      "          35       0.92      0.97      0.95        99\n",
      "          36       0.69      0.70      0.69        97\n",
      "          37       0.72      0.77      0.74        99\n",
      "          38       0.75      0.78      0.77       114\n",
      "          39       0.82      0.79      0.81       113\n",
      "          40       0.89      0.85      0.87       109\n",
      "          41       0.72      0.75      0.73        96\n",
      "          42       0.86      0.84      0.85        90\n",
      "          43       0.84      0.62      0.71        98\n",
      "          44       0.69      0.78      0.73        85\n",
      "          45       0.93      0.96      0.94        70\n",
      "          46       0.97      0.92      0.94       101\n",
      "          47       0.84      0.74      0.79        94\n",
      "          48       0.92      0.66      0.77        90\n",
      "          49       0.87      0.80      0.83       110\n",
      "          50       0.55      0.37      0.45       115\n",
      "          51       1.00      1.00      1.00        46\n",
      "          52       0.98      0.97      0.98        67\n",
      "          53       0.79      0.70      0.74       106\n",
      "\n",
      "    accuracy                           0.81      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.81      0.81      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 2,  accuracy score is 0.8138217522658611\n",
      "at random state 2, confusion matrix is [[97  0  0 ...  0  0  0]\n",
      " [ 0 67  0 ...  0  0  1]\n",
      " [ 0  0 87 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 43  0  0]\n",
      " [ 0  0  0 ...  1 80  0]\n",
      " [ 0  0  0 ...  0  0 65]]\n",
      "at random state 2, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.91      0.87       107\n",
      "           1       0.64      0.68      0.66        98\n",
      "           2       0.89      0.93      0.91        94\n",
      "           3       0.83      0.79      0.81        99\n",
      "           4       0.82      0.87      0.84       113\n",
      "           5       0.76      0.82      0.79        93\n",
      "           6       0.95      1.00      0.98        82\n",
      "           7       0.76      0.79      0.77       107\n",
      "           8       0.93      0.92      0.92        86\n",
      "           9       0.71      0.79      0.75        97\n",
      "          10       0.62      0.73      0.67       101\n",
      "          11       0.83      0.88      0.85       105\n",
      "          12       0.93      0.90      0.92       108\n",
      "          13       0.76      0.75      0.75       108\n",
      "          14       0.88      0.96      0.92       100\n",
      "          15       0.96      0.93      0.94        80\n",
      "          16       0.88      0.91      0.89       106\n",
      "          17       0.96      0.83      0.89       106\n",
      "          18       0.90      0.86      0.88        98\n",
      "          19       0.80      0.77      0.79        97\n",
      "          20       0.92      0.83      0.87       104\n",
      "          21       0.57      0.70      0.63       104\n",
      "          22       0.97      0.94      0.96        90\n",
      "          23       0.81      0.75      0.78       110\n",
      "          24       0.65      0.64      0.65       104\n",
      "          25       0.89      0.93      0.91       117\n",
      "          26       0.66      0.84      0.74        75\n",
      "          27       0.82      0.82      0.82       119\n",
      "          28       0.98      0.90      0.94        97\n",
      "          29       0.92      0.93      0.93       101\n",
      "          30       0.90      0.81      0.85       111\n",
      "          31       0.61      0.67      0.64       104\n",
      "          32       0.84      0.81      0.83       100\n",
      "          33       0.70      0.85      0.77        91\n",
      "          34       0.89      0.86      0.88        99\n",
      "          35       0.87      0.93      0.90        90\n",
      "          36       0.76      0.64      0.70       107\n",
      "          37       0.65      0.71      0.68       103\n",
      "          38       0.75      0.66      0.70       104\n",
      "          39       0.82      0.73      0.77       112\n",
      "          40       0.79      0.81      0.80        98\n",
      "          41       0.78      0.76      0.77       104\n",
      "          42       0.90      0.94      0.92        82\n",
      "          43       0.80      0.75      0.77        99\n",
      "          44       0.81      0.81      0.81        99\n",
      "          45       0.95      0.93      0.94        90\n",
      "          46       0.88      0.90      0.89        92\n",
      "          47       0.76      0.78      0.77        99\n",
      "          48       0.81      0.61      0.69        89\n",
      "          49       0.87      0.83      0.85       102\n",
      "          50       0.56      0.46      0.50        94\n",
      "          51       0.98      1.00      0.99        43\n",
      "          52       1.00      0.93      0.96        86\n",
      "          53       0.81      0.71      0.76        92\n",
      "\n",
      "    accuracy                           0.81      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.81      0.81      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 3,  accuracy score is 0.8168429003021148\n",
      "at random state 3, confusion matrix is [[ 88   0   0 ...   0   0   0]\n",
      " [  0  80   0 ...   0   0   0]\n",
      " [  0   0 102 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  50   0   0]\n",
      " [  0   0   0 ...   0  67   0]\n",
      " [  0   0   2 ...   0   0  70]]\n",
      "at random state 3, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.87      0.80       101\n",
      "           1       0.68      0.75      0.71       107\n",
      "           2       0.89      0.94      0.91       108\n",
      "           3       0.84      0.87      0.85       107\n",
      "           4       0.88      0.84      0.86       103\n",
      "           5       0.74      0.80      0.77       108\n",
      "           6       0.88      0.94      0.91        79\n",
      "           7       0.75      0.90      0.82        82\n",
      "           8       0.97      0.95      0.96       108\n",
      "           9       0.72      0.82      0.77        98\n",
      "          10       0.67      0.77      0.72        99\n",
      "          11       0.79      0.94      0.86        87\n",
      "          12       0.89      0.87      0.88       104\n",
      "          13       0.68      0.81      0.74        91\n",
      "          14       0.87      0.92      0.89       101\n",
      "          15       0.98      0.92      0.95        89\n",
      "          16       0.94      0.90      0.92       105\n",
      "          17       0.92      0.84      0.88       100\n",
      "          18       0.86      0.84      0.85        95\n",
      "          19       0.75      0.80      0.77        94\n",
      "          20       0.90      0.77      0.83       105\n",
      "          21       0.64      0.69      0.67        97\n",
      "          22       0.92      0.94      0.93        95\n",
      "          23       0.86      0.74      0.79       106\n",
      "          24       0.53      0.57      0.55        97\n",
      "          25       0.85      0.96      0.91        84\n",
      "          26       0.72      0.80      0.75        98\n",
      "          27       0.86      0.82      0.84        99\n",
      "          28       0.99      0.91      0.95       113\n",
      "          29       0.94      0.88      0.91        93\n",
      "          30       0.93      0.90      0.91       107\n",
      "          31       0.46      0.57      0.51        99\n",
      "          32       0.78      0.82      0.80        98\n",
      "          33       0.75      0.86      0.80       113\n",
      "          34       0.89      0.93      0.91        86\n",
      "          35       0.89      0.98      0.94       103\n",
      "          36       0.74      0.71      0.73       108\n",
      "          37       0.79      0.80      0.80        95\n",
      "          38       0.78      0.79      0.79        91\n",
      "          39       0.81      0.76      0.78       109\n",
      "          40       0.84      0.85      0.85        95\n",
      "          41       0.73      0.64      0.68        99\n",
      "          42       0.91      0.88      0.90        84\n",
      "          43       0.84      0.65      0.74       107\n",
      "          44       0.87      0.82      0.85       101\n",
      "          45       0.97      0.91      0.94       102\n",
      "          46       0.92      0.92      0.92        92\n",
      "          47       0.84      0.74      0.79       104\n",
      "          48       0.87      0.67      0.76       109\n",
      "          49       0.88      0.78      0.83       111\n",
      "          50       0.51      0.39      0.44       110\n",
      "          51       1.00      1.00      1.00        50\n",
      "          52       0.99      0.94      0.96        71\n",
      "          53       0.86      0.71      0.78        99\n",
      "\n",
      "    accuracy                           0.82      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.82      0.82      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 4,  accuracy score is 0.8196752265861027\n",
      "at random state 4, confusion matrix is [[ 82   0   0 ...   0   0   0]\n",
      " [  0  75   0 ...   0   0   2]\n",
      " [  0   0 100 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  53   0   0]\n",
      " [  0   0   0 ...   1  83   0]\n",
      " [  0   2   2 ...   0   0  82]]\n",
      "at random state 4, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.86      0.82        95\n",
      "           1       0.65      0.80      0.72        94\n",
      "           2       0.83      0.96      0.89       104\n",
      "           3       0.85      0.82      0.83       111\n",
      "           4       0.86      0.95      0.90        94\n",
      "           5       0.78      0.70      0.74       113\n",
      "           6       0.94      0.92      0.93       100\n",
      "           7       0.74      0.75      0.75       100\n",
      "           8       0.92      0.93      0.93       101\n",
      "           9       0.73      0.79      0.76        97\n",
      "          10       0.66      0.76      0.70       107\n",
      "          11       0.85      0.90      0.87       106\n",
      "          12       0.90      0.86      0.88        91\n",
      "          13       0.80      0.79      0.79        99\n",
      "          14       0.87      0.96      0.92        85\n",
      "          15       0.96      0.94      0.95        82\n",
      "          16       0.93      0.88      0.90       107\n",
      "          17       0.91      0.86      0.89        94\n",
      "          18       0.83      0.78      0.80       110\n",
      "          19       0.80      0.82      0.81        90\n",
      "          20       0.81      0.80      0.80       108\n",
      "          21       0.66      0.64      0.65        98\n",
      "          22       0.95      0.91      0.93       101\n",
      "          23       0.79      0.84      0.81       100\n",
      "          24       0.61      0.64      0.63        87\n",
      "          25       0.90      0.97      0.93        98\n",
      "          26       0.75      0.78      0.77       103\n",
      "          27       0.86      0.80      0.83        96\n",
      "          28       0.99      0.93      0.96       105\n",
      "          29       0.95      0.96      0.96        82\n",
      "          30       0.97      0.91      0.94       110\n",
      "          31       0.57      0.73      0.64        93\n",
      "          32       0.87      0.81      0.84        96\n",
      "          33       0.69      0.81      0.74        90\n",
      "          34       0.91      0.85      0.88       112\n",
      "          35       0.95      0.92      0.94       104\n",
      "          36       0.74      0.71      0.73       108\n",
      "          37       0.69      0.80      0.74       101\n",
      "          38       0.80      0.72      0.76       103\n",
      "          39       0.85      0.80      0.82       102\n",
      "          40       0.78      0.81      0.79        93\n",
      "          41       0.71      0.72      0.71        95\n",
      "          42       0.95      0.80      0.87        95\n",
      "          43       0.84      0.76      0.79        94\n",
      "          44       0.81      0.77      0.79       115\n",
      "          45       0.95      0.90      0.93        84\n",
      "          46       0.93      0.98      0.95        84\n",
      "          47       0.81      0.79      0.80       105\n",
      "          48       0.82      0.56      0.66       104\n",
      "          49       0.79      0.78      0.78       105\n",
      "          50       0.54      0.55      0.54        91\n",
      "          51       0.98      1.00      0.99        53\n",
      "          52       0.94      0.95      0.95        87\n",
      "          53       0.85      0.72      0.78       114\n",
      "\n",
      "    accuracy                           0.82      5296\n",
      "   macro avg       0.83      0.82      0.82      5296\n",
      "weighted avg       0.82      0.82      0.82      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 5,  accuracy score is 0.8158987915407855\n",
      "at random state 5, confusion matrix is [[ 86   0   0 ...   0   0   0]\n",
      " [  0  71   0 ...   0   0   3]\n",
      " [  0   0 101 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  56   0   0]\n",
      " [  0   0   0 ...   0  67   0]\n",
      " [  0   0   2 ...   0   0  79]]\n",
      "at random state 5, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.93      0.83        92\n",
      "           1       0.66      0.70      0.68       102\n",
      "           2       0.85      0.94      0.89       107\n",
      "           3       0.83      0.73      0.78       105\n",
      "           4       0.89      0.88      0.88       109\n",
      "           5       0.75      0.68      0.71       102\n",
      "           6       0.92      0.90      0.91        87\n",
      "           7       0.74      0.88      0.81       102\n",
      "           8       0.92      0.94      0.93        97\n",
      "           9       0.69      0.81      0.75        85\n",
      "          10       0.69      0.77      0.73        98\n",
      "          11       0.83      0.85      0.84       107\n",
      "          12       0.93      0.93      0.93       106\n",
      "          13       0.66      0.70      0.68       104\n",
      "          14       0.93      0.90      0.91        99\n",
      "          15       0.95      0.95      0.95        73\n",
      "          16       0.96      0.86      0.91       114\n",
      "          17       0.93      0.87      0.90       100\n",
      "          18       0.82      0.87      0.84        98\n",
      "          19       0.85      0.82      0.83       103\n",
      "          20       0.90      0.82      0.86       100\n",
      "          21       0.62      0.69      0.65       107\n",
      "          22       0.89      0.94      0.92        87\n",
      "          23       0.78      0.84      0.81       102\n",
      "          24       0.54      0.55      0.55       101\n",
      "          25       0.86      0.92      0.89       105\n",
      "          26       0.79      0.79      0.79       113\n",
      "          27       0.89      0.83      0.86        98\n",
      "          28       0.96      0.92      0.94       102\n",
      "          29       0.94      0.93      0.94       103\n",
      "          30       0.92      0.88      0.90        99\n",
      "          31       0.56      0.67      0.61       108\n",
      "          32       0.83      0.78      0.80        91\n",
      "          33       0.73      0.78      0.76        97\n",
      "          34       0.98      0.93      0.95        97\n",
      "          35       0.92      0.90      0.91       110\n",
      "          36       0.77      0.76      0.77       106\n",
      "          37       0.79      0.82      0.80       109\n",
      "          38       0.78      0.77      0.77        96\n",
      "          39       0.74      0.81      0.78        86\n",
      "          40       0.79      0.83      0.81       109\n",
      "          41       0.80      0.61      0.69       105\n",
      "          42       0.93      0.79      0.86        97\n",
      "          43       0.85      0.74      0.79        93\n",
      "          44       0.81      0.82      0.81        88\n",
      "          45       0.98      0.90      0.94        93\n",
      "          46       0.85      0.90      0.88        90\n",
      "          47       0.83      0.79      0.81        91\n",
      "          48       0.80      0.71      0.75        93\n",
      "          49       0.79      0.83      0.81       105\n",
      "          50       0.44      0.34      0.39        93\n",
      "          51       1.00      1.00      1.00        56\n",
      "          52       0.97      0.96      0.96        70\n",
      "          53       0.81      0.75      0.78       106\n",
      "\n",
      "    accuracy                           0.82      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.82      0.82      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 6,  accuracy score is 0.8187311178247734\n",
      "at random state 6, confusion matrix is [[ 91   0   0 ...   0   0   0]\n",
      " [  0  77   0 ...   0   0   0]\n",
      " [  0   0 110 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  55   0   0]\n",
      " [  0   0   0 ...   0  73   0]\n",
      " [  0   0   0 ...   0   0  75]]\n",
      "at random state 6, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.89      0.84       102\n",
      "           1       0.74      0.66      0.70       116\n",
      "           2       0.92      0.96      0.94       114\n",
      "           3       0.87      0.86      0.86        77\n",
      "           4       0.88      0.86      0.87        90\n",
      "           5       0.71      0.73      0.72        92\n",
      "           6       0.96      0.86      0.91        80\n",
      "           7       0.78      0.87      0.82       119\n",
      "           8       0.94      0.97      0.96       101\n",
      "           9       0.79      0.77      0.78       106\n",
      "          10       0.66      0.74      0.70       104\n",
      "          11       0.81      0.92      0.86       118\n",
      "          12       0.90      0.90      0.90        94\n",
      "          13       0.67      0.71      0.69        98\n",
      "          14       0.87      0.96      0.91        90\n",
      "          15       0.91      0.96      0.93        72\n",
      "          16       0.87      0.95      0.91       100\n",
      "          17       0.89      0.85      0.87        96\n",
      "          18       0.80      0.92      0.86       104\n",
      "          19       0.81      0.88      0.84        98\n",
      "          20       0.89      0.85      0.87       100\n",
      "          21       0.66      0.75      0.70       108\n",
      "          22       0.91      0.97      0.94        91\n",
      "          23       0.83      0.75      0.78       110\n",
      "          24       0.70      0.55      0.61       113\n",
      "          25       0.88      0.92      0.90        83\n",
      "          26       0.72      0.71      0.71        95\n",
      "          27       0.86      0.80      0.83        95\n",
      "          28       0.97      0.94      0.95        95\n",
      "          29       0.95      0.93      0.94        98\n",
      "          30       0.95      0.89      0.91        97\n",
      "          31       0.60      0.64      0.62       102\n",
      "          32       0.84      0.77      0.80        99\n",
      "          33       0.72      0.68      0.70       119\n",
      "          34       0.89      0.86      0.87       104\n",
      "          35       0.90      0.96      0.93        99\n",
      "          36       0.84      0.71      0.77       114\n",
      "          37       0.78      0.75      0.76       102\n",
      "          38       0.82      0.76      0.79       113\n",
      "          39       0.80      0.79      0.79        99\n",
      "          40       0.72      0.85      0.78        95\n",
      "          41       0.79      0.67      0.72       112\n",
      "          42       0.91      0.80      0.85        99\n",
      "          43       0.88      0.71      0.78        99\n",
      "          44       0.82      0.84      0.83        94\n",
      "          45       0.95      0.96      0.95        93\n",
      "          46       0.93      0.94      0.93        83\n",
      "          47       0.79      0.88      0.83       106\n",
      "          48       0.79      0.67      0.73        86\n",
      "          49       0.80      0.83      0.81       105\n",
      "          50       0.41      0.46      0.43        84\n",
      "          51       0.98      1.00      0.99        55\n",
      "          52       0.97      0.92      0.95        79\n",
      "          53       0.80      0.76      0.78        99\n",
      "\n",
      "    accuracy                           0.82      5296\n",
      "   macro avg       0.83      0.82      0.82      5296\n",
      "weighted avg       0.82      0.82      0.82      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 7,  accuracy score is 0.8136329305135952\n",
      "at random state 7, confusion matrix is [[ 89   0   0 ...   0   0   0]\n",
      " [  0  87   0 ...   0   0   1]\n",
      " [  0   0 102 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  45   0   0]\n",
      " [  0   0   0 ...   0  72   0]\n",
      " [  0   0   1 ...   0   0  89]]\n",
      "at random state 7, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.92      0.82        97\n",
      "           1       0.66      0.78      0.71       112\n",
      "           2       0.87      0.94      0.91       108\n",
      "           3       0.91      0.73      0.81        97\n",
      "           4       0.86      0.88      0.87       104\n",
      "           5       0.75      0.67      0.71        94\n",
      "           6       0.92      0.92      0.92        85\n",
      "           7       0.75      0.80      0.77       102\n",
      "           8       0.93      0.91      0.92       110\n",
      "           9       0.73      0.81      0.77       114\n",
      "          10       0.65      0.69      0.67       104\n",
      "          11       0.84      0.89      0.87        95\n",
      "          12       0.87      0.81      0.84       116\n",
      "          13       0.67      0.74      0.70       103\n",
      "          14       0.87      0.91      0.89        97\n",
      "          15       0.93      0.97      0.95        80\n",
      "          16       0.93      0.89      0.91       104\n",
      "          17       0.90      0.86      0.88        85\n",
      "          18       0.83      0.84      0.84       109\n",
      "          19       0.74      0.78      0.76        99\n",
      "          20       0.88      0.84      0.86        97\n",
      "          21       0.62      0.70      0.66        97\n",
      "          22       0.96      0.95      0.95        99\n",
      "          23       0.72      0.81      0.76       103\n",
      "          24       0.56      0.66      0.61        92\n",
      "          25       0.92      0.98      0.95        98\n",
      "          26       0.73      0.75      0.74       100\n",
      "          27       0.83      0.83      0.83        89\n",
      "          28       0.97      0.93      0.95       101\n",
      "          29       0.98      0.95      0.97        88\n",
      "          30       0.90      0.81      0.85       107\n",
      "          31       0.61      0.68      0.64        93\n",
      "          32       0.88      0.81      0.84       105\n",
      "          33       0.64      0.79      0.71        85\n",
      "          34       0.91      0.88      0.90       122\n",
      "          35       0.92      0.95      0.94       110\n",
      "          36       0.73      0.72      0.72        89\n",
      "          37       0.72      0.76      0.74       104\n",
      "          38       0.78      0.77      0.77        98\n",
      "          39       0.83      0.79      0.81        98\n",
      "          40       0.77      0.79      0.78        95\n",
      "          41       0.74      0.68      0.71        96\n",
      "          42       0.84      0.80      0.81        83\n",
      "          43       0.92      0.75      0.83       104\n",
      "          44       0.85      0.81      0.83       106\n",
      "          45       0.95      0.95      0.95        92\n",
      "          46       0.95      0.92      0.94       102\n",
      "          47       0.75      0.76      0.75        95\n",
      "          48       0.79      0.68      0.73        91\n",
      "          49       0.83      0.81      0.82        96\n",
      "          50       0.56      0.36      0.44       104\n",
      "          51       1.00      1.00      1.00        45\n",
      "          52       0.99      0.90      0.94        80\n",
      "          53       0.90      0.76      0.82       117\n",
      "\n",
      "    accuracy                           0.81      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.81      0.81      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 8,  accuracy score is 0.8130664652567976\n",
      "at random state 8, confusion matrix is [[77  0  0 ...  0  0  0]\n",
      " [ 0 95  0 ...  0  0  2]\n",
      " [ 0  0 93 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 61  0  0]\n",
      " [ 0  0  0 ...  0 67  0]\n",
      " [ 0  0  4 ...  0  0 77]]\n",
      "at random state 8, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.91      0.84        85\n",
      "           1       0.71      0.80      0.75       119\n",
      "           2       0.85      0.94      0.89        99\n",
      "           3       0.86      0.74      0.80       100\n",
      "           4       0.92      0.90      0.91       119\n",
      "           5       0.76      0.76      0.76        96\n",
      "           6       0.92      0.90      0.91        92\n",
      "           7       0.72      0.87      0.79        97\n",
      "           8       0.96      0.93      0.94        98\n",
      "           9       0.64      0.75      0.69        92\n",
      "          10       0.66      0.68      0.67       107\n",
      "          11       0.80      0.90      0.84       109\n",
      "          12       0.86      0.88      0.87        86\n",
      "          13       0.71      0.70      0.71       101\n",
      "          14       0.89      0.92      0.91       100\n",
      "          15       0.91      0.92      0.91        84\n",
      "          16       0.92      0.91      0.92       114\n",
      "          17       0.88      0.84      0.86       104\n",
      "          18       0.82      0.84      0.83        99\n",
      "          19       0.83      0.77      0.79       111\n",
      "          20       0.88      0.82      0.85        87\n",
      "          21       0.68      0.69      0.69        98\n",
      "          22       0.96      0.94      0.95       103\n",
      "          23       0.73      0.80      0.76        95\n",
      "          24       0.62      0.53      0.57       110\n",
      "          25       0.91      0.96      0.94       114\n",
      "          26       0.69      0.77      0.73        97\n",
      "          27       0.90      0.85      0.87       105\n",
      "          28       0.98      0.86      0.92       113\n",
      "          29       0.95      0.90      0.92       101\n",
      "          30       0.94      0.92      0.93        98\n",
      "          31       0.50      0.62      0.55        87\n",
      "          32       0.85      0.84      0.85        88\n",
      "          33       0.71      0.80      0.75        94\n",
      "          34       0.87      0.85      0.86        95\n",
      "          35       0.87      0.97      0.92        92\n",
      "          36       0.76      0.70      0.73       112\n",
      "          37       0.80      0.72      0.75       109\n",
      "          38       0.76      0.78      0.77       102\n",
      "          39       0.89      0.78      0.83       101\n",
      "          40       0.82      0.80      0.81       108\n",
      "          41       0.71      0.66      0.68       108\n",
      "          42       0.91      0.86      0.89        95\n",
      "          43       0.81      0.80      0.80        99\n",
      "          44       0.85      0.79      0.82       109\n",
      "          45       0.90      0.94      0.92        82\n",
      "          46       0.85      0.89      0.87        71\n",
      "          47       0.75      0.81      0.78        93\n",
      "          48       0.85      0.67      0.75        91\n",
      "          49       0.81      0.86      0.84        92\n",
      "          50       0.47      0.46      0.46        87\n",
      "          51       1.00      1.00      1.00        61\n",
      "          52       0.92      0.85      0.88        79\n",
      "          53       0.85      0.71      0.77       108\n",
      "\n",
      "    accuracy                           0.81      5296\n",
      "   macro avg       0.82      0.82      0.81      5296\n",
      "weighted avg       0.82      0.81      0.81      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 9,  accuracy score is 0.8175981873111783\n",
      "at random state 9, confusion matrix is [[79  0  0 ...  0  0  0]\n",
      " [ 0 70  0 ...  0  0  3]\n",
      " [ 0  0 95 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 47  0  0]\n",
      " [ 0  0  0 ...  0 74  0]\n",
      " [ 0  1  0 ...  0  0 81]]\n",
      "at random state 9, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.91      0.83        87\n",
      "           1       0.66      0.65      0.65       108\n",
      "           2       0.90      0.91      0.90       104\n",
      "           3       0.80      0.77      0.78        94\n",
      "           4       0.86      0.94      0.90       106\n",
      "           5       0.77      0.68      0.72       106\n",
      "           6       0.90      0.97      0.93        93\n",
      "           7       0.75      0.81      0.78        95\n",
      "           8       0.91      0.95      0.93        93\n",
      "           9       0.64      0.92      0.76        92\n",
      "          10       0.70      0.68      0.69       107\n",
      "          11       0.85      0.91      0.88       105\n",
      "          12       0.89      0.86      0.88       102\n",
      "          13       0.71      0.68      0.69       113\n",
      "          14       0.90      0.88      0.89       109\n",
      "          15       0.84      0.95      0.89        59\n",
      "          16       0.93      0.86      0.89       104\n",
      "          17       0.97      0.85      0.90        98\n",
      "          18       0.87      0.89      0.88       108\n",
      "          19       0.74      0.74      0.74        86\n",
      "          20       0.96      0.88      0.92       116\n",
      "          21       0.64      0.74      0.68        92\n",
      "          22       0.99      0.93      0.96       105\n",
      "          23       0.77      0.76      0.76       104\n",
      "          24       0.54      0.53      0.53        96\n",
      "          25       0.91      0.95      0.93       108\n",
      "          26       0.80      0.82      0.81        90\n",
      "          27       0.89      0.80      0.84       110\n",
      "          28       0.99      0.92      0.95        91\n",
      "          29       0.99      0.95      0.97        83\n",
      "          30       0.89      0.86      0.88        96\n",
      "          31       0.54      0.56      0.55       106\n",
      "          32       0.83      0.84      0.84        94\n",
      "          33       0.75      0.74      0.75       111\n",
      "          34       0.92      0.91      0.92       104\n",
      "          35       0.91      0.90      0.91       101\n",
      "          36       0.70      0.68      0.69       101\n",
      "          37       0.73      0.77      0.75       102\n",
      "          38       0.79      0.79      0.79       106\n",
      "          39       0.83      0.88      0.86        94\n",
      "          40       0.84      0.78      0.81       106\n",
      "          41       0.71      0.77      0.74       103\n",
      "          42       0.92      0.85      0.88        89\n",
      "          43       0.84      0.80      0.82        99\n",
      "          44       0.82      0.77      0.79       105\n",
      "          45       0.95      0.93      0.94        87\n",
      "          46       0.93      0.93      0.93        91\n",
      "          47       0.80      0.81      0.80        99\n",
      "          48       0.81      0.73      0.77        97\n",
      "          49       0.81      0.89      0.85       103\n",
      "          50       0.51      0.43      0.47       104\n",
      "          51       1.00      1.00      1.00        47\n",
      "          52       0.97      0.91      0.94        81\n",
      "          53       0.84      0.76      0.80       106\n",
      "\n",
      "    accuracy                           0.82      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.82      0.82      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 10,  accuracy score is 0.8092900302114804\n",
      "at random state 10, confusion matrix is [[ 89   0   0 ...   0   0   0]\n",
      " [  0  67   0 ...   0   0   0]\n",
      " [  0   0 101 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  51   0   0]\n",
      " [  0   0   0 ...   0  68   0]\n",
      " [  0   0   1 ...   0   0  63]]\n",
      "at random state 10, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.89      0.81       100\n",
      "           1       0.60      0.69      0.64        97\n",
      "           2       0.83      0.94      0.88       108\n",
      "           3       0.79      0.78      0.79       102\n",
      "           4       0.84      0.88      0.86        97\n",
      "           5       0.79      0.71      0.74       109\n",
      "           6       0.94      0.99      0.96        81\n",
      "           7       0.76      0.84      0.80        99\n",
      "           8       0.92      0.94      0.93       109\n",
      "           9       0.73      0.76      0.74        97\n",
      "          10       0.54      0.74      0.63       101\n",
      "          11       0.83      0.94      0.88       108\n",
      "          12       0.92      0.84      0.88        92\n",
      "          13       0.72      0.72      0.72       115\n",
      "          14       0.91      0.90      0.90        96\n",
      "          15       0.99      0.95      0.97        75\n",
      "          16       0.86      0.94      0.90       107\n",
      "          17       0.91      0.85      0.88        93\n",
      "          18       0.87      0.87      0.87       104\n",
      "          19       0.87      0.82      0.84       119\n",
      "          20       0.92      0.85      0.88       115\n",
      "          21       0.68      0.70      0.69       115\n",
      "          22       0.97      0.95      0.96       107\n",
      "          23       0.78      0.73      0.76       109\n",
      "          24       0.54      0.54      0.54        83\n",
      "          25       0.85      0.98      0.91        89\n",
      "          26       0.72      0.74      0.73        96\n",
      "          27       0.88      0.81      0.85       122\n",
      "          28       0.95      0.91      0.93       100\n",
      "          29       0.97      0.91      0.94       104\n",
      "          30       0.92      0.85      0.89       100\n",
      "          31       0.59      0.58      0.59       105\n",
      "          32       0.89      0.79      0.84       116\n",
      "          33       0.73      0.69      0.71       107\n",
      "          34       0.88      0.94      0.91        89\n",
      "          35       0.86      0.95      0.90        88\n",
      "          36       0.73      0.71      0.72        91\n",
      "          37       0.74      0.73      0.74        98\n",
      "          38       0.84      0.75      0.79       105\n",
      "          39       0.82      0.68      0.74       109\n",
      "          40       0.73      0.85      0.79        94\n",
      "          41       0.80      0.65      0.72       101\n",
      "          42       0.84      0.80      0.82        90\n",
      "          43       0.91      0.72      0.80        96\n",
      "          44       0.77      0.82      0.79        87\n",
      "          45       0.95      0.95      0.95        83\n",
      "          46       0.96      0.88      0.91       104\n",
      "          47       0.77      0.69      0.73        97\n",
      "          48       0.82      0.65      0.73       112\n",
      "          49       0.77      0.82      0.79        84\n",
      "          50       0.50      0.49      0.49       100\n",
      "          51       1.00      1.00      1.00        51\n",
      "          52       0.96      0.99      0.97        69\n",
      "          53       0.70      0.89      0.78        71\n",
      "\n",
      "    accuracy                           0.81      5296\n",
      "   macro avg       0.82      0.81      0.81      5296\n",
      "weighted avg       0.81      0.81      0.81      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 11,  accuracy score is 0.8140105740181269\n",
      "at random state 11, confusion matrix is [[ 96   0   0 ...   0   0   0]\n",
      " [  0  61   0 ...   0   0   0]\n",
      " [  0   0 109 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  59   0   0]\n",
      " [  0   0   0 ...   0  78   0]\n",
      " [  0   2   0 ...   0   0  75]]\n",
      "at random state 11, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.94      0.87       102\n",
      "           1       0.64      0.70      0.67        87\n",
      "           2       0.91      0.96      0.94       113\n",
      "           3       0.82      0.82      0.82        95\n",
      "           4       0.92      0.95      0.93       115\n",
      "           5       0.79      0.72      0.75        88\n",
      "           6       0.94      0.95      0.94        77\n",
      "           7       0.72      0.84      0.77        93\n",
      "           8       0.97      0.90      0.93       103\n",
      "           9       0.74      0.72      0.73       118\n",
      "          10       0.66      0.79      0.72        98\n",
      "          11       0.81      0.89      0.85       107\n",
      "          12       0.88      0.90      0.89       105\n",
      "          13       0.69      0.77      0.73        99\n",
      "          14       0.88      0.91      0.89       101\n",
      "          15       0.90      0.97      0.93        79\n",
      "          16       0.95      0.90      0.92        99\n",
      "          17       0.92      0.86      0.89        93\n",
      "          18       0.74      0.80      0.77        94\n",
      "          19       0.78      0.78      0.78       100\n",
      "          20       0.84      0.90      0.87        92\n",
      "          21       0.64      0.74      0.69        92\n",
      "          22       0.94      0.96      0.95       101\n",
      "          23       0.78      0.76      0.77        93\n",
      "          24       0.57      0.50      0.53       102\n",
      "          25       0.92      0.96      0.94       102\n",
      "          26       0.76      0.75      0.75       102\n",
      "          27       0.89      0.87      0.88        91\n",
      "          28       0.92      0.89      0.91        94\n",
      "          29       0.96      0.94      0.95       109\n",
      "          30       0.91      0.88      0.90       103\n",
      "          31       0.61      0.67      0.63       108\n",
      "          32       0.89      0.79      0.84       107\n",
      "          33       0.67      0.77      0.72       112\n",
      "          34       0.93      0.93      0.93       107\n",
      "          35       0.92      0.90      0.91       100\n",
      "          36       0.78      0.66      0.71        96\n",
      "          37       0.71      0.67      0.69       105\n",
      "          38       0.79      0.77      0.78       100\n",
      "          39       0.71      0.75      0.73        89\n",
      "          40       0.80      0.80      0.80       109\n",
      "          41       0.74      0.60      0.67       111\n",
      "          42       0.86      0.91      0.88        78\n",
      "          43       0.85      0.74      0.79        97\n",
      "          44       0.79      0.83      0.81        96\n",
      "          45       0.93      0.92      0.93        91\n",
      "          46       0.92      0.93      0.93       114\n",
      "          47       0.78      0.75      0.76        87\n",
      "          48       0.81      0.67      0.73        90\n",
      "          49       0.80      0.81      0.81        91\n",
      "          50       0.51      0.45      0.48       103\n",
      "          51       1.00      1.00      1.00        59\n",
      "          52       0.99      0.94      0.96        83\n",
      "          53       0.79      0.65      0.71       116\n",
      "\n",
      "    accuracy                           0.81      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.81      0.81      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 12,  accuracy score is 0.8132552870090635\n",
      "at random state 12, confusion matrix is [[89  0  0 ...  0  0  0]\n",
      " [ 0 67  0 ...  0  0  2]\n",
      " [ 0  0 91 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 51  0  0]\n",
      " [ 0  0  0 ...  0 76  0]\n",
      " [ 0  0  2 ...  0  0 76]]\n",
      "at random state 12, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.88      0.84       101\n",
      "           1       0.63      0.71      0.67        94\n",
      "           2       0.83      0.99      0.91        92\n",
      "           3       0.79      0.70      0.75        98\n",
      "           4       0.86      0.84      0.85        98\n",
      "           5       0.78      0.72      0.75       107\n",
      "           6       0.92      0.86      0.89        88\n",
      "           7       0.71      0.86      0.78       111\n",
      "           8       0.97      0.90      0.93        99\n",
      "           9       0.64      0.73      0.68       104\n",
      "          10       0.67      0.71      0.69       104\n",
      "          11       0.84      0.86      0.85       108\n",
      "          12       0.95      0.88      0.91       107\n",
      "          13       0.65      0.66      0.65       109\n",
      "          14       0.90      0.92      0.91       105\n",
      "          15       0.94      0.90      0.92        83\n",
      "          16       0.91      0.93      0.92       102\n",
      "          17       0.86      0.90      0.88        89\n",
      "          18       0.85      0.85      0.85        96\n",
      "          19       0.72      0.81      0.76       100\n",
      "          20       0.92      0.85      0.88        98\n",
      "          21       0.61      0.77      0.68        92\n",
      "          22       0.98      0.95      0.96       101\n",
      "          23       0.75      0.84      0.80       109\n",
      "          24       0.61      0.71      0.66       104\n",
      "          25       0.85      0.95      0.90        99\n",
      "          26       0.74      0.69      0.71       110\n",
      "          27       0.90      0.82      0.86       114\n",
      "          28       0.97      0.94      0.95        94\n",
      "          29       0.94      0.88      0.91       105\n",
      "          30       0.97      0.88      0.92       103\n",
      "          31       0.60      0.65      0.62       105\n",
      "          32       0.88      0.82      0.85       102\n",
      "          33       0.75      0.75      0.75       110\n",
      "          34       0.88      0.88      0.88       101\n",
      "          35       0.87      0.88      0.88       102\n",
      "          36       0.72      0.75      0.73        88\n",
      "          37       0.77      0.71      0.74       106\n",
      "          38       0.79      0.77      0.78        88\n",
      "          39       0.80      0.75      0.77        89\n",
      "          40       0.84      0.84      0.84        96\n",
      "          41       0.72      0.72      0.72        98\n",
      "          42       0.89      0.86      0.87        92\n",
      "          43       0.90      0.71      0.79        97\n",
      "          44       0.89      0.81      0.85       103\n",
      "          45       0.95      0.96      0.95        72\n",
      "          46       0.91      0.95      0.93        94\n",
      "          47       0.82      0.78      0.80       103\n",
      "          48       0.84      0.64      0.73       100\n",
      "          49       0.82      0.89      0.85        97\n",
      "          50       0.54      0.41      0.46        91\n",
      "          51       1.00      1.00      1.00        51\n",
      "          52       0.90      0.94      0.92        81\n",
      "          53       0.81      0.72      0.76       106\n",
      "\n",
      "    accuracy                           0.81      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.81      0.81      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 13,  accuracy score is 0.8164652567975831\n",
      "at random state 13, confusion matrix is [[104   0   0 ...   0   0   0]\n",
      " [  0  78   0 ...   0   0   2]\n",
      " [  0   0 110 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  45   0   0]\n",
      " [  0   0   0 ...   0  70   0]\n",
      " [  0   0   3 ...   0   0  68]]\n",
      "at random state 13, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.93      0.89       112\n",
      "           1       0.63      0.79      0.70        99\n",
      "           2       0.88      0.95      0.91       116\n",
      "           3       0.91      0.81      0.86       115\n",
      "           4       0.92      0.91      0.91        98\n",
      "           5       0.71      0.71      0.71        84\n",
      "           6       0.90      0.95      0.92       102\n",
      "           7       0.74      0.84      0.79        88\n",
      "           8       0.97      0.91      0.94       116\n",
      "           9       0.71      0.78      0.74        99\n",
      "          10       0.70      0.63      0.66       121\n",
      "          11       0.85      0.93      0.89        87\n",
      "          12       0.89      0.91      0.90        86\n",
      "          13       0.62      0.72      0.67        95\n",
      "          14       0.89      0.96      0.92        93\n",
      "          15       0.93      0.87      0.90        79\n",
      "          16       0.91      0.91      0.91        93\n",
      "          17       0.94      0.84      0.89       113\n",
      "          18       0.83      0.78      0.80        92\n",
      "          19       0.76      0.80      0.78       104\n",
      "          20       0.94      0.84      0.89        95\n",
      "          21       0.71      0.68      0.69       115\n",
      "          22       0.97      0.90      0.93       110\n",
      "          23       0.75      0.81      0.78       101\n",
      "          24       0.56      0.59      0.58        98\n",
      "          25       0.85      0.95      0.90       105\n",
      "          26       0.72      0.79      0.75        94\n",
      "          27       0.80      0.83      0.82       103\n",
      "          28       0.98      0.93      0.95       101\n",
      "          29       0.95      0.91      0.93        94\n",
      "          30       0.91      0.95      0.93        96\n",
      "          31       0.53      0.60      0.56       101\n",
      "          32       0.92      0.85      0.88       106\n",
      "          33       0.68      0.85      0.75        94\n",
      "          34       0.89      0.93      0.91        94\n",
      "          35       0.94      0.94      0.94       100\n",
      "          36       0.69      0.68      0.68        90\n",
      "          37       0.70      0.78      0.74        97\n",
      "          38       0.88      0.75      0.81       110\n",
      "          39       0.81      0.80      0.81       104\n",
      "          40       0.85      0.81      0.83       109\n",
      "          41       0.65      0.65      0.65        98\n",
      "          42       0.84      0.90      0.87        80\n",
      "          43       0.87      0.69      0.77        96\n",
      "          44       0.83      0.87      0.85       109\n",
      "          45       0.99      0.94      0.96        79\n",
      "          46       0.89      0.97      0.93        78\n",
      "          47       0.80      0.70      0.75       108\n",
      "          48       0.88      0.66      0.76       115\n",
      "          49       0.86      0.83      0.84       100\n",
      "          50       0.52      0.41      0.46       109\n",
      "          51       1.00      1.00      1.00        45\n",
      "          52       0.93      0.92      0.93        76\n",
      "          53       0.83      0.72      0.77        94\n",
      "\n",
      "    accuracy                           0.82      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.82      0.82      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 14,  accuracy score is 0.8158987915407855\n",
      "at random state 14, confusion matrix is [[100   0   0 ...   0   0   0]\n",
      " [  0  85   0 ...   0   0   2]\n",
      " [  0   0  84 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  57   0   0]\n",
      " [  0   0   0 ...   0  65   0]\n",
      " [  0   0   2 ...   0   0  73]]\n",
      "at random state 14, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.91      0.86       110\n",
      "           1       0.62      0.77      0.68       111\n",
      "           2       0.80      0.92      0.86        91\n",
      "           3       0.90      0.74      0.81        94\n",
      "           4       0.91      0.91      0.91       108\n",
      "           5       0.85      0.71      0.77        96\n",
      "           6       0.95      0.97      0.96        92\n",
      "           7       0.64      0.80      0.71        85\n",
      "           8       0.97      0.91      0.94       100\n",
      "           9       0.74      0.81      0.77       100\n",
      "          10       0.64      0.69      0.66       105\n",
      "          11       0.82      0.91      0.86        98\n",
      "          12       0.94      0.87      0.90        85\n",
      "          13       0.70      0.73      0.71       102\n",
      "          14       0.84      0.91      0.88       101\n",
      "          15       0.98      0.93      0.95        86\n",
      "          16       0.86      0.94      0.90        88\n",
      "          17       0.93      0.76      0.84        94\n",
      "          18       0.88      0.88      0.88       102\n",
      "          19       0.82      0.88      0.85       108\n",
      "          20       0.86      0.88      0.87        95\n",
      "          21       0.58      0.76      0.66        93\n",
      "          22       0.93      0.95      0.94       105\n",
      "          23       0.79      0.88      0.83        97\n",
      "          24       0.73      0.67      0.70       104\n",
      "          25       0.88      0.98      0.93        94\n",
      "          26       0.72      0.72      0.72       114\n",
      "          27       0.94      0.78      0.85        97\n",
      "          28       0.98      0.92      0.95       106\n",
      "          29       0.96      0.96      0.96        99\n",
      "          30       0.88      0.92      0.90        95\n",
      "          31       0.57      0.66      0.61       111\n",
      "          32       0.86      0.77      0.81        96\n",
      "          33       0.68      0.75      0.71       104\n",
      "          34       0.87      0.93      0.90        97\n",
      "          35       0.88      0.88      0.88       104\n",
      "          36       0.77      0.71      0.74       107\n",
      "          37       0.79      0.63      0.70       123\n",
      "          38       0.74      0.76      0.75       101\n",
      "          39       0.84      0.79      0.81       102\n",
      "          40       0.82      0.79      0.80       113\n",
      "          41       0.80      0.71      0.75        93\n",
      "          42       0.90      0.85      0.87        91\n",
      "          43       0.84      0.78      0.81       101\n",
      "          44       0.82      0.88      0.85       104\n",
      "          45       0.92      0.88      0.90        78\n",
      "          46       0.89      0.85      0.87        98\n",
      "          47       0.79      0.73      0.76        96\n",
      "          48       0.91      0.65      0.76        98\n",
      "          49       0.82      0.84      0.83        96\n",
      "          50       0.55      0.47      0.51       101\n",
      "          51       1.00      1.00      1.00        57\n",
      "          52       0.96      0.93      0.94        70\n",
      "          53       0.79      0.73      0.76       100\n",
      "\n",
      "    accuracy                           0.82      5296\n",
      "   macro avg       0.83      0.82      0.82      5296\n",
      "weighted avg       0.82      0.82      0.82      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 15,  accuracy score is 0.8119335347432024\n",
      "at random state 15, confusion matrix is [[ 72   0   0 ...   0   0   0]\n",
      " [  0  85   0 ...   0   0   0]\n",
      " [  0   0 102 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  48   0   0]\n",
      " [  0   0   0 ...   0  82   0]\n",
      " [  0   0   0 ...   0   0  78]]\n",
      "at random state 15, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.83      0.76        87\n",
      "           1       0.69      0.79      0.73       108\n",
      "           2       0.84      0.98      0.91       104\n",
      "           3       0.82      0.77      0.79        98\n",
      "           4       0.78      0.87      0.82        92\n",
      "           5       0.82      0.66      0.73       105\n",
      "           6       0.96      0.97      0.96        94\n",
      "           7       0.76      0.82      0.79       103\n",
      "           8       0.96      0.90      0.93       113\n",
      "           9       0.76      0.78      0.77       109\n",
      "          10       0.60      0.75      0.66        96\n",
      "          11       0.85      0.92      0.88       108\n",
      "          12       0.94      0.83      0.88       118\n",
      "          13       0.70      0.66      0.68       105\n",
      "          14       0.90      0.92      0.91        93\n",
      "          15       0.93      0.95      0.94        75\n",
      "          16       0.82      0.93      0.87        91\n",
      "          17       0.89      0.91      0.90        94\n",
      "          18       0.81      0.83      0.82        94\n",
      "          19       0.76      0.79      0.77        95\n",
      "          20       0.93      0.85      0.89        93\n",
      "          21       0.64      0.72      0.68       104\n",
      "          22       0.96      0.92      0.94        97\n",
      "          23       0.76      0.77      0.76       111\n",
      "          24       0.58      0.58      0.58        90\n",
      "          25       0.91      0.99      0.95        98\n",
      "          26       0.75      0.73      0.74       100\n",
      "          27       0.83      0.70      0.76        97\n",
      "          28       0.97      0.90      0.93        98\n",
      "          29       0.96      0.96      0.96       106\n",
      "          30       0.93      0.83      0.88        95\n",
      "          31       0.56      0.56      0.56       106\n",
      "          32       0.88      0.75      0.81       115\n",
      "          33       0.75      0.77      0.76       110\n",
      "          34       0.87      0.90      0.88       109\n",
      "          35       0.91      0.92      0.92       105\n",
      "          36       0.70      0.80      0.75        97\n",
      "          37       0.69      0.78      0.73        88\n",
      "          38       0.85      0.75      0.80       109\n",
      "          39       0.81      0.84      0.82        89\n",
      "          40       0.83      0.87      0.85       102\n",
      "          41       0.72      0.74      0.73        96\n",
      "          42       0.87      0.80      0.83        84\n",
      "          43       0.93      0.67      0.78       112\n",
      "          44       0.79      0.82      0.80       103\n",
      "          45       0.95      0.96      0.96        82\n",
      "          46       0.95      0.90      0.93        93\n",
      "          47       0.82      0.83      0.82        92\n",
      "          48       0.85      0.63      0.72        99\n",
      "          49       0.71      0.82      0.76        98\n",
      "          50       0.47      0.47      0.47        95\n",
      "          51       1.00      1.00      1.00        48\n",
      "          52       0.99      0.95      0.97        86\n",
      "          53       0.86      0.73      0.79       107\n",
      "\n",
      "    accuracy                           0.81      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.81      0.81      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 16,  accuracy score is 0.8130664652567976\n",
      "at random state 16, confusion matrix is [[72  0  0 ...  0  0  0]\n",
      " [ 0 63  0 ...  0  0  1]\n",
      " [ 0  0 97 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 53  0  0]\n",
      " [ 0  0  0 ...  0 73  0]\n",
      " [ 0  0  0 ...  0  0 77]]\n",
      "at random state 16, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.88      0.77        82\n",
      "           1       0.57      0.71      0.63        89\n",
      "           2       0.87      0.92      0.90       105\n",
      "           3       0.84      0.76      0.80        84\n",
      "           4       0.87      0.93      0.90        99\n",
      "           5       0.72      0.66      0.69       114\n",
      "           6       0.89      0.98      0.93        83\n",
      "           7       0.72      0.88      0.79        96\n",
      "           8       0.91      0.94      0.92        98\n",
      "           9       0.79      0.81      0.80       111\n",
      "          10       0.61      0.63      0.62       112\n",
      "          11       0.79      0.92      0.85        97\n",
      "          12       0.98      0.86      0.92       116\n",
      "          13       0.65      0.70      0.68       105\n",
      "          14       0.89      0.89      0.89        93\n",
      "          15       0.96      0.95      0.96        85\n",
      "          16       0.87      0.86      0.87        88\n",
      "          17       0.92      0.84      0.88        90\n",
      "          18       0.90      0.88      0.89        93\n",
      "          19       0.74      0.87      0.80       104\n",
      "          20       0.89      0.86      0.87       104\n",
      "          21       0.71      0.71      0.71       115\n",
      "          22       1.00      0.90      0.95       110\n",
      "          23       0.78      0.81      0.79       108\n",
      "          24       0.59      0.55      0.57       103\n",
      "          25       0.88      0.98      0.93       101\n",
      "          26       0.78      0.77      0.78       113\n",
      "          27       0.81      0.80      0.80        94\n",
      "          28       0.96      0.94      0.95       114\n",
      "          29       0.97      0.89      0.93        88\n",
      "          30       0.97      0.89      0.93        87\n",
      "          31       0.54      0.56      0.55        99\n",
      "          32       0.89      0.77      0.83       109\n",
      "          33       0.64      0.89      0.75        81\n",
      "          34       0.88      0.85      0.87       107\n",
      "          35       0.92      0.92      0.92       106\n",
      "          36       0.81      0.71      0.76        98\n",
      "          37       0.76      0.70      0.73       115\n",
      "          38       0.84      0.73      0.78       107\n",
      "          39       0.79      0.74      0.77       101\n",
      "          40       0.81      0.81      0.81        95\n",
      "          41       0.70      0.70      0.70       102\n",
      "          42       0.87      0.84      0.86        88\n",
      "          43       0.84      0.76      0.80        78\n",
      "          44       0.82      0.84      0.83       110\n",
      "          45       0.93      0.94      0.94        90\n",
      "          46       0.93      0.93      0.93        92\n",
      "          47       0.75      0.80      0.77        90\n",
      "          48       0.89      0.71      0.79       114\n",
      "          49       0.77      0.82      0.79       111\n",
      "          50       0.53      0.44      0.48        95\n",
      "          51       1.00      1.00      1.00        53\n",
      "          52       0.96      0.95      0.95        77\n",
      "          53       0.86      0.79      0.82        97\n",
      "\n",
      "    accuracy                           0.81      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.81      0.81      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 17,  accuracy score is 0.8125\n",
      "at random state 17, confusion matrix is [[102   0   0 ...   0   0   0]\n",
      " [  0  69   0 ...   0   0   1]\n",
      " [  0   0 100 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  60   0   0]\n",
      " [  0   0   0 ...   0  63   0]\n",
      " [  0   0   1 ...   0   0  72]]\n",
      "at random state 17, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.91      0.84       112\n",
      "           1       0.60      0.72      0.65        96\n",
      "           2       0.89      0.88      0.89       113\n",
      "           3       0.78      0.74      0.76        86\n",
      "           4       0.83      0.89      0.86        95\n",
      "           5       0.81      0.69      0.75       104\n",
      "           6       0.98      0.95      0.96        92\n",
      "           7       0.74      0.88      0.81        99\n",
      "           8       0.92      0.94      0.93       105\n",
      "           9       0.65      0.80      0.72        91\n",
      "          10       0.65      0.72      0.68       107\n",
      "          11       0.87      0.96      0.91       107\n",
      "          12       0.86      0.92      0.89       102\n",
      "          13       0.67      0.68      0.68        98\n",
      "          14       0.88      0.90      0.89       116\n",
      "          15       0.96      0.92      0.94        88\n",
      "          16       0.93      0.88      0.91       109\n",
      "          17       0.90      0.83      0.87        90\n",
      "          18       0.93      0.88      0.90        90\n",
      "          19       0.79      0.82      0.80       110\n",
      "          20       0.92      0.84      0.87       104\n",
      "          21       0.61      0.69      0.64        96\n",
      "          22       0.94      0.97      0.96        99\n",
      "          23       0.76      0.74      0.75       105\n",
      "          24       0.61      0.56      0.58        90\n",
      "          25       0.84      0.98      0.91        93\n",
      "          26       0.69      0.78      0.73        97\n",
      "          27       0.85      0.80      0.82       105\n",
      "          28       0.93      0.88      0.90        97\n",
      "          29       0.97      0.94      0.95        88\n",
      "          30       0.93      0.88      0.90       107\n",
      "          31       0.48      0.54      0.51       101\n",
      "          32       0.91      0.84      0.87        94\n",
      "          33       0.74      0.70      0.72        91\n",
      "          34       0.90      0.81      0.86       124\n",
      "          35       0.89      0.90      0.90       105\n",
      "          36       0.74      0.73      0.73       100\n",
      "          37       0.71      0.66      0.68        96\n",
      "          38       0.80      0.75      0.77       109\n",
      "          39       0.82      0.69      0.75       108\n",
      "          40       0.74      0.88      0.80        97\n",
      "          41       0.79      0.64      0.71       105\n",
      "          42       0.91      0.92      0.92        89\n",
      "          43       0.87      0.75      0.81        97\n",
      "          44       0.77      0.84      0.80        95\n",
      "          45       0.96      0.93      0.94        83\n",
      "          46       0.94      0.91      0.93        91\n",
      "          47       0.82      0.82      0.82        88\n",
      "          48       0.78      0.70      0.74       105\n",
      "          49       0.84      0.89      0.86       107\n",
      "          50       0.47      0.46      0.46        90\n",
      "          51       1.00      1.00      1.00        60\n",
      "          52       0.98      0.98      0.98        64\n",
      "          53       0.89      0.68      0.77       106\n",
      "\n",
      "    accuracy                           0.81      5296\n",
      "   macro avg       0.82      0.81      0.81      5296\n",
      "weighted avg       0.82      0.81      0.81      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 18,  accuracy score is 0.8194864048338368\n",
      "at random state 18, confusion matrix is [[ 90   0   0 ...   0   0   0]\n",
      " [  0  80   0 ...   0   0   0]\n",
      " [  0   0 101 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  54   0   0]\n",
      " [  0   0   0 ...   0  60   0]\n",
      " [  0   1   2 ...   0   0  69]]\n",
      "at random state 18, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.91      0.85        99\n",
      "           1       0.65      0.77      0.70       104\n",
      "           2       0.83      0.94      0.89       107\n",
      "           3       0.82      0.78      0.80        88\n",
      "           4       0.86      0.88      0.87        90\n",
      "           5       0.82      0.81      0.82        97\n",
      "           6       0.92      0.92      0.92        99\n",
      "           7       0.78      0.89      0.83       104\n",
      "           8       0.94      0.92      0.93        86\n",
      "           9       0.68      0.73      0.70       103\n",
      "          10       0.73      0.73      0.73        98\n",
      "          11       0.86      0.92      0.89       106\n",
      "          12       0.92      0.85      0.89       109\n",
      "          13       0.68      0.68      0.68       101\n",
      "          14       0.86      0.88      0.87        93\n",
      "          15       0.97      0.94      0.96       104\n",
      "          16       0.89      0.90      0.90       102\n",
      "          17       0.92      0.84      0.88       102\n",
      "          18       0.84      0.85      0.84        89\n",
      "          19       0.84      0.74      0.79       116\n",
      "          20       0.93      0.89      0.91       100\n",
      "          21       0.65      0.75      0.70        93\n",
      "          22       0.93      0.97      0.95        88\n",
      "          23       0.80      0.83      0.81       109\n",
      "          24       0.59      0.62      0.60       109\n",
      "          25       0.91      0.95      0.93       111\n",
      "          26       0.65      0.81      0.72        98\n",
      "          27       0.87      0.83      0.85       117\n",
      "          28       0.98      0.84      0.90       106\n",
      "          29       0.93      0.91      0.92        86\n",
      "          30       0.88      0.88      0.88        98\n",
      "          31       0.66      0.62      0.64       100\n",
      "          32       0.85      0.82      0.84        90\n",
      "          33       0.73      0.87      0.80        95\n",
      "          34       0.86      0.91      0.89       102\n",
      "          35       0.88      0.96      0.92       100\n",
      "          36       0.74      0.72      0.73       110\n",
      "          37       0.76      0.80      0.78        96\n",
      "          38       0.86      0.74      0.79        99\n",
      "          39       0.77      0.78      0.78        97\n",
      "          40       0.82      0.81      0.81       111\n",
      "          41       0.75      0.71      0.73       105\n",
      "          42       0.89      0.79      0.84        96\n",
      "          43       0.87      0.72      0.79       105\n",
      "          44       0.79      0.87      0.83       100\n",
      "          45       0.94      0.89      0.92        85\n",
      "          46       0.85      0.94      0.89        85\n",
      "          47       0.81      0.68      0.74       104\n",
      "          48       0.91      0.71      0.80        99\n",
      "          49       0.82      0.85      0.83        94\n",
      "          50       0.53      0.42      0.47        93\n",
      "          51       1.00      1.00      1.00        54\n",
      "          52       0.97      0.92      0.94        65\n",
      "          53       0.79      0.70      0.74        99\n",
      "\n",
      "    accuracy                           0.82      5296\n",
      "   macro avg       0.83      0.82      0.82      5296\n",
      "weighted avg       0.82      0.82      0.82      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 19,  accuracy score is 0.8155211480362538\n",
      "at random state 19, confusion matrix is [[ 95   0   0 ...   0   0   0]\n",
      " [  0  72   0 ...   0   0   0]\n",
      " [  0   0 106 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  47   0   0]\n",
      " [  0   0   0 ...   1  78   0]\n",
      " [  0   0   2 ...   0   0  78]]\n",
      "at random state 19, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.87      0.83       109\n",
      "           1       0.61      0.67      0.64       107\n",
      "           2       0.88      0.97      0.93       109\n",
      "           3       0.92      0.74      0.82       124\n",
      "           4       0.87      0.95      0.91        97\n",
      "           5       0.76      0.71      0.74       100\n",
      "           6       0.92      0.94      0.93        86\n",
      "           7       0.72      0.86      0.78        95\n",
      "           8       0.96      0.87      0.91       101\n",
      "           9       0.70      0.82      0.76        97\n",
      "          10       0.60      0.74      0.67        86\n",
      "          11       0.83      0.90      0.86       105\n",
      "          12       0.96      0.87      0.91       113\n",
      "          13       0.71      0.69      0.70       102\n",
      "          14       0.88      0.98      0.92        93\n",
      "          15       0.92      0.90      0.91        78\n",
      "          16       0.87      0.92      0.90       104\n",
      "          17       0.94      0.83      0.88       109\n",
      "          18       0.89      0.83      0.86       111\n",
      "          19       0.76      0.78      0.77        98\n",
      "          20       0.89      0.85      0.87        96\n",
      "          21       0.64      0.68      0.66       103\n",
      "          22       0.93      0.98      0.95        91\n",
      "          23       0.69      0.84      0.76        88\n",
      "          24       0.68      0.58      0.63       122\n",
      "          25       0.86      0.99      0.92        87\n",
      "          26       0.74      0.77      0.76       109\n",
      "          27       0.86      0.90      0.88       105\n",
      "          28       0.93      0.89      0.91        92\n",
      "          29       0.97      0.94      0.95        94\n",
      "          30       0.96      0.86      0.91       103\n",
      "          31       0.55      0.74      0.63        98\n",
      "          32       0.85      0.81      0.83       121\n",
      "          33       0.69      0.88      0.77        92\n",
      "          34       0.87      0.87      0.87        83\n",
      "          35       0.89      0.95      0.92        98\n",
      "          36       0.85      0.69      0.76       107\n",
      "          37       0.75      0.79      0.77       103\n",
      "          38       0.86      0.77      0.81        94\n",
      "          39       0.84      0.84      0.84        92\n",
      "          40       0.78      0.88      0.82        99\n",
      "          41       0.71      0.63      0.67       100\n",
      "          42       0.89      0.85      0.87        88\n",
      "          43       0.85      0.75      0.80        91\n",
      "          44       0.82      0.77      0.80       101\n",
      "          45       0.98      0.91      0.94        92\n",
      "          46       0.93      0.90      0.91        89\n",
      "          47       0.74      0.81      0.77        93\n",
      "          48       0.79      0.65      0.72        89\n",
      "          49       0.76      0.82      0.79        92\n",
      "          50       0.64      0.34      0.44       125\n",
      "          51       0.98      1.00      0.99        47\n",
      "          52       0.97      0.91      0.94        86\n",
      "          53       0.84      0.76      0.80       102\n",
      "\n",
      "    accuracy                           0.82      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.82      0.81      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 20,  accuracy score is 0.8209969788519638\n",
      "at random state 20, confusion matrix is [[ 76   0   0 ...   0   0   0]\n",
      " [  0  68   0 ...   0   0   1]\n",
      " [  0   0 100 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  53   0   0]\n",
      " [  0   0   0 ...   0  65   0]\n",
      " [  0   0   2 ...   0   0  74]]\n",
      "at random state 20, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.92      0.84        83\n",
      "           1       0.61      0.74      0.67        92\n",
      "           2       0.88      0.93      0.90       108\n",
      "           3       0.82      0.79      0.80       112\n",
      "           4       0.81      0.93      0.86       102\n",
      "           5       0.80      0.68      0.74       114\n",
      "           6       0.93      0.97      0.95        87\n",
      "           7       0.74      0.88      0.81        95\n",
      "           8       0.93      0.93      0.93       119\n",
      "           9       0.68      0.74      0.71       100\n",
      "          10       0.73      0.66      0.70       116\n",
      "          11       0.86      0.91      0.89       111\n",
      "          12       0.90      0.88      0.89        85\n",
      "          13       0.67      0.61      0.64       106\n",
      "          14       0.89      0.92      0.91       106\n",
      "          15       0.95      0.95      0.95        94\n",
      "          16       0.87      0.90      0.88        93\n",
      "          17       0.91      0.81      0.86       118\n",
      "          18       0.83      0.85      0.84        97\n",
      "          19       0.77      0.80      0.78        91\n",
      "          20       0.89      0.84      0.87       108\n",
      "          21       0.70      0.73      0.72        97\n",
      "          22       0.99      0.92      0.95       107\n",
      "          23       0.75      0.75      0.75        87\n",
      "          24       0.59      0.68      0.63        92\n",
      "          25       0.87      0.99      0.92        91\n",
      "          26       0.72      0.75      0.73        96\n",
      "          27       0.89      0.79      0.84       102\n",
      "          28       0.97      0.92      0.95       105\n",
      "          29       0.99      0.93      0.96       111\n",
      "          30       0.93      0.86      0.89       101\n",
      "          31       0.54      0.54      0.54       100\n",
      "          32       0.90      0.83      0.86       107\n",
      "          33       0.79      0.79      0.79       107\n",
      "          34       0.85      0.94      0.89        82\n",
      "          35       0.92      0.92      0.92        86\n",
      "          36       0.83      0.75      0.79       106\n",
      "          37       0.82      0.70      0.76       101\n",
      "          38       0.78      0.79      0.79        86\n",
      "          39       0.79      0.87      0.83        97\n",
      "          40       0.81      0.75      0.78       106\n",
      "          41       0.71      0.75      0.73        95\n",
      "          42       0.84      0.82      0.83        87\n",
      "          43       0.88      0.83      0.85        99\n",
      "          44       0.86      0.88      0.87        99\n",
      "          45       0.92      0.97      0.94        93\n",
      "          46       0.91      0.92      0.92        91\n",
      "          47       0.77      0.83      0.80        96\n",
      "          48       0.82      0.66      0.73       106\n",
      "          49       0.81      0.84      0.83       109\n",
      "          50       0.52      0.48      0.50        98\n",
      "          51       1.00      1.00      1.00        53\n",
      "          52       0.98      0.93      0.96        70\n",
      "          53       0.87      0.77      0.82        96\n",
      "\n",
      "    accuracy                           0.82      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.82      0.82      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 21,  accuracy score is 0.8168429003021148\n",
      "at random state 21, confusion matrix is [[83  0  0 ...  0  0  0]\n",
      " [ 0 67  0 ...  0  0  1]\n",
      " [ 0  0 89 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 53  0  0]\n",
      " [ 0  0  0 ...  0 71  0]\n",
      " [ 0  0  2 ...  0  0 66]]\n",
      "at random state 21, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.89      0.76        93\n",
      "           1       0.66      0.74      0.69        91\n",
      "           2       0.85      0.96      0.90        93\n",
      "           3       0.84      0.78      0.81        89\n",
      "           4       0.84      0.91      0.87        99\n",
      "           5       0.75      0.76      0.76       108\n",
      "           6       0.94      0.99      0.96        79\n",
      "           7       0.74      0.82      0.78       110\n",
      "           8       0.95      0.93      0.94       115\n",
      "           9       0.67      0.82      0.74       101\n",
      "          10       0.65      0.73      0.69        92\n",
      "          11       0.83      0.91      0.87       104\n",
      "          12       0.92      0.89      0.91        97\n",
      "          13       0.70      0.69      0.69       107\n",
      "          14       0.90      0.89      0.89       118\n",
      "          15       0.97      0.98      0.97        92\n",
      "          16       0.91      0.94      0.92        97\n",
      "          17       0.87      0.83      0.85        96\n",
      "          18       0.88      0.87      0.87        98\n",
      "          19       0.74      0.79      0.76       109\n",
      "          20       0.86      0.81      0.83       103\n",
      "          21       0.73      0.76      0.74        98\n",
      "          22       0.90      0.96      0.93        95\n",
      "          23       0.76      0.79      0.77       108\n",
      "          24       0.61      0.57      0.59       105\n",
      "          25       0.91      0.97      0.94       100\n",
      "          26       0.71      0.73      0.72        99\n",
      "          27       0.83      0.78      0.80        99\n",
      "          28       0.95      0.89      0.92       102\n",
      "          29       0.93      0.93      0.93        89\n",
      "          30       0.93      0.83      0.88        95\n",
      "          31       0.53      0.58      0.55       103\n",
      "          32       0.93      0.84      0.88        93\n",
      "          33       0.69      0.80      0.74        99\n",
      "          34       0.91      0.88      0.90       118\n",
      "          35       0.93      0.94      0.94       115\n",
      "          36       0.77      0.80      0.79        97\n",
      "          37       0.72      0.67      0.69       100\n",
      "          38       0.82      0.74      0.78        86\n",
      "          39       0.87      0.76      0.81       115\n",
      "          40       0.83      0.84      0.83       102\n",
      "          41       0.80      0.70      0.75       107\n",
      "          42       0.93      0.85      0.89        89\n",
      "          43       0.88      0.73      0.80        96\n",
      "          44       0.76      0.87      0.81        75\n",
      "          45       0.98      0.91      0.94        93\n",
      "          46       0.91      0.94      0.93       100\n",
      "          47       0.80      0.77      0.78       108\n",
      "          48       0.84      0.72      0.77        86\n",
      "          49       0.83      0.77      0.80       110\n",
      "          50       0.55      0.42      0.48        97\n",
      "          51       1.00      1.00      1.00        53\n",
      "          52       0.99      0.92      0.95        77\n",
      "          53       0.81      0.69      0.75        96\n",
      "\n",
      "    accuracy                           0.82      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.82      0.82      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 22,  accuracy score is 0.8238293051359517\n",
      "at random state 22, confusion matrix is [[87  0  0 ...  0  0  0]\n",
      " [ 0 82  0 ...  0  0  4]\n",
      " [ 0  0 86 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 48  0  0]\n",
      " [ 0  0  0 ...  0 69  0]\n",
      " [ 0  0  1 ...  0  0 84]]\n",
      "at random state 22, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.95      0.88        92\n",
      "           1       0.71      0.72      0.71       114\n",
      "           2       0.90      0.89      0.89        97\n",
      "           3       0.79      0.90      0.85        94\n",
      "           4       0.85      0.88      0.86       101\n",
      "           5       0.85      0.70      0.76       102\n",
      "           6       0.95      0.98      0.96        84\n",
      "           7       0.75      0.82      0.78        97\n",
      "           8       0.97      0.91      0.94       107\n",
      "           9       0.73      0.72      0.72       103\n",
      "          10       0.71      0.75      0.73       103\n",
      "          11       0.86      0.89      0.87       109\n",
      "          12       0.96      0.87      0.91       100\n",
      "          13       0.64      0.77      0.70        96\n",
      "          14       0.88      0.91      0.90       110\n",
      "          15       0.95      0.92      0.94        78\n",
      "          16       0.90      0.92      0.91       105\n",
      "          17       0.92      0.85      0.88        98\n",
      "          18       0.85      0.79      0.82       100\n",
      "          19       0.82      0.84      0.83       102\n",
      "          20       0.93      0.88      0.90       107\n",
      "          21       0.66      0.74      0.70       100\n",
      "          22       0.99      0.94      0.97        88\n",
      "          23       0.79      0.77      0.78       102\n",
      "          24       0.64      0.61      0.63        93\n",
      "          25       0.84      0.97      0.90        94\n",
      "          26       0.76      0.78      0.77       108\n",
      "          27       0.90      0.83      0.87       112\n",
      "          28       0.95      0.94      0.95       103\n",
      "          29       0.97      0.96      0.96        92\n",
      "          30       0.91      0.85      0.88       114\n",
      "          31       0.57      0.63      0.60       101\n",
      "          32       0.81      0.86      0.84        96\n",
      "          33       0.74      0.74      0.74        94\n",
      "          34       0.91      0.88      0.89       100\n",
      "          35       0.88      0.94      0.91        97\n",
      "          36       0.74      0.76      0.75        90\n",
      "          37       0.76      0.78      0.77       111\n",
      "          38       0.78      0.74      0.76       111\n",
      "          39       0.80      0.75      0.77        96\n",
      "          40       0.81      0.85      0.83       104\n",
      "          41       0.71      0.66      0.68        97\n",
      "          42       0.84      0.85      0.84        93\n",
      "          43       0.86      0.75      0.80       101\n",
      "          44       0.76      0.77      0.76        88\n",
      "          45       0.93      0.94      0.94        85\n",
      "          46       0.92      0.93      0.93       103\n",
      "          47       0.87      0.75      0.81       105\n",
      "          48       0.86      0.68      0.76       107\n",
      "          49       0.80      0.88      0.84       101\n",
      "          50       0.51      0.51      0.51        85\n",
      "          51       1.00      1.00      1.00        48\n",
      "          52       0.99      0.86      0.92        80\n",
      "          53       0.79      0.86      0.82        98\n",
      "\n",
      "    accuracy                           0.82      5296\n",
      "   macro avg       0.83      0.83      0.83      5296\n",
      "weighted avg       0.83      0.82      0.82      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 23,  accuracy score is 0.8136329305135952\n",
      "at random state 23, confusion matrix is [[96  0  0 ...  0  0  0]\n",
      " [ 0 73  0 ...  0  0  1]\n",
      " [ 0  0 89 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 51  0  0]\n",
      " [ 0  0  0 ...  1 82  0]\n",
      " [ 0  0  0 ...  0  0 76]]\n",
      "at random state 23, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.91      0.83       106\n",
      "           1       0.66      0.77      0.71        95\n",
      "           2       0.92      0.94      0.93        95\n",
      "           3       0.88      0.72      0.79       110\n",
      "           4       0.78      0.86      0.82        94\n",
      "           5       0.75      0.79      0.77       100\n",
      "           6       0.91      0.94      0.92        85\n",
      "           7       0.74      0.89      0.81        94\n",
      "           8       0.96      0.93      0.94       124\n",
      "           9       0.69      0.75      0.72        89\n",
      "          10       0.65      0.74      0.69       111\n",
      "          11       0.83      0.90      0.86       106\n",
      "          12       0.88      0.93      0.91       103\n",
      "          13       0.66      0.66      0.66       105\n",
      "          14       0.88      0.95      0.91        81\n",
      "          15       0.91      0.92      0.91        73\n",
      "          16       0.91      0.92      0.91       106\n",
      "          17       0.89      0.80      0.85        92\n",
      "          18       0.89      0.81      0.85       106\n",
      "          19       0.78      0.82      0.80       101\n",
      "          20       0.89      0.81      0.85       101\n",
      "          21       0.59      0.74      0.65        88\n",
      "          22       0.98      0.93      0.96       106\n",
      "          23       0.76      0.81      0.79       102\n",
      "          24       0.65      0.65      0.65       108\n",
      "          25       0.88      0.98      0.93       104\n",
      "          26       0.74      0.75      0.74       108\n",
      "          27       0.91      0.85      0.88        94\n",
      "          28       0.96      0.96      0.96        92\n",
      "          29       0.98      0.93      0.95        99\n",
      "          30       0.91      0.77      0.84       105\n",
      "          31       0.55      0.63      0.59       103\n",
      "          32       0.86      0.81      0.84       102\n",
      "          33       0.74      0.83      0.78       100\n",
      "          34       0.93      0.92      0.92       100\n",
      "          35       0.87      0.90      0.88        97\n",
      "          36       0.75      0.70      0.73       104\n",
      "          37       0.77      0.78      0.77        94\n",
      "          38       0.85      0.82      0.84       106\n",
      "          39       0.83      0.74      0.79       101\n",
      "          40       0.77      0.81      0.79        93\n",
      "          41       0.74      0.73      0.74        96\n",
      "          42       0.93      0.83      0.88        95\n",
      "          43       0.74      0.70      0.72        81\n",
      "          44       0.87      0.83      0.85        89\n",
      "          45       0.93      0.89      0.91        93\n",
      "          46       0.92      0.92      0.92       105\n",
      "          47       0.76      0.73      0.74       100\n",
      "          48       0.78      0.67      0.72        83\n",
      "          49       0.85      0.82      0.83       103\n",
      "          50       0.46      0.38      0.41       112\n",
      "          51       0.98      1.00      0.99        51\n",
      "          52       0.95      0.93      0.94        88\n",
      "          53       0.86      0.65      0.74       117\n",
      "\n",
      "    accuracy                           0.81      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.81      0.81      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 24,  accuracy score is 0.8136329305135952\n",
      "at random state 24, confusion matrix is [[90  0  0 ...  0  0  0]\n",
      " [ 0 67  0 ...  0  0  3]\n",
      " [ 0  0 97 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 58  0  0]\n",
      " [ 0  0  0 ...  0 67  0]\n",
      " [ 0  0  1 ...  0  0 77]]\n",
      "at random state 24, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.82      0.79       110\n",
      "           1       0.56      0.74      0.64        90\n",
      "           2       0.84      0.95      0.89       102\n",
      "           3       0.85      0.79      0.82       103\n",
      "           4       0.90      0.86      0.88       112\n",
      "           5       0.75      0.72      0.73       111\n",
      "           6       0.95      0.92      0.93       100\n",
      "           7       0.76      0.83      0.79       107\n",
      "           8       0.97      0.88      0.92        98\n",
      "           9       0.78      0.78      0.78       116\n",
      "          10       0.57      0.60      0.58        99\n",
      "          11       0.83      0.94      0.88        83\n",
      "          12       0.94      0.87      0.90        97\n",
      "          13       0.71      0.70      0.70        96\n",
      "          14       0.90      0.94      0.92       100\n",
      "          15       0.95      0.94      0.94        64\n",
      "          16       0.89      0.95      0.92        99\n",
      "          17       0.89      0.89      0.89        95\n",
      "          18       0.94      0.87      0.90       106\n",
      "          19       0.73      0.78      0.75       108\n",
      "          20       0.89      0.83      0.86       101\n",
      "          21       0.70      0.75      0.72       102\n",
      "          22       0.92      0.95      0.93       103\n",
      "          23       0.74      0.78      0.76        96\n",
      "          24       0.67      0.59      0.63       103\n",
      "          25       0.93      0.98      0.95       103\n",
      "          26       0.69      0.61      0.65       100\n",
      "          27       0.89      0.85      0.87       110\n",
      "          28       0.98      0.92      0.95        92\n",
      "          29       0.98      0.94      0.96        86\n",
      "          30       0.90      0.88      0.89       102\n",
      "          31       0.51      0.63      0.56        91\n",
      "          32       0.94      0.78      0.85       100\n",
      "          33       0.67      0.79      0.72        89\n",
      "          34       0.89      0.88      0.88        96\n",
      "          35       0.90      0.94      0.92        94\n",
      "          36       0.72      0.67      0.70       101\n",
      "          37       0.74      0.81      0.77        91\n",
      "          38       0.81      0.70      0.75       118\n",
      "          39       0.84      0.85      0.85       122\n",
      "          40       0.78      0.84      0.81       101\n",
      "          41       0.68      0.65      0.66        91\n",
      "          42       0.91      0.82      0.86       101\n",
      "          43       0.72      0.76      0.74        86\n",
      "          44       0.77      0.80      0.78        99\n",
      "          45       0.94      0.93      0.94        90\n",
      "          46       0.94      0.90      0.92       101\n",
      "          47       0.77      0.79      0.78       102\n",
      "          48       0.81      0.66      0.73       104\n",
      "          49       0.74      0.81      0.77       103\n",
      "          50       0.61      0.52      0.56        99\n",
      "          51       1.00      1.00      1.00        58\n",
      "          52       0.94      0.96      0.95        70\n",
      "          53       0.86      0.81      0.83        95\n",
      "\n",
      "    accuracy                           0.81      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.81      0.81      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 25,  accuracy score is 0.8175981873111783\n",
      "at random state 25, confusion matrix is [[102   0   0 ...   0   0   0]\n",
      " [  0  73   0 ...   0   0   1]\n",
      " [  0   0 107 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  45   0   0]\n",
      " [  0   0   0 ...   0  77   0]\n",
      " [  0   0   1 ...   0   0  61]]\n",
      "at random state 25, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.92      0.86       111\n",
      "           1       0.65      0.75      0.70        97\n",
      "           2       0.86      0.95      0.90       113\n",
      "           3       0.85      0.78      0.81       114\n",
      "           4       0.90      0.93      0.91        95\n",
      "           5       0.76      0.78      0.77        99\n",
      "           6       0.88      0.99      0.93        82\n",
      "           7       0.72      0.85      0.78        92\n",
      "           8       0.94      0.92      0.93        99\n",
      "           9       0.74      0.72      0.73       112\n",
      "          10       0.73      0.66      0.69       112\n",
      "          11       0.79      0.88      0.83        96\n",
      "          12       0.92      0.88      0.90       107\n",
      "          13       0.61      0.73      0.66       102\n",
      "          14       0.88      0.85      0.86       101\n",
      "          15       0.91      0.91      0.91        65\n",
      "          16       0.96      0.88      0.92       103\n",
      "          17       0.85      0.77      0.81        92\n",
      "          18       0.89      0.75      0.82       101\n",
      "          19       0.78      0.84      0.81       103\n",
      "          20       0.91      0.89      0.90        89\n",
      "          21       0.68      0.72      0.70        99\n",
      "          22       0.99      0.93      0.96        99\n",
      "          23       0.75      0.84      0.79       107\n",
      "          24       0.59      0.62      0.61        96\n",
      "          25       0.80      0.94      0.87        86\n",
      "          26       0.76      0.74      0.75       113\n",
      "          27       0.89      0.84      0.87        96\n",
      "          28       1.00      0.93      0.96       107\n",
      "          29       0.98      0.89      0.93       110\n",
      "          30       0.95      0.87      0.91        84\n",
      "          31       0.65      0.70      0.68       103\n",
      "          32       0.88      0.86      0.87       101\n",
      "          33       0.64      0.69      0.66       103\n",
      "          34       0.88      0.96      0.92        95\n",
      "          35       0.89      0.94      0.92       103\n",
      "          36       0.74      0.70      0.72       115\n",
      "          37       0.81      0.75      0.78       107\n",
      "          38       0.81      0.81      0.81       100\n",
      "          39       0.79      0.85      0.82        99\n",
      "          40       0.72      0.87      0.79        93\n",
      "          41       0.74      0.67      0.70        94\n",
      "          42       0.84      0.83      0.83        88\n",
      "          43       0.83      0.70      0.76       108\n",
      "          44       0.88      0.87      0.88       106\n",
      "          45       0.99      0.88      0.93        85\n",
      "          46       0.93      0.97      0.95        86\n",
      "          47       0.84      0.82      0.83       108\n",
      "          48       0.92      0.63      0.75       105\n",
      "          49       0.86      0.81      0.83       112\n",
      "          50       0.55      0.45      0.50        95\n",
      "          51       1.00      1.00      1.00        45\n",
      "          52       0.95      0.95      0.95        81\n",
      "          53       0.71      0.74      0.73        82\n",
      "\n",
      "    accuracy                           0.82      5296\n",
      "   macro avg       0.83      0.82      0.82      5296\n",
      "weighted avg       0.82      0.82      0.82      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 26,  accuracy score is 0.8194864048338368\n",
      "at random state 26, confusion matrix is [[101   0   0 ...   0   0   0]\n",
      " [  0  69   0 ...   0   0   1]\n",
      " [  0   0 104 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  51   0   0]\n",
      " [  0   0   0 ...   1  63   0]\n",
      " [  0   1   1 ...   0   0  71]]\n",
      "at random state 26, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.90      0.87       112\n",
      "           1       0.62      0.66      0.64       104\n",
      "           2       0.89      0.95      0.92       109\n",
      "           3       0.91      0.76      0.83       108\n",
      "           4       0.85      0.89      0.87        96\n",
      "           5       0.71      0.73      0.72        94\n",
      "           6       0.98      0.95      0.97        87\n",
      "           7       0.75      0.78      0.77       102\n",
      "           8       0.94      0.96      0.95       101\n",
      "           9       0.62      0.75      0.68        97\n",
      "          10       0.71      0.68      0.69       116\n",
      "          11       0.83      0.90      0.86       115\n",
      "          12       0.96      0.92      0.94        88\n",
      "          13       0.73      0.73      0.73       113\n",
      "          14       0.86      0.93      0.89        94\n",
      "          15       0.98      0.93      0.95        85\n",
      "          16       0.89      0.94      0.92        87\n",
      "          17       0.93      0.81      0.87       112\n",
      "          18       0.76      0.86      0.81       101\n",
      "          19       0.78      0.77      0.78       115\n",
      "          20       0.90      0.89      0.90        83\n",
      "          21       0.66      0.72      0.69       105\n",
      "          22       0.95      0.98      0.96        88\n",
      "          23       0.78      0.86      0.82        86\n",
      "          24       0.68      0.58      0.63       108\n",
      "          25       0.92      0.97      0.94       106\n",
      "          26       0.74      0.72      0.73       105\n",
      "          27       0.85      0.79      0.82        92\n",
      "          28       0.97      0.89      0.93       115\n",
      "          29       0.96      0.98      0.97       101\n",
      "          30       0.92      0.89      0.90       107\n",
      "          31       0.58      0.58      0.58       106\n",
      "          32       0.83      0.79      0.81        94\n",
      "          33       0.74      0.78      0.76       105\n",
      "          34       0.93      0.96      0.95        82\n",
      "          35       0.91      0.92      0.92       102\n",
      "          36       0.74      0.76      0.75       106\n",
      "          37       0.70      0.79      0.74        94\n",
      "          38       0.85      0.75      0.80       112\n",
      "          39       0.78      0.81      0.79        99\n",
      "          40       0.82      0.84      0.83        99\n",
      "          41       0.73      0.69      0.71        94\n",
      "          42       0.93      0.87      0.90        86\n",
      "          43       0.87      0.76      0.81       104\n",
      "          44       0.84      0.86      0.85       109\n",
      "          45       0.95      0.93      0.94        85\n",
      "          46       0.92      0.91      0.91        87\n",
      "          47       0.78      0.82      0.80        98\n",
      "          48       0.83      0.62      0.71       105\n",
      "          49       0.79      0.85      0.82        94\n",
      "          50       0.48      0.47      0.48        89\n",
      "          51       0.98      1.00      0.99        51\n",
      "          52       0.94      0.93      0.93        68\n",
      "          53       0.86      0.75      0.80        95\n",
      "\n",
      "    accuracy                           0.82      5296\n",
      "   macro avg       0.83      0.82      0.82      5296\n",
      "weighted avg       0.82      0.82      0.82      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 27,  accuracy score is 0.8164652567975831\n",
      "at random state 27, confusion matrix is [[101   0   0 ...   0   0   0]\n",
      " [  0  71   0 ...   0   0   2]\n",
      " [  0   0 102 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  57   0   0]\n",
      " [  0   0   0 ...   1  85   0]\n",
      " [  0   0   2 ...   0   0  75]]\n",
      "at random state 27, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.88      0.84       115\n",
      "           1       0.67      0.70      0.68       102\n",
      "           2       0.82      0.91      0.86       112\n",
      "           3       0.87      0.82      0.84        99\n",
      "           4       0.85      0.86      0.85       112\n",
      "           5       0.79      0.73      0.76        92\n",
      "           6       0.92      0.95      0.94        86\n",
      "           7       0.72      0.86      0.79        87\n",
      "           8       0.90      0.93      0.91       108\n",
      "           9       0.66      0.73      0.69        95\n",
      "          10       0.68      0.75      0.71       103\n",
      "          11       0.77      0.91      0.83       102\n",
      "          12       0.96      0.90      0.93       111\n",
      "          13       0.67      0.66      0.66       103\n",
      "          14       0.92      0.92      0.92       106\n",
      "          15       0.92      0.92      0.92        71\n",
      "          16       0.93      0.89      0.91       106\n",
      "          17       0.91      0.84      0.87       108\n",
      "          18       0.87      0.79      0.83       103\n",
      "          19       0.81      0.87      0.84       101\n",
      "          20       0.83      0.88      0.85        92\n",
      "          21       0.63      0.65      0.64       110\n",
      "          22       0.98      0.92      0.95        97\n",
      "          23       0.83      0.86      0.84       101\n",
      "          24       0.55      0.55      0.55        95\n",
      "          25       0.90      0.96      0.93       100\n",
      "          26       0.69      0.74      0.72        86\n",
      "          27       0.86      0.80      0.83        93\n",
      "          28       0.98      0.91      0.94        90\n",
      "          29       0.95      0.95      0.95        93\n",
      "          30       0.89      0.86      0.88        99\n",
      "          31       0.56      0.65      0.60        92\n",
      "          32       0.89      0.83      0.86       106\n",
      "          33       0.68      0.69      0.69        94\n",
      "          34       0.91      0.90      0.90       106\n",
      "          35       0.90      0.97      0.94        98\n",
      "          36       0.72      0.78      0.75        93\n",
      "          37       0.80      0.72      0.76       102\n",
      "          38       0.77      0.77      0.77        96\n",
      "          39       0.78      0.81      0.80        91\n",
      "          40       0.75      0.85      0.79       104\n",
      "          41       0.76      0.66      0.71       108\n",
      "          42       0.89      0.79      0.84        91\n",
      "          43       0.89      0.73      0.80       111\n",
      "          44       0.82      0.88      0.84       112\n",
      "          45       0.91      0.98      0.94        87\n",
      "          46       0.92      0.90      0.91        90\n",
      "          47       0.89      0.79      0.83        98\n",
      "          48       0.82      0.70      0.75        96\n",
      "          49       0.76      0.83      0.79        98\n",
      "          50       0.53      0.42      0.47        99\n",
      "          51       0.98      1.00      0.99        57\n",
      "          52       0.98      0.93      0.96        91\n",
      "          53       0.87      0.77      0.82        98\n",
      "\n",
      "    accuracy                           0.82      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.82      0.82      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 28,  accuracy score is 0.8143882175226587\n",
      "at random state 28, confusion matrix is [[100   0   0 ...   0   0   0]\n",
      " [  0  74   0 ...   0   0   0]\n",
      " [  0   0  87 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  46   0   0]\n",
      " [  0   0   0 ...   1  73   0]\n",
      " [  0   0   0 ...   0   0  60]]\n",
      "at random state 28, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.85      0.84       117\n",
      "           1       0.63      0.79      0.70        94\n",
      "           2       0.84      0.94      0.88        93\n",
      "           3       0.88      0.81      0.84        97\n",
      "           4       0.88      0.89      0.88       105\n",
      "           5       0.67      0.76      0.71        88\n",
      "           6       0.96      0.92      0.94        96\n",
      "           7       0.73      0.92      0.82        90\n",
      "           8       0.97      0.91      0.94        96\n",
      "           9       0.79      0.81      0.80       105\n",
      "          10       0.71      0.75      0.73        93\n",
      "          11       0.91      0.92      0.92       105\n",
      "          12       0.94      0.83      0.88       107\n",
      "          13       0.61      0.71      0.65        96\n",
      "          14       0.89      0.95      0.92       100\n",
      "          15       0.93      0.93      0.93        80\n",
      "          16       0.88      0.92      0.90        95\n",
      "          17       0.89      0.86      0.88       103\n",
      "          18       0.90      0.90      0.90       111\n",
      "          19       0.81      0.85      0.83        97\n",
      "          20       0.89      0.87      0.88       100\n",
      "          21       0.66      0.68      0.67       111\n",
      "          22       0.94      0.91      0.92       107\n",
      "          23       0.74      0.78      0.76        88\n",
      "          24       0.67      0.56      0.61       113\n",
      "          25       0.89      0.93      0.91       100\n",
      "          26       0.68      0.71      0.70       108\n",
      "          27       0.82      0.82      0.82        93\n",
      "          28       0.97      0.90      0.94       113\n",
      "          29       0.92      0.98      0.95        97\n",
      "          30       0.93      0.85      0.89        96\n",
      "          31       0.62      0.61      0.61       104\n",
      "          32       0.88      0.81      0.84       105\n",
      "          33       0.68      0.74      0.71       105\n",
      "          34       0.94      0.87      0.90       108\n",
      "          35       0.89      0.94      0.91        90\n",
      "          36       0.68      0.72      0.70        94\n",
      "          37       0.78      0.75      0.77       101\n",
      "          38       0.82      0.74      0.78       110\n",
      "          39       0.80      0.78      0.79        98\n",
      "          40       0.79      0.79      0.79       103\n",
      "          41       0.73      0.71      0.72        95\n",
      "          42       0.87      0.84      0.85        92\n",
      "          43       0.85      0.76      0.80        95\n",
      "          44       0.79      0.77      0.78       106\n",
      "          45       0.88      0.91      0.90        89\n",
      "          46       0.99      0.92      0.95       100\n",
      "          47       0.76      0.70      0.73        97\n",
      "          48       0.85      0.65      0.74       106\n",
      "          49       0.83      0.81      0.82       110\n",
      "          50       0.42      0.47      0.44        85\n",
      "          51       0.98      1.00      0.99        46\n",
      "          52       0.94      0.94      0.94        78\n",
      "          53       0.75      0.71      0.73        85\n",
      "\n",
      "    accuracy                           0.81      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.81      0.82      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 29,  accuracy score is 0.8126888217522659\n",
      "at random state 29, confusion matrix is [[95  0  0 ...  0  0  0]\n",
      " [ 0 70  0 ...  0  0  3]\n",
      " [ 0  0 75 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 55  0  0]\n",
      " [ 0  0  0 ...  0 67  0]\n",
      " [ 0  0  1 ...  0  0 67]]\n",
      "at random state 29, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.95      0.86       100\n",
      "           1       0.64      0.74      0.69        95\n",
      "           2       0.80      0.94      0.86        80\n",
      "           3       0.87      0.78      0.82        93\n",
      "           4       0.83      0.92      0.87        96\n",
      "           5       0.85      0.67      0.75       101\n",
      "           6       0.96      0.95      0.96       101\n",
      "           7       0.76      0.79      0.77        86\n",
      "           8       0.92      0.94      0.93        96\n",
      "           9       0.76      0.78      0.77       125\n",
      "          10       0.67      0.83      0.74       109\n",
      "          11       0.80      0.94      0.87        99\n",
      "          12       0.92      0.87      0.89        90\n",
      "          13       0.65      0.65      0.65       101\n",
      "          14       0.90      0.95      0.92        97\n",
      "          15       0.93      0.95      0.94        87\n",
      "          16       0.86      0.91      0.88        87\n",
      "          17       0.95      0.89      0.92       101\n",
      "          18       0.89      0.73      0.80       108\n",
      "          19       0.78      0.84      0.81       104\n",
      "          20       0.89      0.87      0.88       107\n",
      "          21       0.66      0.66      0.66       111\n",
      "          22       0.93      0.95      0.94       104\n",
      "          23       0.78      0.85      0.81        87\n",
      "          24       0.56      0.55      0.56        96\n",
      "          25       0.85      0.95      0.90       105\n",
      "          26       0.82      0.71      0.76       110\n",
      "          27       0.81      0.90      0.85        91\n",
      "          28       0.96      0.85      0.90       121\n",
      "          29       0.92      0.94      0.93       101\n",
      "          30       0.93      0.87      0.90       104\n",
      "          31       0.55      0.61      0.58        93\n",
      "          32       0.89      0.83      0.86       109\n",
      "          33       0.63      0.81      0.71        95\n",
      "          34       0.90      0.87      0.88        89\n",
      "          35       0.90      0.93      0.92       106\n",
      "          36       0.78      0.70      0.74       109\n",
      "          37       0.69      0.83      0.76        90\n",
      "          38       0.78      0.70      0.74       105\n",
      "          39       0.82      0.82      0.82       102\n",
      "          40       0.77      0.86      0.81       104\n",
      "          41       0.75      0.67      0.71        98\n",
      "          42       0.92      0.75      0.83       113\n",
      "          43       0.84      0.81      0.83        96\n",
      "          44       0.78      0.83      0.80        87\n",
      "          45       0.89      0.93      0.91        91\n",
      "          46       0.91      0.82      0.86        97\n",
      "          47       0.84      0.80      0.82        85\n",
      "          48       0.78      0.63      0.70       101\n",
      "          49       0.81      0.82      0.81        94\n",
      "          50       0.53      0.41      0.46       108\n",
      "          51       1.00      1.00      1.00        55\n",
      "          52       0.97      0.86      0.91        78\n",
      "          53       0.78      0.68      0.73        98\n",
      "\n",
      "    accuracy                           0.81      5296\n",
      "   macro avg       0.82      0.82      0.81      5296\n",
      "weighted avg       0.82      0.81      0.81      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 30,  accuracy score is 0.8132552870090635\n",
      "at random state 30, confusion matrix is [[92  0  0 ...  0  0  0]\n",
      " [ 0 73  0 ...  0  0  1]\n",
      " [ 0  0 90 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 56  0  0]\n",
      " [ 0  0  0 ...  0 77  0]\n",
      " [ 0  0  1 ...  0  0 76]]\n",
      "at random state 30, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.90      0.83       102\n",
      "           1       0.62      0.71      0.66       103\n",
      "           2       0.84      0.89      0.87       101\n",
      "           3       0.85      0.82      0.84        96\n",
      "           4       0.91      0.89      0.90       119\n",
      "           5       0.73      0.75      0.74       109\n",
      "           6       0.95      0.96      0.96        85\n",
      "           7       0.72      0.88      0.79        90\n",
      "           8       0.93      0.94      0.94       109\n",
      "           9       0.74      0.71      0.73       108\n",
      "          10       0.62      0.70      0.66       110\n",
      "          11       0.78      0.95      0.86       103\n",
      "          12       0.92      0.90      0.91       105\n",
      "          13       0.66      0.78      0.71        94\n",
      "          14       0.87      0.90      0.89        99\n",
      "          15       0.92      0.95      0.94        85\n",
      "          16       0.91      0.91      0.91       116\n",
      "          17       0.87      0.85      0.86        96\n",
      "          18       0.82      0.85      0.83       100\n",
      "          19       0.77      0.84      0.80       101\n",
      "          20       0.89      0.80      0.84        99\n",
      "          21       0.66      0.74      0.70        93\n",
      "          22       0.96      0.96      0.96        99\n",
      "          23       0.78      0.88      0.83        84\n",
      "          24       0.64      0.65      0.64        99\n",
      "          25       0.88      0.91      0.89        96\n",
      "          26       0.66      0.65      0.66       101\n",
      "          27       0.83      0.76      0.79       101\n",
      "          28       0.92      0.90      0.91       103\n",
      "          29       0.95      0.94      0.94        94\n",
      "          30       0.95      0.87      0.91       108\n",
      "          31       0.61      0.64      0.62        94\n",
      "          32       0.84      0.79      0.81        97\n",
      "          33       0.77      0.79      0.78       107\n",
      "          34       0.91      0.91      0.91       103\n",
      "          35       0.95      0.93      0.94       114\n",
      "          36       0.74      0.76      0.75        92\n",
      "          37       0.71      0.64      0.67        91\n",
      "          38       0.72      0.78      0.75        94\n",
      "          39       0.79      0.82      0.81       107\n",
      "          40       0.77      0.85      0.81        85\n",
      "          41       0.76      0.66      0.70        99\n",
      "          42       0.86      0.86      0.86        87\n",
      "          43       0.89      0.71      0.79        96\n",
      "          44       0.88      0.76      0.81       119\n",
      "          45       0.87      0.93      0.90        86\n",
      "          46       0.95      0.89      0.92        90\n",
      "          47       0.74      0.68      0.71        84\n",
      "          48       0.80      0.60      0.68        99\n",
      "          49       0.82      0.73      0.78       109\n",
      "          50       0.56      0.48      0.52        89\n",
      "          51       1.00      1.00      1.00        56\n",
      "          52       0.97      0.96      0.97        80\n",
      "          53       0.87      0.69      0.77       110\n",
      "\n",
      "    accuracy                           0.81      5296\n",
      "   macro avg       0.82      0.82      0.81      5296\n",
      "weighted avg       0.82      0.81      0.81      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 31,  accuracy score is 0.8145770392749244\n",
      "at random state 31, confusion matrix is [[97  0  0 ...  0  0  0]\n",
      " [ 0 74  0 ...  0  0  0]\n",
      " [ 0  0 90 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 47  0  0]\n",
      " [ 0  0  0 ...  0 68  0]\n",
      " [ 0  0  0 ...  0  0 80]]\n",
      "at random state 31, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.93      0.85       104\n",
      "           1       0.63      0.74      0.68       100\n",
      "           2       0.87      0.97      0.92        93\n",
      "           3       0.80      0.80      0.80        83\n",
      "           4       0.86      0.91      0.88       101\n",
      "           5       0.75      0.66      0.70       108\n",
      "           6       0.99      0.90      0.94        99\n",
      "           7       0.76      0.81      0.78        96\n",
      "           8       0.95      0.95      0.95        94\n",
      "           9       0.72      0.80      0.75        98\n",
      "          10       0.65      0.76      0.70       102\n",
      "          11       0.87      0.93      0.90       107\n",
      "          12       0.88      0.86      0.87        98\n",
      "          13       0.72      0.72      0.72       112\n",
      "          14       0.89      0.95      0.92        98\n",
      "          15       0.94      0.93      0.93        83\n",
      "          16       0.91      0.91      0.91       110\n",
      "          17       0.90      0.85      0.88        99\n",
      "          18       0.90      0.79      0.84       122\n",
      "          19       0.75      0.87      0.81        95\n",
      "          20       0.87      0.83      0.85       108\n",
      "          21       0.66      0.76      0.70        94\n",
      "          22       0.96      0.98      0.97       105\n",
      "          23       0.83      0.73      0.78       118\n",
      "          24       0.64      0.59      0.61        99\n",
      "          25       0.81      0.97      0.88        94\n",
      "          26       0.77      0.69      0.73       104\n",
      "          27       0.84      0.76      0.79       115\n",
      "          28       0.96      0.89      0.92        90\n",
      "          29       0.95      0.92      0.93       101\n",
      "          30       0.92      0.87      0.90        94\n",
      "          31       0.56      0.65      0.60        96\n",
      "          32       0.88      0.78      0.83       106\n",
      "          33       0.67      0.72      0.69        97\n",
      "          34       0.88      0.89      0.89       104\n",
      "          35       0.90      0.95      0.93        99\n",
      "          36       0.74      0.62      0.67        94\n",
      "          37       0.75      0.75      0.75       104\n",
      "          38       0.75      0.82      0.78        92\n",
      "          39       0.77      0.77      0.77        93\n",
      "          40       0.84      0.76      0.80        97\n",
      "          41       0.77      0.72      0.75       105\n",
      "          42       0.85      0.80      0.83        95\n",
      "          43       0.82      0.77      0.80        97\n",
      "          44       0.84      0.77      0.81       113\n",
      "          45       0.95      0.93      0.94        90\n",
      "          46       0.93      0.94      0.94        89\n",
      "          47       0.76      0.78      0.77       102\n",
      "          48       0.85      0.65      0.73        93\n",
      "          49       0.77      0.82      0.79        91\n",
      "          50       0.51      0.49      0.50       105\n",
      "          51       1.00      1.00      1.00        47\n",
      "          52       0.99      0.96      0.97        71\n",
      "          53       0.83      0.87      0.85        92\n",
      "\n",
      "    accuracy                           0.81      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.81      0.81      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 32,  accuracy score is 0.8125\n",
      "at random state 32, confusion matrix is [[92  0  0 ...  0  0  0]\n",
      " [ 0 65  0 ...  0  0  1]\n",
      " [ 0  0 86 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 56  0  0]\n",
      " [ 0  0  0 ...  0 76  0]\n",
      " [ 0  0  3 ...  0  0 68]]\n",
      "at random state 32, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.88      0.85       104\n",
      "           1       0.53      0.74      0.62        88\n",
      "           2       0.85      0.91      0.88        95\n",
      "           3       0.94      0.72      0.82       101\n",
      "           4       0.88      0.91      0.90       101\n",
      "           5       0.76      0.73      0.74       103\n",
      "           6       0.94      0.96      0.95        70\n",
      "           7       0.75      0.89      0.81        97\n",
      "           8       0.96      0.87      0.91       102\n",
      "           9       0.69      0.76      0.72        92\n",
      "          10       0.68      0.75      0.71       107\n",
      "          11       0.84      0.89      0.86       107\n",
      "          12       0.96      0.86      0.91       101\n",
      "          13       0.70      0.76      0.73        88\n",
      "          14       0.89      0.92      0.91       114\n",
      "          15       0.96      0.87      0.91        77\n",
      "          16       0.89      0.92      0.91       104\n",
      "          17       0.92      0.84      0.88       102\n",
      "          18       0.80      0.86      0.83        83\n",
      "          19       0.78      0.79      0.79        95\n",
      "          20       0.91      0.83      0.87       108\n",
      "          21       0.70      0.77      0.73       100\n",
      "          22       0.95      0.94      0.94        94\n",
      "          23       0.76      0.89      0.82        93\n",
      "          24       0.56      0.61      0.58       100\n",
      "          25       0.86      0.98      0.92       103\n",
      "          26       0.69      0.76      0.72       105\n",
      "          27       0.82      0.84      0.83        95\n",
      "          28       1.00      0.89      0.94        92\n",
      "          29       0.94      0.97      0.95        96\n",
      "          30       0.94      0.87      0.90       106\n",
      "          31       0.57      0.56      0.56        99\n",
      "          32       0.85      0.85      0.85        99\n",
      "          33       0.77      0.69      0.73       112\n",
      "          34       0.90      0.88      0.89        96\n",
      "          35       0.90      0.92      0.91       103\n",
      "          36       0.75      0.62      0.68       110\n",
      "          37       0.76      0.71      0.73       110\n",
      "          38       0.77      0.83      0.80        96\n",
      "          39       0.79      0.80      0.79       110\n",
      "          40       0.78      0.84      0.81        95\n",
      "          41       0.77      0.68      0.72       103\n",
      "          42       0.90      0.85      0.88        82\n",
      "          43       0.79      0.73      0.76       105\n",
      "          44       0.86      0.79      0.82       100\n",
      "          45       0.95      0.94      0.94        99\n",
      "          46       0.87      0.90      0.88        89\n",
      "          47       0.77      0.75      0.76       102\n",
      "          48       0.88      0.66      0.75       118\n",
      "          49       0.81      0.81      0.81       112\n",
      "          50       0.51      0.46      0.48       104\n",
      "          51       1.00      1.00      1.00        56\n",
      "          52       0.99      0.93      0.96        82\n",
      "          53       0.72      0.75      0.74        91\n",
      "\n",
      "    accuracy                           0.81      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.81      0.81      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 33,  accuracy score is 0.8213746223564955\n",
      "at random state 33, confusion matrix is [[ 95   0   0 ...   0   0   0]\n",
      " [  0  80   0 ...   0   0   1]\n",
      " [  0   0 110 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  58   0   0]\n",
      " [  0   0   0 ...   0  73   0]\n",
      " [  0   0   1 ...   0   0  76]]\n",
      "at random state 33, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.92      0.82       103\n",
      "           1       0.73      0.75      0.74       107\n",
      "           2       0.88      0.96      0.92       115\n",
      "           3       0.83      0.81      0.82        99\n",
      "           4       0.88      0.88      0.88       104\n",
      "           5       0.77      0.78      0.78       106\n",
      "           6       0.91      0.98      0.94        90\n",
      "           7       0.83      0.85      0.84       117\n",
      "           8       0.96      0.95      0.95        92\n",
      "           9       0.71      0.81      0.76        97\n",
      "          10       0.61      0.72      0.66        94\n",
      "          11       0.85      0.91      0.88       116\n",
      "          12       0.94      0.93      0.94       106\n",
      "          13       0.73      0.76      0.74        97\n",
      "          14       0.90      0.97      0.93       101\n",
      "          15       0.96      0.94      0.95        77\n",
      "          16       0.80      0.86      0.83        86\n",
      "          17       0.89      0.81      0.85        97\n",
      "          18       0.86      0.81      0.83        98\n",
      "          19       0.81      0.73      0.77       108\n",
      "          20       0.90      0.82      0.86       100\n",
      "          21       0.57      0.76      0.65        90\n",
      "          22       0.96      0.93      0.95       104\n",
      "          23       0.77      0.76      0.77       102\n",
      "          24       0.69      0.59      0.63       100\n",
      "          25       0.91      0.98      0.94       104\n",
      "          26       0.76      0.77      0.77       105\n",
      "          27       0.83      0.85      0.84        96\n",
      "          28       0.97      0.89      0.93       101\n",
      "          29       0.92      0.94      0.93        87\n",
      "          30       0.91      0.88      0.89        89\n",
      "          31       0.55      0.65      0.60        98\n",
      "          32       0.88      0.79      0.83       104\n",
      "          33       0.72      0.83      0.77        95\n",
      "          34       0.89      0.86      0.87        99\n",
      "          35       0.96      0.94      0.95       123\n",
      "          36       0.70      0.70      0.70       101\n",
      "          37       0.73      0.78      0.76        97\n",
      "          38       0.80      0.80      0.80        92\n",
      "          39       0.81      0.82      0.81        88\n",
      "          40       0.81      0.81      0.81        94\n",
      "          41       0.82      0.64      0.72       120\n",
      "          42       0.84      0.84      0.84        76\n",
      "          43       0.82      0.73      0.77        95\n",
      "          44       0.90      0.82      0.86       107\n",
      "          45       0.95      0.95      0.95        79\n",
      "          46       0.88      0.92      0.90        91\n",
      "          47       0.82      0.74      0.78       102\n",
      "          48       0.82      0.77      0.79        95\n",
      "          49       0.84      0.76      0.80       100\n",
      "          50       0.55      0.43      0.48       112\n",
      "          51       1.00      1.00      1.00        58\n",
      "          52       0.97      0.91      0.94        80\n",
      "          53       0.83      0.75      0.78       102\n",
      "\n",
      "    accuracy                           0.82      5296\n",
      "   macro avg       0.83      0.83      0.82      5296\n",
      "weighted avg       0.82      0.82      0.82      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 34,  accuracy score is 0.8209969788519638\n",
      "at random state 34, confusion matrix is [[ 83   0   0 ...   0   0   0]\n",
      " [  0  71   0 ...   0   0   0]\n",
      " [  0   0 102 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  43   0   0]\n",
      " [  0   0   0 ...   0  74   0]\n",
      " [  0   0   3 ...   0   0  76]]\n",
      "at random state 34, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.87      0.86        95\n",
      "           1       0.65      0.72      0.68        99\n",
      "           2       0.86      0.93      0.89       110\n",
      "           3       0.82      0.80      0.81       101\n",
      "           4       0.86      0.90      0.88       112\n",
      "           5       0.77      0.64      0.70        95\n",
      "           6       0.93      0.98      0.95        83\n",
      "           7       0.76      0.85      0.80        87\n",
      "           8       0.96      0.92      0.94       101\n",
      "           9       0.73      0.76      0.75       106\n",
      "          10       0.64      0.67      0.65       100\n",
      "          11       0.81      0.93      0.87       108\n",
      "          12       0.96      0.93      0.94        99\n",
      "          13       0.67      0.74      0.70       106\n",
      "          14       0.84      0.86      0.85        98\n",
      "          15       0.96      0.93      0.95        85\n",
      "          16       0.87      0.88      0.87        98\n",
      "          17       0.92      0.80      0.85       109\n",
      "          18       0.82      0.84      0.83       100\n",
      "          19       0.73      0.81      0.77        97\n",
      "          20       0.88      0.83      0.85       111\n",
      "          21       0.67      0.72      0.70       105\n",
      "          22       0.95      0.88      0.92       104\n",
      "          23       0.86      0.79      0.82        97\n",
      "          24       0.67      0.71      0.69        85\n",
      "          25       0.90      0.97      0.93       101\n",
      "          26       0.69      0.75      0.72       101\n",
      "          27       0.86      0.82      0.84       101\n",
      "          28       0.95      0.90      0.92       122\n",
      "          29       0.95      0.95      0.95       102\n",
      "          30       0.92      0.89      0.90        98\n",
      "          31       0.62      0.75      0.68        97\n",
      "          32       0.89      0.82      0.85       119\n",
      "          33       0.82      0.75      0.78       106\n",
      "          34       0.93      0.87      0.90       115\n",
      "          35       0.91      0.94      0.93       100\n",
      "          36       0.73      0.69      0.71        97\n",
      "          37       0.69      0.79      0.74        86\n",
      "          38       0.82      0.71      0.76       113\n",
      "          39       0.80      0.84      0.82        97\n",
      "          40       0.70      0.85      0.77        80\n",
      "          41       0.77      0.66      0.71       108\n",
      "          42       0.85      0.85      0.85        96\n",
      "          43       0.87      0.71      0.78        94\n",
      "          44       0.83      0.91      0.87        99\n",
      "          45       0.91      0.91      0.91        93\n",
      "          46       0.90      0.98      0.94        91\n",
      "          47       0.80      0.80      0.80        98\n",
      "          48       0.77      0.65      0.71        89\n",
      "          49       0.74      0.90      0.81        78\n",
      "          50       0.61      0.48      0.54        93\n",
      "          51       1.00      1.00      1.00        43\n",
      "          52       0.99      0.96      0.97        77\n",
      "          53       0.88      0.68      0.77       111\n",
      "\n",
      "    accuracy                           0.82      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.82      0.82      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 35,  accuracy score is 0.8106117824773413\n",
      "at random state 35, confusion matrix is [[ 84   0   0 ...   0   0   0]\n",
      " [  0  69   0 ...   0   0   1]\n",
      " [  0   0 111 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  43   0   0]\n",
      " [  0   0   0 ...   0  72   0]\n",
      " [  0   3   0 ...   0   0  70]]\n",
      "at random state 35, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.89      0.81        94\n",
      "           1       0.57      0.73      0.64        94\n",
      "           2       0.90      0.95      0.92       117\n",
      "           3       0.81      0.83      0.82        95\n",
      "           4       0.87      0.88      0.87        99\n",
      "           5       0.63      0.65      0.64        75\n",
      "           6       0.97      0.95      0.96       100\n",
      "           7       0.78      0.82      0.80        97\n",
      "           8       0.96      0.91      0.93        96\n",
      "           9       0.72      0.80      0.76       110\n",
      "          10       0.62      0.69      0.65       112\n",
      "          11       0.79      0.88      0.83       109\n",
      "          12       0.92      0.90      0.91       100\n",
      "          13       0.71      0.74      0.72       118\n",
      "          14       0.93      0.89      0.91        97\n",
      "          15       0.97      0.94      0.96        81\n",
      "          16       0.86      0.88      0.87       113\n",
      "          17       0.90      0.86      0.88       104\n",
      "          18       0.90      0.81      0.85       122\n",
      "          19       0.69      0.81      0.74        91\n",
      "          20       0.90      0.80      0.85        90\n",
      "          21       0.68      0.78      0.73       100\n",
      "          22       0.97      0.95      0.96        87\n",
      "          23       0.80      0.80      0.80       103\n",
      "          24       0.58      0.53      0.56       101\n",
      "          25       0.91      0.96      0.94       111\n",
      "          26       0.71      0.71      0.71       101\n",
      "          27       0.87      0.75      0.81       109\n",
      "          28       0.96      0.93      0.95       102\n",
      "          29       0.92      0.93      0.92       112\n",
      "          30       0.93      0.85      0.89       105\n",
      "          31       0.55      0.69      0.61       104\n",
      "          32       0.89      0.88      0.89       103\n",
      "          33       0.68      0.74      0.71        99\n",
      "          34       0.89      0.81      0.85        93\n",
      "          35       0.92      0.96      0.94       113\n",
      "          36       0.69      0.67      0.68        88\n",
      "          37       0.78      0.78      0.78       103\n",
      "          38       0.77      0.75      0.76        92\n",
      "          39       0.85      0.77      0.81       100\n",
      "          40       0.79      0.83      0.81        93\n",
      "          41       0.76      0.67      0.71       106\n",
      "          42       0.89      0.89      0.89        87\n",
      "          43       0.86      0.73      0.79        94\n",
      "          44       0.78      0.87      0.82        90\n",
      "          45       0.96      0.91      0.94        79\n",
      "          46       0.93      0.96      0.94        98\n",
      "          47       0.79      0.70      0.74        91\n",
      "          48       0.88      0.65      0.75       103\n",
      "          49       0.77      0.73      0.75        90\n",
      "          50       0.52      0.46      0.49       104\n",
      "          51       1.00      1.00      1.00        43\n",
      "          52       0.96      0.94      0.95        77\n",
      "          53       0.85      0.69      0.77       101\n",
      "\n",
      "    accuracy                           0.81      5296\n",
      "   macro avg       0.82      0.81      0.81      5296\n",
      "weighted avg       0.82      0.81      0.81      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 36,  accuracy score is 0.8119335347432024\n",
      "at random state 36, confusion matrix is [[ 93   0   0 ...   0   0   0]\n",
      " [  0  64   0 ...   0   0   1]\n",
      " [  0   0 105 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  55   0   0]\n",
      " [  0   0   0 ...   0  66   0]\n",
      " [  0   1   0 ...   0   0  79]]\n",
      "at random state 36, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.86      0.81       108\n",
      "           1       0.63      0.69      0.66        93\n",
      "           2       0.83      0.95      0.89       110\n",
      "           3       0.84      0.81      0.83        96\n",
      "           4       0.85      0.93      0.89        98\n",
      "           5       0.79      0.65      0.72       112\n",
      "           6       0.99      0.89      0.93        88\n",
      "           7       0.76      0.79      0.77       106\n",
      "           8       0.94      0.89      0.91        99\n",
      "           9       0.76      0.74      0.75       102\n",
      "          10       0.67      0.66      0.67       116\n",
      "          11       0.83      0.91      0.87       114\n",
      "          12       0.96      0.86      0.90       105\n",
      "          13       0.66      0.69      0.68       106\n",
      "          14       0.94      0.92      0.93       108\n",
      "          15       0.97      0.91      0.94        78\n",
      "          16       0.94      0.89      0.92       104\n",
      "          17       0.91      0.81      0.86        97\n",
      "          18       0.89      0.79      0.84       101\n",
      "          19       0.77      0.87      0.82        99\n",
      "          20       0.89      0.88      0.89        94\n",
      "          21       0.70      0.76      0.73        97\n",
      "          22       0.88      0.97      0.92        96\n",
      "          23       0.81      0.79      0.80       106\n",
      "          24       0.52      0.62      0.57        92\n",
      "          25       0.89      0.97      0.93       115\n",
      "          26       0.77      0.85      0.81        99\n",
      "          27       0.88      0.80      0.84       113\n",
      "          28       0.98      0.91      0.94       107\n",
      "          29       0.90      0.96      0.92        90\n",
      "          30       0.95      0.87      0.91       107\n",
      "          31       0.47      0.62      0.54       100\n",
      "          32       0.88      0.82      0.84       103\n",
      "          33       0.66      0.80      0.72        80\n",
      "          34       0.85      0.88      0.86        97\n",
      "          35       0.88      0.93      0.91       106\n",
      "          36       0.65      0.71      0.68        78\n",
      "          37       0.66      0.83      0.74        93\n",
      "          38       0.84      0.78      0.81       106\n",
      "          39       0.78      0.84      0.81        80\n",
      "          40       0.80      0.82      0.81       102\n",
      "          41       0.76      0.63      0.69       106\n",
      "          42       0.90      0.83      0.86        93\n",
      "          43       0.88      0.76      0.82       106\n",
      "          44       0.85      0.81      0.83       108\n",
      "          45       0.92      0.94      0.93        78\n",
      "          46       0.92      0.87      0.89        82\n",
      "          47       0.85      0.76      0.80        88\n",
      "          48       0.85      0.62      0.72       100\n",
      "          49       0.74      0.88      0.80        96\n",
      "          50       0.47      0.34      0.39       110\n",
      "          51       1.00      1.00      1.00        55\n",
      "          52       1.00      0.92      0.96        72\n",
      "          53       0.81      0.78      0.80       101\n",
      "\n",
      "    accuracy                           0.81      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.81      0.81      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 37,  accuracy score is 0.8225075528700906\n",
      "at random state 37, confusion matrix is [[103   0   0 ...   0   0   0]\n",
      " [  0  67   0 ...   0   0   0]\n",
      " [  0   0  89 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  50   0   0]\n",
      " [  0   0   0 ...   0  74   0]\n",
      " [  0   1   1 ...   0   0  62]]\n",
      "at random state 37, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.92      0.87       112\n",
      "           1       0.55      0.72      0.63        93\n",
      "           2       0.91      0.92      0.91        97\n",
      "           3       0.86      0.84      0.85       103\n",
      "           4       0.87      0.89      0.88       101\n",
      "           5       0.74      0.77      0.76       102\n",
      "           6       0.97      0.97      0.97        78\n",
      "           7       0.77      0.89      0.82       105\n",
      "           8       0.91      0.95      0.93       101\n",
      "           9       0.67      0.81      0.73        94\n",
      "          10       0.68      0.84      0.75        85\n",
      "          11       0.81      0.96      0.88        97\n",
      "          12       0.94      0.91      0.93        90\n",
      "          13       0.71      0.70      0.70        89\n",
      "          14       0.91      0.85      0.88        93\n",
      "          15       0.99      0.95      0.97        77\n",
      "          16       0.88      0.89      0.88       115\n",
      "          17       0.89      0.85      0.87        80\n",
      "          18       0.90      0.90      0.90       111\n",
      "          19       0.84      0.83      0.83       106\n",
      "          20       0.90      0.80      0.85        91\n",
      "          21       0.62      0.75      0.68       102\n",
      "          22       0.95      0.97      0.96        99\n",
      "          23       0.83      0.85      0.84       113\n",
      "          24       0.62      0.65      0.63        99\n",
      "          25       0.86      0.97      0.92       111\n",
      "          26       0.74      0.72      0.73       101\n",
      "          27       0.87      0.81      0.84        95\n",
      "          28       0.96      0.91      0.94       103\n",
      "          29       0.96      0.92      0.94       102\n",
      "          30       0.95      0.85      0.90       109\n",
      "          31       0.58      0.62      0.60       101\n",
      "          32       0.88      0.82      0.85       103\n",
      "          33       0.74      0.74      0.74        96\n",
      "          34       0.83      0.88      0.86       101\n",
      "          35       0.91      0.91      0.91       102\n",
      "          36       0.74      0.73      0.74       111\n",
      "          37       0.76      0.81      0.78       101\n",
      "          38       0.74      0.70      0.72        93\n",
      "          39       0.81      0.87      0.84        95\n",
      "          40       0.81      0.81      0.81       108\n",
      "          41       0.77      0.62      0.69       116\n",
      "          42       0.88      0.86      0.87        98\n",
      "          43       0.86      0.73      0.79       102\n",
      "          44       0.74      0.71      0.72       100\n",
      "          45       0.98      0.97      0.97        86\n",
      "          46       0.93      0.94      0.93        97\n",
      "          47       0.89      0.81      0.85       116\n",
      "          48       0.89      0.68      0.77       105\n",
      "          49       0.82      0.78      0.80        86\n",
      "          50       0.61      0.45      0.52       102\n",
      "          51       1.00      1.00      1.00        50\n",
      "          52       0.99      0.97      0.98        76\n",
      "          53       0.78      0.64      0.70        97\n",
      "\n",
      "    accuracy                           0.82      5296\n",
      "   macro avg       0.83      0.83      0.83      5296\n",
      "weighted avg       0.83      0.82      0.82      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 38,  accuracy score is 0.8066465256797583\n",
      "at random state 38, confusion matrix is [[84  0  0 ...  0  0  0]\n",
      " [ 0 65  0 ...  0  0  1]\n",
      " [ 0  0 96 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 53  0  0]\n",
      " [ 0  0  0 ...  0 81  0]\n",
      " [ 0  0  1 ...  0  0 76]]\n",
      "at random state 38, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.90      0.83        93\n",
      "           1       0.64      0.69      0.66        94\n",
      "           2       0.86      0.94      0.90       102\n",
      "           3       0.75      0.75      0.75       106\n",
      "           4       0.86      0.88      0.87        94\n",
      "           5       0.72      0.70      0.71        97\n",
      "           6       0.90      0.96      0.93        78\n",
      "           7       0.76      0.83      0.80        96\n",
      "           8       0.92      0.91      0.92       107\n",
      "           9       0.68      0.79      0.73        82\n",
      "          10       0.62      0.70      0.66        90\n",
      "          11       0.85      0.90      0.87       111\n",
      "          12       0.93      0.93      0.93       107\n",
      "          13       0.73      0.74      0.73       104\n",
      "          14       0.83      0.89      0.86        85\n",
      "          15       0.94      0.95      0.94        76\n",
      "          16       0.91      0.89      0.90       108\n",
      "          17       0.89      0.78      0.83       100\n",
      "          18       0.79      0.86      0.82        98\n",
      "          19       0.78      0.86      0.82        97\n",
      "          20       0.92      0.84      0.87        91\n",
      "          21       0.66      0.71      0.68       110\n",
      "          22       0.94      0.92      0.93        95\n",
      "          23       0.80      0.77      0.79       111\n",
      "          24       0.55      0.52      0.54       107\n",
      "          25       0.89      0.96      0.92        96\n",
      "          26       0.77      0.75      0.76       102\n",
      "          27       0.79      0.89      0.83        88\n",
      "          28       0.98      0.95      0.96       101\n",
      "          29       0.93      0.93      0.93        96\n",
      "          30       0.97      0.89      0.92        96\n",
      "          31       0.57      0.61      0.59       106\n",
      "          32       0.86      0.82      0.84       112\n",
      "          33       0.74      0.81      0.77       110\n",
      "          34       0.88      0.88      0.88       102\n",
      "          35       0.89      0.97      0.93       100\n",
      "          36       0.68      0.70      0.69        93\n",
      "          37       0.70      0.75      0.73        83\n",
      "          38       0.77      0.80      0.78       102\n",
      "          39       0.85      0.79      0.82       117\n",
      "          40       0.74      0.82      0.78        94\n",
      "          41       0.74      0.69      0.72        98\n",
      "          42       0.89      0.83      0.86        96\n",
      "          43       0.90      0.65      0.76       114\n",
      "          44       0.84      0.78      0.81       120\n",
      "          45       0.93      0.91      0.92        77\n",
      "          46       0.95      0.89      0.92       103\n",
      "          47       0.76      0.68      0.72       113\n",
      "          48       0.83      0.63      0.72        94\n",
      "          49       0.76      0.78      0.77        95\n",
      "          50       0.50      0.40      0.45       109\n",
      "          51       1.00      1.00      1.00        53\n",
      "          52       0.99      0.92      0.95        88\n",
      "          53       0.79      0.77      0.78        99\n",
      "\n",
      "    accuracy                           0.81      5296\n",
      "   macro avg       0.81      0.81      0.81      5296\n",
      "weighted avg       0.81      0.81      0.81      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 39,  accuracy score is 0.8030589123867069\n",
      "at random state 39, confusion matrix is [[81  0  0 ...  0  0  0]\n",
      " [ 0 74  0 ...  0  0  1]\n",
      " [ 0  0 96 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 50  0  0]\n",
      " [ 0  0  0 ...  1 68  0]\n",
      " [ 0  0  2 ...  0  0 64]]\n",
      "at random state 39, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.87      0.82        93\n",
      "           1       0.66      0.76      0.70        98\n",
      "           2       0.87      0.91      0.89       105\n",
      "           3       0.80      0.75      0.77       100\n",
      "           4       0.86      0.90      0.88        96\n",
      "           5       0.71      0.76      0.74        99\n",
      "           6       0.93      0.94      0.94        85\n",
      "           7       0.73      0.84      0.78       108\n",
      "           8       0.91      0.94      0.93       101\n",
      "           9       0.71      0.76      0.74       105\n",
      "          10       0.56      0.65      0.60        89\n",
      "          11       0.78      0.92      0.85        89\n",
      "          12       0.92      0.81      0.86       109\n",
      "          13       0.64      0.72      0.68        92\n",
      "          14       0.83      0.90      0.86        94\n",
      "          15       0.89      0.93      0.91        88\n",
      "          16       0.91      0.87      0.89       118\n",
      "          17       0.89      0.77      0.83       109\n",
      "          18       0.88      0.80      0.84       111\n",
      "          19       0.81      0.77      0.79       115\n",
      "          20       0.92      0.85      0.88        97\n",
      "          21       0.65      0.72      0.68       103\n",
      "          22       0.96      0.91      0.93       106\n",
      "          23       0.78      0.78      0.78       107\n",
      "          24       0.49      0.58      0.53        88\n",
      "          25       0.88      0.95      0.91        95\n",
      "          26       0.74      0.68      0.71        98\n",
      "          27       0.79      0.80      0.79        96\n",
      "          28       0.98      0.98      0.98       100\n",
      "          29       0.93      0.92      0.92        84\n",
      "          30       0.92      0.87      0.89       113\n",
      "          31       0.61      0.65      0.63       111\n",
      "          32       0.87      0.75      0.81       106\n",
      "          33       0.68      0.86      0.76        86\n",
      "          34       0.85      0.94      0.89        99\n",
      "          35       0.90      0.94      0.92       110\n",
      "          36       0.69      0.70      0.70        91\n",
      "          37       0.69      0.78      0.73        87\n",
      "          38       0.76      0.70      0.73        91\n",
      "          39       0.76      0.74      0.75       113\n",
      "          40       0.81      0.85      0.83       107\n",
      "          41       0.74      0.66      0.70        97\n",
      "          42       0.88      0.85      0.86        93\n",
      "          43       0.88      0.77      0.82        90\n",
      "          44       0.77      0.77      0.77        95\n",
      "          45       0.92      0.90      0.91        92\n",
      "          46       0.89      0.90      0.90        93\n",
      "          47       0.80      0.84      0.82        86\n",
      "          48       0.82      0.60      0.69       111\n",
      "          49       0.82      0.75      0.79       110\n",
      "          50       0.63      0.43      0.51       120\n",
      "          51       0.98      1.00      0.99        50\n",
      "          52       0.96      0.91      0.93        75\n",
      "          53       0.85      0.70      0.77        92\n",
      "\n",
      "    accuracy                           0.80      5296\n",
      "   macro avg       0.81      0.81      0.81      5296\n",
      "weighted avg       0.81      0.80      0.80      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 40,  accuracy score is 0.8189199395770392\n",
      "at random state 40, confusion matrix is [[93  0  0 ...  0  0  0]\n",
      " [ 0 67  0 ...  0  0  2]\n",
      " [ 0  0 91 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 46  0  0]\n",
      " [ 0  0  0 ...  0 90  0]\n",
      " [ 0  0  0 ...  0  0 84]]\n",
      "at random state 40, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.93      0.83       100\n",
      "           1       0.58      0.67      0.62       100\n",
      "           2       0.81      0.92      0.86        99\n",
      "           3       0.84      0.77      0.80        91\n",
      "           4       0.86      0.90      0.88       111\n",
      "           5       0.79      0.73      0.76       105\n",
      "           6       0.91      0.92      0.92        79\n",
      "           7       0.70      0.85      0.77        79\n",
      "           8       0.95      0.92      0.94       105\n",
      "           9       0.70      0.77      0.73        91\n",
      "          10       0.70      0.71      0.71        97\n",
      "          11       0.79      0.91      0.85       102\n",
      "          12       0.94      0.92      0.93       104\n",
      "          13       0.72      0.72      0.72       103\n",
      "          14       0.91      0.94      0.92        83\n",
      "          15       0.92      0.95      0.93        91\n",
      "          16       0.93      0.89      0.91       100\n",
      "          17       0.92      0.80      0.86       100\n",
      "          18       0.85      0.84      0.84        98\n",
      "          19       0.81      0.84      0.82       103\n",
      "          20       0.93      0.90      0.91        89\n",
      "          21       0.65      0.78      0.71       102\n",
      "          22       0.93      0.98      0.95       108\n",
      "          23       0.76      0.83      0.79        90\n",
      "          24       0.61      0.58      0.59        95\n",
      "          25       0.91      0.93      0.92        90\n",
      "          26       0.70      0.79      0.74       109\n",
      "          27       0.87      0.81      0.84        98\n",
      "          28       0.98      0.82      0.89       100\n",
      "          29       0.95      0.95      0.95        86\n",
      "          30       0.91      0.83      0.87        96\n",
      "          31       0.56      0.56      0.56        98\n",
      "          32       0.82      0.86      0.84        96\n",
      "          33       0.75      0.79      0.77       109\n",
      "          34       0.94      0.90      0.92       113\n",
      "          35       0.91      0.99      0.95       108\n",
      "          36       0.77      0.62      0.69       105\n",
      "          37       0.73      0.76      0.74       109\n",
      "          38       0.89      0.76      0.82       123\n",
      "          39       0.82      0.78      0.80       108\n",
      "          40       0.79      0.79      0.79       112\n",
      "          41       0.73      0.64      0.68       113\n",
      "          42       0.88      0.86      0.87        96\n",
      "          43       0.80      0.80      0.80        87\n",
      "          44       0.84      0.76      0.79       107\n",
      "          45       0.98      0.93      0.95        89\n",
      "          46       0.94      0.94      0.94       107\n",
      "          47       0.79      0.77      0.78        87\n",
      "          48       0.84      0.69      0.76        78\n",
      "          49       0.87      0.80      0.84       111\n",
      "          50       0.47      0.51      0.49        84\n",
      "          51       1.00      1.00      1.00        46\n",
      "          52       0.99      0.94      0.96        96\n",
      "          53       0.82      0.76      0.79       110\n",
      "\n",
      "    accuracy                           0.82      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.82      0.82      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 41,  accuracy score is 0.8058912386706949\n",
      "at random state 41, confusion matrix is [[96  0  0 ...  0  0  0]\n",
      " [ 0 77  0 ...  0  0  1]\n",
      " [ 0  0 88 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 51  0  0]\n",
      " [ 0  0  0 ...  1 75  0]\n",
      " [ 0  0  3 ...  0  0 70]]\n",
      "at random state 41, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.91      0.83       106\n",
      "           1       0.70      0.69      0.70       111\n",
      "           2       0.85      0.95      0.89        93\n",
      "           3       0.82      0.82      0.82       101\n",
      "           4       0.84      0.89      0.86       100\n",
      "           5       0.66      0.74      0.70       105\n",
      "           6       0.90      0.91      0.91        89\n",
      "           7       0.80      0.80      0.80        99\n",
      "           8       0.97      0.92      0.95       114\n",
      "           9       0.59      0.70      0.64        89\n",
      "          10       0.64      0.74      0.69        90\n",
      "          11       0.82      0.88      0.85       121\n",
      "          12       0.92      0.83      0.88       103\n",
      "          13       0.61      0.68      0.64        93\n",
      "          14       0.86      0.89      0.88        93\n",
      "          15       0.94      0.82      0.88        77\n",
      "          16       0.90      0.84      0.87       113\n",
      "          17       0.93      0.90      0.91        99\n",
      "          18       0.84      0.85      0.84       104\n",
      "          19       0.77      0.75      0.76        96\n",
      "          20       0.94      0.87      0.90        89\n",
      "          21       0.65      0.69      0.67       102\n",
      "          22       0.90      0.93      0.92       101\n",
      "          23       0.82      0.80      0.81       123\n",
      "          24       0.55      0.59      0.57        98\n",
      "          25       0.80      0.98      0.88        88\n",
      "          26       0.73      0.82      0.78        97\n",
      "          27       0.79      0.78      0.79       104\n",
      "          28       0.97      0.94      0.95        96\n",
      "          29       0.96      0.91      0.93        94\n",
      "          30       0.92      0.83      0.87        95\n",
      "          31       0.62      0.68      0.65       105\n",
      "          32       0.91      0.84      0.87       114\n",
      "          33       0.71      0.81      0.75       104\n",
      "          34       0.91      0.88      0.89       101\n",
      "          35       0.88      0.89      0.88       102\n",
      "          36       0.75      0.72      0.74        83\n",
      "          37       0.72      0.74      0.73        89\n",
      "          38       0.79      0.84      0.82        95\n",
      "          39       0.80      0.77      0.79       100\n",
      "          40       0.81      0.82      0.82       116\n",
      "          41       0.73      0.66      0.69        93\n",
      "          42       0.88      0.80      0.84       100\n",
      "          43       0.82      0.76      0.79        92\n",
      "          44       0.83      0.81      0.82       101\n",
      "          45       0.95      0.93      0.94        84\n",
      "          46       0.89      0.90      0.90        91\n",
      "          47       0.78      0.73      0.75       102\n",
      "          48       0.76      0.63      0.69        92\n",
      "          49       0.83      0.79      0.81       111\n",
      "          50       0.49      0.36      0.42       110\n",
      "          51       0.98      1.00      0.99        51\n",
      "          52       0.96      0.94      0.95        80\n",
      "          53       0.85      0.72      0.78        97\n",
      "\n",
      "    accuracy                           0.81      5296\n",
      "   macro avg       0.81      0.81      0.81      5296\n",
      "weighted avg       0.81      0.81      0.81      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 42,  accuracy score is 0.8149546827794562\n",
      "at random state 42, confusion matrix is [[93  0  0 ...  0  0  0]\n",
      " [ 0 75  0 ...  0  0  1]\n",
      " [ 0  0 95 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 50  0  0]\n",
      " [ 0  0  0 ...  0 57  0]\n",
      " [ 0  0  0 ...  0  0 68]]\n",
      "at random state 42, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.90      0.86       103\n",
      "           1       0.59      0.76      0.66        99\n",
      "           2       0.87      0.95      0.91       100\n",
      "           3       0.82      0.75      0.79       100\n",
      "           4       0.89      0.87      0.88       103\n",
      "           5       0.75      0.75      0.75       105\n",
      "           6       0.94      0.93      0.94        89\n",
      "           7       0.74      0.81      0.77       113\n",
      "           8       0.90      0.96      0.93        94\n",
      "           9       0.78      0.73      0.76       113\n",
      "          10       0.61      0.73      0.67        92\n",
      "          11       0.84      0.95      0.89       113\n",
      "          12       0.91      0.91      0.91       104\n",
      "          13       0.67      0.77      0.72        92\n",
      "          14       0.90      0.89      0.90       111\n",
      "          15       0.93      0.95      0.94        84\n",
      "          16       0.91      0.91      0.91       106\n",
      "          17       0.95      0.85      0.90        99\n",
      "          18       0.88      0.81      0.84       114\n",
      "          19       0.75      0.76      0.75        94\n",
      "          20       0.90      0.80      0.85       107\n",
      "          21       0.62      0.67      0.65        98\n",
      "          22       0.97      0.94      0.96       104\n",
      "          23       0.77      0.84      0.80        93\n",
      "          24       0.60      0.58      0.59        95\n",
      "          25       0.92      0.96      0.94       114\n",
      "          26       0.58      0.73      0.65        97\n",
      "          27       0.91      0.79      0.85       104\n",
      "          28       0.98      0.92      0.95        88\n",
      "          29       0.97      0.96      0.96        94\n",
      "          30       0.87      0.88      0.87       107\n",
      "          31       0.54      0.59      0.57       103\n",
      "          32       0.91      0.91      0.91        89\n",
      "          33       0.73      0.83      0.77       110\n",
      "          34       0.92      0.89      0.91        93\n",
      "          35       0.90      0.95      0.92        92\n",
      "          36       0.62      0.77      0.69        77\n",
      "          37       0.72      0.80      0.76        87\n",
      "          38       0.89      0.76      0.82       108\n",
      "          39       0.73      0.79      0.76        94\n",
      "          40       0.80      0.81      0.81       100\n",
      "          41       0.80      0.63      0.70       113\n",
      "          42       0.92      0.79      0.85       100\n",
      "          43       0.91      0.70      0.79        98\n",
      "          44       0.89      0.75      0.81       114\n",
      "          45       0.99      0.84      0.91        92\n",
      "          46       0.85      0.95      0.90        86\n",
      "          47       0.85      0.75      0.80       118\n",
      "          48       0.82      0.72      0.77        90\n",
      "          49       0.79      0.76      0.78       101\n",
      "          50       0.56      0.50      0.53       100\n",
      "          51       1.00      1.00      1.00        50\n",
      "          52       0.93      0.95      0.94        60\n",
      "          53       0.86      0.74      0.80        92\n",
      "\n",
      "    accuracy                           0.81      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.81      0.82      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 43,  accuracy score is 0.8074018126888217\n",
      "at random state 43, confusion matrix is [[92  0  0 ...  0  0  0]\n",
      " [ 0 64  0 ...  0  0  2]\n",
      " [ 0  0 97 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 51  0  0]\n",
      " [ 0  0  0 ...  0 71  0]\n",
      " [ 0  0  1 ...  0  0 82]]\n",
      "at random state 43, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.88      0.83       104\n",
      "           1       0.59      0.67      0.63        95\n",
      "           2       0.89      0.92      0.91       105\n",
      "           3       0.83      0.77      0.80        90\n",
      "           4       0.89      0.85      0.87       100\n",
      "           5       0.77      0.78      0.78        97\n",
      "           6       0.96      0.93      0.94        82\n",
      "           7       0.69      0.80      0.74       101\n",
      "           8       0.92      0.95      0.93        93\n",
      "           9       0.83      0.75      0.79       114\n",
      "          10       0.61      0.78      0.68        80\n",
      "          11       0.82      0.91      0.86       109\n",
      "          12       0.91      0.89      0.90       100\n",
      "          13       0.62      0.78      0.69        96\n",
      "          14       0.87      0.91      0.89        85\n",
      "          15       0.95      0.92      0.94        78\n",
      "          16       0.87      0.92      0.89       101\n",
      "          17       0.83      0.83      0.83       102\n",
      "          18       0.80      0.82      0.81       101\n",
      "          19       0.81      0.82      0.81       109\n",
      "          20       0.85      0.88      0.86        96\n",
      "          21       0.68      0.72      0.70       118\n",
      "          22       0.95      0.97      0.96        93\n",
      "          23       0.79      0.82      0.80       110\n",
      "          24       0.51      0.58      0.54        97\n",
      "          25       0.87      0.96      0.91        89\n",
      "          26       0.68      0.78      0.73        87\n",
      "          27       0.88      0.80      0.84       105\n",
      "          28       0.98      0.91      0.94        99\n",
      "          29       0.97      0.93      0.95       100\n",
      "          30       0.95      0.83      0.89       109\n",
      "          31       0.60      0.56      0.58       112\n",
      "          32       0.89      0.79      0.84       107\n",
      "          33       0.70      0.76      0.73       107\n",
      "          34       0.97      0.85      0.90       110\n",
      "          35       0.93      0.95      0.94       100\n",
      "          36       0.82      0.67      0.74        97\n",
      "          37       0.64      0.75      0.69        99\n",
      "          38       0.78      0.73      0.75        99\n",
      "          39       0.80      0.77      0.79       109\n",
      "          40       0.75      0.83      0.79        98\n",
      "          41       0.74      0.64      0.69       109\n",
      "          42       0.92      0.82      0.86        93\n",
      "          43       0.85      0.81      0.83        90\n",
      "          44       0.84      0.81      0.83       108\n",
      "          45       0.94      0.95      0.94        81\n",
      "          46       0.90      0.94      0.92        88\n",
      "          47       0.74      0.73      0.74        90\n",
      "          48       0.79      0.59      0.67        94\n",
      "          49       0.83      0.83      0.83       110\n",
      "          50       0.49      0.44      0.47       106\n",
      "          51       1.00      1.00      1.00        51\n",
      "          52       0.96      0.91      0.93        78\n",
      "          53       0.84      0.71      0.77       115\n",
      "\n",
      "    accuracy                           0.81      5296\n",
      "   macro avg       0.82      0.81      0.81      5296\n",
      "weighted avg       0.81      0.81      0.81      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 44,  accuracy score is 0.8115558912386707\n",
      "at random state 44, confusion matrix is [[92  0  0 ...  0  0  0]\n",
      " [ 0 71  0 ...  0  0  3]\n",
      " [ 0  0 97 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 58  0  0]\n",
      " [ 0  0  0 ...  0 57  0]\n",
      " [ 0  1  0 ...  0  0 65]]\n",
      "at random state 44, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.88      0.84       105\n",
      "           1       0.62      0.76      0.69        93\n",
      "           2       0.82      0.96      0.89       101\n",
      "           3       0.87      0.83      0.85        96\n",
      "           4       0.86      0.90      0.88        99\n",
      "           5       0.80      0.78      0.79       108\n",
      "           6       0.96      0.94      0.95        86\n",
      "           7       0.78      0.81      0.80       106\n",
      "           8       0.94      0.86      0.90       110\n",
      "           9       0.75      0.73      0.74       110\n",
      "          10       0.62      0.67      0.65       101\n",
      "          11       0.79      0.93      0.86        87\n",
      "          12       0.90      0.88      0.89       117\n",
      "          13       0.64      0.71      0.67        94\n",
      "          14       0.92      0.91      0.91        95\n",
      "          15       0.94      0.99      0.96        76\n",
      "          16       0.90      0.94      0.92       100\n",
      "          17       0.90      0.83      0.86        99\n",
      "          18       0.89      0.84      0.87       101\n",
      "          19       0.75      0.82      0.79        94\n",
      "          20       0.87      0.85      0.86       105\n",
      "          21       0.65      0.68      0.66       115\n",
      "          22       0.96      0.96      0.96        97\n",
      "          23       0.78      0.84      0.81        97\n",
      "          24       0.60      0.55      0.57       113\n",
      "          25       0.97      0.91      0.94       102\n",
      "          26       0.78      0.79      0.79       110\n",
      "          27       0.85      0.82      0.84       107\n",
      "          28       0.97      0.90      0.94       114\n",
      "          29       0.98      0.98      0.98        92\n",
      "          30       0.91      0.87      0.89        99\n",
      "          31       0.50      0.57      0.53       109\n",
      "          32       0.89      0.85      0.87       111\n",
      "          33       0.70      0.76      0.73       102\n",
      "          34       0.92      0.93      0.92       104\n",
      "          35       0.79      0.92      0.85        83\n",
      "          36       0.68      0.73      0.71        90\n",
      "          37       0.67      0.72      0.69        80\n",
      "          38       0.75      0.81      0.78        95\n",
      "          39       0.85      0.79      0.82        90\n",
      "          40       0.77      0.83      0.80        93\n",
      "          41       0.73      0.65      0.69        97\n",
      "          42       0.88      0.81      0.85        85\n",
      "          43       0.87      0.67      0.76       123\n",
      "          44       0.81      0.79      0.80       100\n",
      "          45       0.94      0.95      0.95        88\n",
      "          46       0.95      0.91      0.92        95\n",
      "          47       0.78      0.75      0.77       100\n",
      "          48       0.86      0.70      0.77        92\n",
      "          49       0.84      0.86      0.85       110\n",
      "          50       0.49      0.39      0.43       106\n",
      "          51       1.00      1.00      1.00        58\n",
      "          52       0.92      0.95      0.93        60\n",
      "          53       0.76      0.68      0.71        96\n",
      "\n",
      "    accuracy                           0.81      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.81      0.81      0.81      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 45,  accuracy score is 0.8100453172205438\n",
      "at random state 45, confusion matrix is [[92  0  0 ...  0  0  0]\n",
      " [ 0 76  0 ...  0  0  2]\n",
      " [ 0  0 90 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 58  0  0]\n",
      " [ 0  0  0 ...  0 69  0]\n",
      " [ 0  0  1 ...  0  0 70]]\n",
      "at random state 45, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.86      0.82       107\n",
      "           1       0.64      0.77      0.70        99\n",
      "           2       0.83      0.93      0.88        97\n",
      "           3       0.87      0.75      0.81       106\n",
      "           4       0.82      0.93      0.87        84\n",
      "           5       0.72      0.72      0.72       103\n",
      "           6       0.98      0.89      0.93        95\n",
      "           7       0.69      0.79      0.73        84\n",
      "           8       0.88      0.92      0.90        93\n",
      "           9       0.69      0.83      0.75       102\n",
      "          10       0.59      0.76      0.66        85\n",
      "          11       0.82      0.93      0.87       107\n",
      "          12       0.91      0.88      0.89        97\n",
      "          13       0.76      0.70      0.73        98\n",
      "          14       0.85      0.91      0.88        93\n",
      "          15       0.92      0.90      0.91        79\n",
      "          16       0.91      0.89      0.90        94\n",
      "          17       0.92      0.82      0.87       104\n",
      "          18       0.89      0.85      0.87       110\n",
      "          19       0.80      0.78      0.79       111\n",
      "          20       0.91      0.88      0.90       109\n",
      "          21       0.68      0.65      0.66        97\n",
      "          22       0.92      0.98      0.95        90\n",
      "          23       0.80      0.79      0.79        99\n",
      "          24       0.51      0.54      0.52        93\n",
      "          25       0.85      0.99      0.92       101\n",
      "          26       0.72      0.66      0.69       107\n",
      "          27       0.84      0.81      0.82       107\n",
      "          28       0.98      0.96      0.97       104\n",
      "          29       0.97      0.98      0.97       113\n",
      "          30       0.95      0.85      0.90       116\n",
      "          31       0.55      0.57      0.56        98\n",
      "          32       0.85      0.82      0.83       111\n",
      "          33       0.76      0.87      0.81       104\n",
      "          34       0.88      0.87      0.87       107\n",
      "          35       0.92      0.91      0.92        91\n",
      "          36       0.69      0.75      0.72        85\n",
      "          37       0.74      0.74      0.74       104\n",
      "          38       0.84      0.71      0.77       100\n",
      "          39       0.74      0.73      0.73       108\n",
      "          40       0.78      0.82      0.80       101\n",
      "          41       0.76      0.65      0.70       101\n",
      "          42       0.88      0.84      0.86        99\n",
      "          43       0.84      0.66      0.73        93\n",
      "          44       0.78      0.88      0.83        91\n",
      "          45       0.94      0.92      0.93        90\n",
      "          46       0.92      0.93      0.93        87\n",
      "          47       0.82      0.77      0.80        86\n",
      "          48       0.87      0.73      0.79       105\n",
      "          49       0.80      0.80      0.80       112\n",
      "          50       0.52      0.38      0.44       108\n",
      "          51       1.00      1.00      1.00        58\n",
      "          52       0.99      0.91      0.95        76\n",
      "          53       0.72      0.72      0.72        97\n",
      "\n",
      "    accuracy                           0.81      5296\n",
      "   macro avg       0.81      0.81      0.81      5296\n",
      "weighted avg       0.81      0.81      0.81      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 46,  accuracy score is 0.8147658610271903\n",
      "at random state 46, confusion matrix is [[94  0  0 ...  0  0  0]\n",
      " [ 0 73  0 ...  0  0  1]\n",
      " [ 0  0 99 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 36  0  0]\n",
      " [ 0  0  0 ...  0 72  0]\n",
      " [ 0  0  3 ...  0  0 87]]\n",
      "at random state 46, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.97      0.86        97\n",
      "           1       0.61      0.78      0.69        94\n",
      "           2       0.82      0.93      0.88       106\n",
      "           3       0.81      0.79      0.80       103\n",
      "           4       0.80      0.94      0.86       111\n",
      "           5       0.70      0.73      0.71       103\n",
      "           6       0.94      0.93      0.93        84\n",
      "           7       0.80      0.84      0.82       102\n",
      "           8       0.98      0.91      0.94        95\n",
      "           9       0.74      0.72      0.73        86\n",
      "          10       0.66      0.70      0.68       108\n",
      "          11       0.77      0.95      0.85        87\n",
      "          12       0.93      0.94      0.93       114\n",
      "          13       0.62      0.69      0.66        94\n",
      "          14       0.92      0.87      0.89       107\n",
      "          15       0.89      0.93      0.91        82\n",
      "          16       0.89      0.92      0.90       120\n",
      "          17       0.91      0.83      0.87        94\n",
      "          18       0.87      0.82      0.84        99\n",
      "          19       0.74      0.91      0.81        87\n",
      "          20       0.85      0.85      0.85        93\n",
      "          21       0.61      0.68      0.64        93\n",
      "          22       0.93      0.99      0.96        92\n",
      "          23       0.82      0.76      0.79       103\n",
      "          24       0.66      0.64      0.65       103\n",
      "          25       0.91      0.96      0.94       113\n",
      "          26       0.73      0.69      0.71       108\n",
      "          27       0.80      0.84      0.82        88\n",
      "          28       0.98      0.95      0.97       105\n",
      "          29       0.96      0.93      0.94        81\n",
      "          30       0.98      0.81      0.89       111\n",
      "          31       0.62      0.59      0.61       103\n",
      "          32       0.82      0.77      0.79       105\n",
      "          33       0.71      0.75      0.73       104\n",
      "          34       0.86      0.88      0.87       101\n",
      "          35       0.88      0.91      0.89       102\n",
      "          36       0.76      0.65      0.70        96\n",
      "          37       0.78      0.76      0.77       100\n",
      "          38       0.81      0.81      0.81       103\n",
      "          39       0.82      0.85      0.84        94\n",
      "          40       0.77      0.79      0.78       101\n",
      "          41       0.75      0.73      0.74        93\n",
      "          42       0.85      0.89      0.87        96\n",
      "          43       0.82      0.74      0.78        93\n",
      "          44       0.85      0.77      0.81       111\n",
      "          45       0.96      0.90      0.93        90\n",
      "          46       0.96      0.94      0.95        93\n",
      "          47       0.82      0.76      0.79        93\n",
      "          48       0.82      0.67      0.73       108\n",
      "          49       0.82      0.73      0.77       115\n",
      "          50       0.56      0.47      0.51       103\n",
      "          51       1.00      1.00      1.00        36\n",
      "          52       1.00      0.94      0.97        77\n",
      "          53       0.86      0.75      0.80       116\n",
      "\n",
      "    accuracy                           0.81      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.81      0.81      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 47,  accuracy score is 0.8138217522658611\n",
      "at random state 47, confusion matrix is [[85  0  0 ...  0  0  0]\n",
      " [ 0 69  0 ...  0  0  0]\n",
      " [ 0  0 85 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 49  0  0]\n",
      " [ 0  0  0 ...  0 61  0]\n",
      " [ 0  0  1 ...  0  0 82]]\n",
      "at random state 47, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.88      0.79        97\n",
      "           1       0.69      0.66      0.68       104\n",
      "           2       0.79      0.94      0.86        90\n",
      "           3       0.87      0.74      0.80        90\n",
      "           4       0.87      0.85      0.86       104\n",
      "           5       0.80      0.75      0.78       125\n",
      "           6       0.95      0.93      0.94        97\n",
      "           7       0.75      0.89      0.81        98\n",
      "           8       0.92      0.92      0.92       106\n",
      "           9       0.75      0.79      0.77       105\n",
      "          10       0.74      0.77      0.75       104\n",
      "          11       0.80      0.93      0.86       111\n",
      "          12       0.96      0.88      0.92        99\n",
      "          13       0.66      0.74      0.70        97\n",
      "          14       0.91      0.94      0.92       114\n",
      "          15       0.92      0.96      0.94        80\n",
      "          16       0.92      0.89      0.90       110\n",
      "          17       0.82      0.84      0.83        95\n",
      "          18       0.86      0.77      0.81       101\n",
      "          19       0.77      0.87      0.82       102\n",
      "          20       0.88      0.84      0.86        96\n",
      "          21       0.59      0.74      0.65        92\n",
      "          22       0.94      0.96      0.95       106\n",
      "          23       0.76      0.81      0.79       100\n",
      "          24       0.69      0.58      0.63       112\n",
      "          25       0.92      0.94      0.93        99\n",
      "          26       0.81      0.76      0.79        97\n",
      "          27       0.83      0.73      0.78        94\n",
      "          28       0.97      0.86      0.91        87\n",
      "          29       0.95      0.94      0.94        96\n",
      "          30       0.87      0.85      0.86       101\n",
      "          31       0.51      0.71      0.59        82\n",
      "          32       0.92      0.79      0.85        99\n",
      "          33       0.71      0.65      0.68       105\n",
      "          34       0.90      0.92      0.91        96\n",
      "          35       0.88      0.96      0.92        96\n",
      "          36       0.76      0.75      0.76       106\n",
      "          37       0.71      0.71      0.71        95\n",
      "          38       0.80      0.70      0.75       107\n",
      "          39       0.81      0.83      0.82       104\n",
      "          40       0.79      0.82      0.80       110\n",
      "          41       0.72      0.72      0.72        93\n",
      "          42       0.88      0.78      0.83        82\n",
      "          43       0.80      0.73      0.77       105\n",
      "          44       0.83      0.76      0.80       118\n",
      "          45       0.98      0.94      0.96        97\n",
      "          46       0.92      0.98      0.95        81\n",
      "          47       0.83      0.82      0.82        92\n",
      "          48       0.84      0.59      0.69       103\n",
      "          49       0.82      0.81      0.81       115\n",
      "          50       0.44      0.45      0.44        84\n",
      "          51       0.98      1.00      0.99        49\n",
      "          52       0.97      0.92      0.95        66\n",
      "          53       0.85      0.80      0.82       102\n",
      "\n",
      "    accuracy                           0.81      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.81      0.81      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 48,  accuracy score is 0.8128776435045317\n",
      "at random state 48, confusion matrix is [[98  0  0 ...  0  0  0]\n",
      " [ 0 79  0 ...  0  0  0]\n",
      " [ 0  0 92 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 56  0  0]\n",
      " [ 0  0  0 ...  0 77  0]\n",
      " [ 0  0  1 ...  0  0 71]]\n",
      "at random state 48, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.89      0.84       110\n",
      "           1       0.69      0.75      0.71       106\n",
      "           2       0.91      0.88      0.90       104\n",
      "           3       0.93      0.77      0.84        99\n",
      "           4       0.83      0.94      0.88        78\n",
      "           5       0.80      0.74      0.76       106\n",
      "           6       0.87      0.95      0.91        88\n",
      "           7       0.71      0.90      0.79        96\n",
      "           8       0.93      0.95      0.94        82\n",
      "           9       0.70      0.82      0.76        96\n",
      "          10       0.70      0.72      0.71       118\n",
      "          11       0.76      0.88      0.81        80\n",
      "          12       0.93      0.85      0.89        87\n",
      "          13       0.73      0.72      0.72       110\n",
      "          14       0.89      0.91      0.90       111\n",
      "          15       0.96      0.93      0.94        70\n",
      "          16       0.84      0.92      0.88       100\n",
      "          17       0.90      0.88      0.89        92\n",
      "          18       0.82      0.81      0.82       101\n",
      "          19       0.87      0.78      0.82       110\n",
      "          20       0.89      0.82      0.85        96\n",
      "          21       0.60      0.79      0.68        95\n",
      "          22       0.94      0.94      0.94       102\n",
      "          23       0.75      0.83      0.79        93\n",
      "          24       0.45      0.52      0.48        96\n",
      "          25       0.88      0.98      0.92        88\n",
      "          26       0.76      0.76      0.76       101\n",
      "          27       0.92      0.80      0.86       111\n",
      "          28       0.98      0.88      0.93       104\n",
      "          29       0.92      0.95      0.94       101\n",
      "          30       0.94      0.89      0.91        98\n",
      "          31       0.53      0.59      0.56       105\n",
      "          32       0.92      0.80      0.86       112\n",
      "          33       0.73      0.67      0.70       110\n",
      "          34       0.90      0.84      0.87       113\n",
      "          35       0.93      0.88      0.90       112\n",
      "          36       0.77      0.78      0.78       100\n",
      "          37       0.77      0.74      0.75       119\n",
      "          38       0.75      0.65      0.70        97\n",
      "          39       0.80      0.80      0.80        98\n",
      "          40       0.80      0.82      0.81       100\n",
      "          41       0.80      0.73      0.76       102\n",
      "          42       0.88      0.86      0.87        80\n",
      "          43       0.85      0.75      0.80       108\n",
      "          44       0.80      0.81      0.80       103\n",
      "          45       0.94      0.93      0.93        80\n",
      "          46       0.91      0.92      0.92        92\n",
      "          47       0.77      0.81      0.79       106\n",
      "          48       0.86      0.73      0.79        97\n",
      "          49       0.83      0.88      0.85       100\n",
      "          50       0.44      0.36      0.40       109\n",
      "          51       1.00      1.00      1.00        56\n",
      "          52       0.95      0.92      0.93        84\n",
      "          53       0.89      0.85      0.87        84\n",
      "\n",
      "    accuracy                           0.81      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.81      0.81      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 49,  accuracy score is 0.8217522658610272\n",
      "at random state 49, confusion matrix is [[102   0   0 ...   0   0   0]\n",
      " [  0  66   0 ...   0   0   1]\n",
      " [  0   0  82 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  57   0   0]\n",
      " [  0   0   0 ...   0  71   0]\n",
      " [  0   0   3 ...   0   0  84]]\n",
      "at random state 49, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.91      0.86       112\n",
      "           1       0.60      0.70      0.65        94\n",
      "           2       0.80      0.94      0.87        87\n",
      "           3       0.81      0.82      0.81        94\n",
      "           4       0.84      0.87      0.85        89\n",
      "           5       0.76      0.74      0.75       111\n",
      "           6       0.92      0.97      0.94        91\n",
      "           7       0.74      0.80      0.77        85\n",
      "           8       0.94      0.91      0.92       119\n",
      "           9       0.70      0.89      0.79        97\n",
      "          10       0.68      0.69      0.69       100\n",
      "          11       0.88      0.93      0.90       114\n",
      "          12       0.89      0.90      0.90       104\n",
      "          13       0.72      0.74      0.73        92\n",
      "          14       0.85      0.91      0.88        82\n",
      "          15       0.94      0.87      0.90        85\n",
      "          16       0.92      0.89      0.90       101\n",
      "          17       0.92      0.88      0.90        98\n",
      "          18       0.84      0.86      0.85        99\n",
      "          19       0.76      0.84      0.80        88\n",
      "          20       0.89      0.83      0.86       103\n",
      "          21       0.65      0.73      0.69       103\n",
      "          22       0.94      0.92      0.93        92\n",
      "          23       0.82      0.80      0.81        97\n",
      "          24       0.56      0.63      0.60       104\n",
      "          25       0.85      0.95      0.90       100\n",
      "          26       0.72      0.80      0.76        84\n",
      "          27       0.91      0.82      0.86       103\n",
      "          28       1.00      0.95      0.97       102\n",
      "          29       0.92      0.95      0.94       106\n",
      "          30       0.90      0.90      0.90       103\n",
      "          31       0.51      0.59      0.55        98\n",
      "          32       0.88      0.83      0.86        90\n",
      "          33       0.78      0.77      0.77       113\n",
      "          34       0.94      0.86      0.90       106\n",
      "          35       0.96      0.89      0.92       103\n",
      "          36       0.86      0.79      0.82       108\n",
      "          37       0.75      0.72      0.74       108\n",
      "          38       0.77      0.80      0.79       106\n",
      "          39       0.84      0.80      0.82       103\n",
      "          40       0.77      0.78      0.78       106\n",
      "          41       0.86      0.75      0.80       110\n",
      "          42       0.87      0.87      0.87        87\n",
      "          43       0.85      0.79      0.82        80\n",
      "          44       0.79      0.83      0.81        84\n",
      "          45       0.98      0.93      0.95        89\n",
      "          46       0.90      0.96      0.92        90\n",
      "          47       0.79      0.78      0.78        95\n",
      "          48       0.84      0.62      0.72       104\n",
      "          49       0.81      0.83      0.82       110\n",
      "          50       0.60      0.41      0.49       123\n",
      "          51       1.00      1.00      1.00        57\n",
      "          52       0.97      0.89      0.93        80\n",
      "          53       0.88      0.79      0.83       107\n",
      "\n",
      "    accuracy                           0.82      5296\n",
      "   macro avg       0.83      0.83      0.83      5296\n",
      "weighted avg       0.83      0.82      0.82      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 50,  accuracy score is 0.8219410876132931\n",
      "at random state 50, confusion matrix is [[88  0  0 ...  0  0  0]\n",
      " [ 0 73  0 ...  0  0  0]\n",
      " [ 0  0 86 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 61  0  0]\n",
      " [ 0  0  0 ...  0 66  0]\n",
      " [ 0  0  0 ...  0  0 61]]\n",
      "at random state 50, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.85      0.83       103\n",
      "           1       0.57      0.72      0.64       102\n",
      "           2       0.84      0.92      0.88        93\n",
      "           3       0.80      0.75      0.77        95\n",
      "           4       0.90      0.90      0.90       106\n",
      "           5       0.77      0.77      0.77        90\n",
      "           6       0.93      0.91      0.92        92\n",
      "           7       0.70      0.81      0.75       102\n",
      "           8       0.96      0.92      0.94       105\n",
      "           9       0.73      0.80      0.77        96\n",
      "          10       0.74      0.84      0.79       103\n",
      "          11       0.92      0.93      0.93       115\n",
      "          12       0.94      0.92      0.93       106\n",
      "          13       0.79      0.74      0.76       110\n",
      "          14       0.90      0.94      0.92        95\n",
      "          15       0.96      0.94      0.95        81\n",
      "          16       0.92      0.90      0.91       109\n",
      "          17       0.89      0.86      0.87        99\n",
      "          18       0.85      0.89      0.87        91\n",
      "          19       0.77      0.80      0.78        98\n",
      "          20       0.91      0.85      0.88        87\n",
      "          21       0.67      0.69      0.68       108\n",
      "          22       0.93      0.99      0.96        97\n",
      "          23       0.75      0.79      0.77        97\n",
      "          24       0.63      0.56      0.59       115\n",
      "          25       0.86      0.96      0.91       105\n",
      "          26       0.72      0.84      0.78        91\n",
      "          27       0.82      0.79      0.80        96\n",
      "          28       0.95      0.86      0.90       100\n",
      "          29       0.95      0.91      0.93       104\n",
      "          30       0.93      0.90      0.91       106\n",
      "          31       0.56      0.59      0.58       103\n",
      "          32       0.88      0.82      0.85       107\n",
      "          33       0.77      0.73      0.75        98\n",
      "          34       0.91      0.92      0.92       101\n",
      "          35       0.87      0.96      0.91        99\n",
      "          36       0.75      0.76      0.75        99\n",
      "          37       0.70      0.70      0.70        99\n",
      "          38       0.81      0.75      0.78       114\n",
      "          39       0.84      0.78      0.81       113\n",
      "          40       0.78      0.87      0.82       100\n",
      "          41       0.78      0.69      0.73       105\n",
      "          42       0.87      0.86      0.87        88\n",
      "          43       0.88      0.78      0.83        97\n",
      "          44       0.81      0.86      0.84       103\n",
      "          45       0.97      0.91      0.94        92\n",
      "          46       0.92      0.92      0.92        83\n",
      "          47       0.81      0.80      0.80        90\n",
      "          48       0.88      0.70      0.78       107\n",
      "          49       0.80      0.84      0.82        97\n",
      "          50       0.46      0.41      0.43        86\n",
      "          51       1.00      1.00      1.00        61\n",
      "          52       0.93      0.94      0.94        70\n",
      "          53       0.81      0.70      0.75        87\n",
      "\n",
      "    accuracy                           0.82      5296\n",
      "   macro avg       0.83      0.82      0.82      5296\n",
      "weighted avg       0.82      0.82      0.82      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 51,  accuracy score is 0.8064577039274925\n",
      "at random state 51, confusion matrix is [[97  0  0 ...  0  0  0]\n",
      " [ 0 80  0 ...  0  0  1]\n",
      " [ 0  0 91 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 41  0  0]\n",
      " [ 0  0  0 ...  0 74  0]\n",
      " [ 0  0  3 ...  0  0 80]]\n",
      "at random state 51, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.91      0.83       107\n",
      "           1       0.76      0.71      0.74       112\n",
      "           2       0.83      0.94      0.88        97\n",
      "           3       0.82      0.82      0.82        95\n",
      "           4       0.84      0.90      0.87       114\n",
      "           5       0.77      0.72      0.75       116\n",
      "           6       0.91      0.97      0.94        76\n",
      "           7       0.69      0.83      0.75        94\n",
      "           8       0.95      0.94      0.95       102\n",
      "           9       0.74      0.80      0.77       100\n",
      "          10       0.67      0.70      0.68       114\n",
      "          11       0.82      0.93      0.87        96\n",
      "          12       0.91      0.90      0.91       119\n",
      "          13       0.57      0.69      0.62        97\n",
      "          14       0.88      0.90      0.89       102\n",
      "          15       0.94      0.98      0.96        81\n",
      "          16       0.94      0.88      0.91        99\n",
      "          17       0.90      0.87      0.88        99\n",
      "          18       0.89      0.83      0.86       103\n",
      "          19       0.84      0.83      0.83        92\n",
      "          20       0.88      0.83      0.86       103\n",
      "          21       0.62      0.76      0.69       105\n",
      "          22       0.97      0.97      0.97        96\n",
      "          23       0.80      0.74      0.77        94\n",
      "          24       0.51      0.56      0.53        91\n",
      "          25       0.91      0.99      0.95        87\n",
      "          26       0.66      0.65      0.65        94\n",
      "          27       0.85      0.75      0.80       102\n",
      "          28       0.97      0.87      0.92        90\n",
      "          29       0.96      0.93      0.94        97\n",
      "          30       0.89      0.76      0.82        95\n",
      "          31       0.49      0.60      0.54        83\n",
      "          32       0.84      0.78      0.81       101\n",
      "          33       0.80      0.75      0.78       102\n",
      "          34       0.89      0.88      0.89        97\n",
      "          35       0.81      0.87      0.84       102\n",
      "          36       0.68      0.74      0.71        86\n",
      "          37       0.79      0.75      0.77       106\n",
      "          38       0.72      0.77      0.75       106\n",
      "          39       0.85      0.81      0.83       102\n",
      "          40       0.77      0.79      0.78       109\n",
      "          41       0.80      0.70      0.74        96\n",
      "          42       0.89      0.79      0.84       106\n",
      "          43       0.87      0.75      0.80       110\n",
      "          44       0.83      0.82      0.83       111\n",
      "          45       0.95      0.82      0.88        87\n",
      "          46       0.90      0.93      0.91        98\n",
      "          47       0.73      0.75      0.74        95\n",
      "          48       0.82      0.67      0.74        99\n",
      "          49       0.74      0.80      0.77        91\n",
      "          50       0.60      0.49      0.54       108\n",
      "          51       1.00      1.00      1.00        41\n",
      "          52       0.97      0.93      0.95        80\n",
      "          53       0.78      0.72      0.75       111\n",
      "\n",
      "    accuracy                           0.81      5296\n",
      "   macro avg       0.81      0.81      0.81      5296\n",
      "weighted avg       0.81      0.81      0.81      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 52,  accuracy score is 0.8153323262839879\n",
      "at random state 52, confusion matrix is [[ 92   0   0 ...   0   0   0]\n",
      " [  0  73   0 ...   0   0   2]\n",
      " [  0   0 100 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  57   0   0]\n",
      " [  0   0   0 ...   1  74   0]\n",
      " [  0   0   2 ...   0   0  71]]\n",
      "at random state 52, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.90      0.83       102\n",
      "           1       0.62      0.72      0.66       102\n",
      "           2       0.88      0.91      0.90       110\n",
      "           3       0.86      0.81      0.83        83\n",
      "           4       0.88      0.88      0.88       101\n",
      "           5       0.76      0.75      0.75       106\n",
      "           6       0.93      0.97      0.95        91\n",
      "           7       0.73      0.86      0.79        93\n",
      "           8       0.92      0.94      0.93        94\n",
      "           9       0.76      0.78      0.77       104\n",
      "          10       0.66      0.81      0.72        94\n",
      "          11       0.85      0.92      0.88       103\n",
      "          12       0.94      0.84      0.88       105\n",
      "          13       0.65      0.75      0.70        88\n",
      "          14       0.84      0.87      0.86       100\n",
      "          15       0.91      0.91      0.91        69\n",
      "          16       0.86      0.92      0.89        97\n",
      "          17       0.89      0.78      0.83        95\n",
      "          18       0.87      0.90      0.88       111\n",
      "          19       0.87      0.79      0.83       114\n",
      "          20       0.91      0.82      0.87       102\n",
      "          21       0.61      0.76      0.68        90\n",
      "          22       0.92      0.93      0.93       102\n",
      "          23       0.79      0.83      0.81        94\n",
      "          24       0.63      0.59      0.61        96\n",
      "          25       0.88      0.93      0.90       105\n",
      "          26       0.74      0.81      0.77        93\n",
      "          27       0.90      0.86      0.88       103\n",
      "          28       0.96      0.92      0.94       106\n",
      "          29       0.93      0.87      0.90        93\n",
      "          30       0.92      0.89      0.91        93\n",
      "          31       0.58      0.64      0.61       105\n",
      "          32       0.81      0.84      0.82        98\n",
      "          33       0.76      0.80      0.78       107\n",
      "          34       0.89      0.86      0.88       115\n",
      "          35       0.88      0.94      0.91       108\n",
      "          36       0.72      0.78      0.75       111\n",
      "          37       0.78      0.75      0.76       110\n",
      "          38       0.78      0.77      0.77        95\n",
      "          39       0.76      0.78      0.77        85\n",
      "          40       0.75      0.85      0.80        92\n",
      "          41       0.77      0.69      0.73       112\n",
      "          42       0.88      0.78      0.83        82\n",
      "          43       0.93      0.72      0.81       104\n",
      "          44       0.85      0.82      0.83       101\n",
      "          45       0.94      0.83      0.88        78\n",
      "          46       0.86      0.87      0.86        92\n",
      "          47       0.79      0.75      0.77       108\n",
      "          48       0.85      0.64      0.73       117\n",
      "          49       0.87      0.75      0.81       105\n",
      "          50       0.52      0.47      0.50        93\n",
      "          51       0.98      1.00      0.99        57\n",
      "          52       0.93      0.91      0.92        81\n",
      "          53       0.88      0.70      0.78       101\n",
      "\n",
      "    accuracy                           0.82      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.82      0.82      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 53,  accuracy score is 0.8209969788519638\n",
      "at random state 53, confusion matrix is [[ 90   0   0 ...   0   0   0]\n",
      " [  0  79   0 ...   0   0   0]\n",
      " [  0   0 101 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  51   0   0]\n",
      " [  0   0   0 ...   1  72   0]\n",
      " [  0   3   1 ...   0   0  78]]\n",
      "at random state 53, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.89      0.85       101\n",
      "           1       0.71      0.72      0.71       110\n",
      "           2       0.88      0.94      0.91       107\n",
      "           3       0.82      0.80      0.81        89\n",
      "           4       0.86      0.94      0.90       109\n",
      "           5       0.81      0.75      0.78       110\n",
      "           6       0.93      0.97      0.95        95\n",
      "           7       0.74      0.83      0.78        89\n",
      "           8       0.95      0.94      0.94        93\n",
      "           9       0.77      0.85      0.81       103\n",
      "          10       0.67      0.73      0.70       110\n",
      "          11       0.74      0.91      0.82        88\n",
      "          12       0.93      0.94      0.94       107\n",
      "          13       0.69      0.72      0.70        95\n",
      "          14       0.90      0.94      0.92        94\n",
      "          15       0.86      0.96      0.91        84\n",
      "          16       0.89      0.86      0.87       112\n",
      "          17       0.94      0.84      0.88        91\n",
      "          18       0.88      0.84      0.86       100\n",
      "          19       0.81      0.83      0.82        92\n",
      "          20       0.89      0.86      0.87       112\n",
      "          21       0.60      0.76      0.67        89\n",
      "          22       1.00      0.92      0.96       107\n",
      "          23       0.81      0.86      0.83        97\n",
      "          24       0.51      0.58      0.55        89\n",
      "          25       0.90      0.90      0.90       106\n",
      "          26       0.74      0.80      0.77        88\n",
      "          27       0.82      0.81      0.81        98\n",
      "          28       0.98      0.92      0.95        92\n",
      "          29       0.92      0.95      0.94        86\n",
      "          30       0.89      0.86      0.87        90\n",
      "          31       0.59      0.68      0.64       101\n",
      "          32       0.88      0.82      0.85        96\n",
      "          33       0.73      0.75      0.74       102\n",
      "          34       0.89      0.85      0.87       102\n",
      "          35       0.91      0.92      0.91        99\n",
      "          36       0.78      0.70      0.74        94\n",
      "          37       0.81      0.77      0.79       114\n",
      "          38       0.79      0.79      0.79       106\n",
      "          39       0.78      0.79      0.79       107\n",
      "          40       0.79      0.82      0.80        98\n",
      "          41       0.69      0.78      0.73        99\n",
      "          42       0.94      0.89      0.91        99\n",
      "          43       0.86      0.75      0.80       104\n",
      "          44       0.88      0.77      0.82       113\n",
      "          45       0.94      0.91      0.92        80\n",
      "          46       0.90      0.94      0.92       100\n",
      "          47       0.84      0.79      0.81        90\n",
      "          48       0.86      0.64      0.73       101\n",
      "          49       0.79      0.78      0.78        99\n",
      "          50       0.62      0.44      0.52       114\n",
      "          51       0.98      1.00      0.99        51\n",
      "          52       1.00      0.88      0.94        82\n",
      "          53       0.81      0.70      0.75       112\n",
      "\n",
      "    accuracy                           0.82      5296\n",
      "   macro avg       0.83      0.83      0.82      5296\n",
      "weighted avg       0.82      0.82      0.82      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 54,  accuracy score is 0.8047583081570997\n",
      "at random state 54, confusion matrix is [[91  0  0 ...  0  0  0]\n",
      " [ 0 78  0 ...  0  0  3]\n",
      " [ 0  0 89 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 52  0  0]\n",
      " [ 0  0  0 ...  0 70  0]\n",
      " [ 0  0  2 ...  0  0 63]]\n",
      "at random state 54, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.93      0.86        98\n",
      "           1       0.68      0.74      0.71       105\n",
      "           2       0.87      0.93      0.90        96\n",
      "           3       0.79      0.84      0.81        92\n",
      "           4       0.88      0.89      0.89       102\n",
      "           5       0.74      0.73      0.74       120\n",
      "           6       0.91      0.96      0.93        81\n",
      "           7       0.72      0.80      0.76        92\n",
      "           8       0.92      0.95      0.93        92\n",
      "           9       0.68      0.84      0.75       110\n",
      "          10       0.68      0.78      0.72        91\n",
      "          11       0.72      0.91      0.81        98\n",
      "          12       0.90      0.89      0.90       101\n",
      "          13       0.72      0.62      0.67       117\n",
      "          14       0.90      0.88      0.89        90\n",
      "          15       0.95      0.97      0.96        73\n",
      "          16       0.90      0.88      0.89       110\n",
      "          17       0.92      0.84      0.88       108\n",
      "          18       0.82      0.87      0.84        98\n",
      "          19       0.76      0.76      0.76        91\n",
      "          20       0.87      0.84      0.86        96\n",
      "          21       0.69      0.68      0.69       113\n",
      "          22       0.96      0.90      0.93        81\n",
      "          23       0.82      0.72      0.77       116\n",
      "          24       0.56      0.63      0.59       105\n",
      "          25       0.79      1.00      0.88        77\n",
      "          26       0.73      0.77      0.75       110\n",
      "          27       0.81      0.83      0.82        96\n",
      "          28       0.96      0.90      0.93       105\n",
      "          29       0.97      0.92      0.95       115\n",
      "          30       0.93      0.88      0.90        90\n",
      "          31       0.55      0.60      0.58        95\n",
      "          32       0.85      0.80      0.82       103\n",
      "          33       0.71      0.71      0.71        97\n",
      "          34       0.92      0.87      0.90        95\n",
      "          35       0.92      0.88      0.90       102\n",
      "          36       0.68      0.70      0.69        97\n",
      "          37       0.74      0.74      0.74       109\n",
      "          38       0.80      0.77      0.78       116\n",
      "          39       0.76      0.81      0.78       100\n",
      "          40       0.75      0.86      0.80       103\n",
      "          41       0.74      0.64      0.69       104\n",
      "          42       0.91      0.86      0.88       101\n",
      "          43       0.86      0.67      0.75        97\n",
      "          44       0.81      0.81      0.81        99\n",
      "          45       0.95      0.92      0.93        86\n",
      "          46       0.92      0.89      0.91        94\n",
      "          47       0.82      0.82      0.82        87\n",
      "          48       0.79      0.63      0.70       104\n",
      "          49       0.84      0.78      0.81       112\n",
      "          50       0.53      0.38      0.44       104\n",
      "          51       1.00      1.00      1.00        52\n",
      "          52       0.95      0.93      0.94        75\n",
      "          53       0.76      0.66      0.71        95\n",
      "\n",
      "    accuracy                           0.80      5296\n",
      "   macro avg       0.81      0.81      0.81      5296\n",
      "weighted avg       0.81      0.80      0.80      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 55,  accuracy score is 0.8121223564954683\n",
      "at random state 55, confusion matrix is [[93  0  0 ...  0  0  0]\n",
      " [ 0 76  0 ...  0  0  0]\n",
      " [ 0  0 91 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 56  0  0]\n",
      " [ 0  0  0 ...  0 75  0]\n",
      " [ 0  0  1 ...  0  0 67]]\n",
      "at random state 55, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.90      0.84       103\n",
      "           1       0.68      0.75      0.72       101\n",
      "           2       0.82      0.92      0.87        99\n",
      "           3       0.78      0.84      0.81        83\n",
      "           4       0.85      0.90      0.87        91\n",
      "           5       0.72      0.71      0.72       105\n",
      "           6       0.93      0.94      0.93        84\n",
      "           7       0.73      0.86      0.79       114\n",
      "           8       0.91      0.93      0.92       109\n",
      "           9       0.65      0.79      0.71        95\n",
      "          10       0.73      0.76      0.75       104\n",
      "          11       0.87      0.91      0.89       102\n",
      "          12       0.93      0.85      0.89        93\n",
      "          13       0.68      0.63      0.65        97\n",
      "          14       0.88      0.91      0.89        96\n",
      "          15       0.95      0.90      0.92        80\n",
      "          16       0.95      0.87      0.91       100\n",
      "          17       0.92      0.85      0.88       104\n",
      "          18       0.79      0.83      0.81        86\n",
      "          19       0.69      0.77      0.73        86\n",
      "          20       0.83      0.88      0.86        98\n",
      "          21       0.67      0.67      0.67       117\n",
      "          22       0.97      0.95      0.96       108\n",
      "          23       0.80      0.74      0.77        99\n",
      "          24       0.72      0.57      0.64       102\n",
      "          25       0.81      0.94      0.87        98\n",
      "          26       0.76      0.74      0.75       109\n",
      "          27       0.81      0.84      0.83       109\n",
      "          28       0.98      0.89      0.93       104\n",
      "          29       0.95      0.91      0.93        87\n",
      "          30       0.90      0.87      0.88       115\n",
      "          31       0.55      0.64      0.59       106\n",
      "          32       0.84      0.78      0.81       102\n",
      "          33       0.62      0.70      0.66        97\n",
      "          34       0.94      0.89      0.91        96\n",
      "          35       0.92      0.96      0.94       104\n",
      "          36       0.78      0.77      0.77        95\n",
      "          37       0.71      0.68      0.70        96\n",
      "          38       0.79      0.73      0.76       105\n",
      "          39       0.87      0.83      0.85       102\n",
      "          40       0.85      0.84      0.85       101\n",
      "          41       0.77      0.65      0.71       104\n",
      "          42       0.86      0.88      0.87        88\n",
      "          43       0.84      0.71      0.77       105\n",
      "          44       0.85      0.86      0.85       102\n",
      "          45       0.99      0.89      0.93        88\n",
      "          46       0.96      0.98      0.97        98\n",
      "          47       0.80      0.79      0.80       114\n",
      "          48       0.87      0.68      0.76       109\n",
      "          49       0.69      0.72      0.71        87\n",
      "          50       0.51      0.53      0.52        94\n",
      "          51       1.00      1.00      1.00        56\n",
      "          52       0.97      0.95      0.96        79\n",
      "          53       0.78      0.74      0.76        90\n",
      "\n",
      "    accuracy                           0.81      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.81      0.81      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 56,  accuracy score is 0.8106117824773413\n",
      "at random state 56, confusion matrix is [[ 96   0   0 ...   0   0   0]\n",
      " [  0  75   0 ...   0   0   1]\n",
      " [  0   0 107 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  55   0   0]\n",
      " [  0   0   0 ...   0  73   0]\n",
      " [  0   0   1 ...   0   0  73]]\n",
      "at random state 56, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.92      0.87       104\n",
      "           1       0.69      0.67      0.68       112\n",
      "           2       0.86      0.92      0.89       116\n",
      "           3       0.89      0.83      0.86       112\n",
      "           4       0.91      0.89      0.90       108\n",
      "           5       0.81      0.67      0.73       109\n",
      "           6       0.95      0.96      0.95        94\n",
      "           7       0.68      0.81      0.74        80\n",
      "           8       0.89      0.95      0.92       106\n",
      "           9       0.76      0.92      0.83        95\n",
      "          10       0.72      0.66      0.69       110\n",
      "          11       0.80      0.95      0.87        97\n",
      "          12       0.91      0.89      0.90        89\n",
      "          13       0.64      0.78      0.70        89\n",
      "          14       0.88      0.92      0.90        93\n",
      "          15       0.93      0.90      0.92        84\n",
      "          16       0.85      0.92      0.88       103\n",
      "          17       0.96      0.87      0.91        90\n",
      "          18       0.83      0.90      0.86        92\n",
      "          19       0.75      0.84      0.79       100\n",
      "          20       0.90      0.89      0.90       107\n",
      "          21       0.59      0.75      0.66       102\n",
      "          22       0.95      0.91      0.93        97\n",
      "          23       0.84      0.77      0.80       104\n",
      "          24       0.51      0.67      0.58        95\n",
      "          25       0.85      0.94      0.89        93\n",
      "          26       0.74      0.84      0.79        97\n",
      "          27       0.80      0.77      0.78        99\n",
      "          28       0.97      0.91      0.94        92\n",
      "          29       0.96      0.91      0.93       100\n",
      "          30       0.89      0.90      0.90        94\n",
      "          31       0.49      0.56      0.52        89\n",
      "          32       0.88      0.78      0.83       110\n",
      "          33       0.76      0.75      0.76       101\n",
      "          34       0.92      0.88      0.90       116\n",
      "          35       0.92      0.89      0.90        98\n",
      "          36       0.72      0.70      0.71       104\n",
      "          37       0.74      0.75      0.75       100\n",
      "          38       0.85      0.71      0.78       112\n",
      "          39       0.82      0.78      0.80        96\n",
      "          40       0.77      0.82      0.80       100\n",
      "          41       0.74      0.66      0.70       122\n",
      "          42       0.89      0.81      0.84        77\n",
      "          43       0.82      0.67      0.74        99\n",
      "          44       0.84      0.80      0.82       108\n",
      "          45       0.88      0.91      0.89        86\n",
      "          46       0.92      0.94      0.93        83\n",
      "          47       0.74      0.78      0.76        95\n",
      "          48       0.81      0.58      0.67       102\n",
      "          49       0.74      0.84      0.79        94\n",
      "          50       0.54      0.37      0.44       103\n",
      "          51       1.00      1.00      1.00        55\n",
      "          52       0.96      0.97      0.97        75\n",
      "          53       0.87      0.68      0.76       108\n",
      "\n",
      "    accuracy                           0.81      5296\n",
      "   macro avg       0.82      0.82      0.81      5296\n",
      "weighted avg       0.82      0.81      0.81      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 57,  accuracy score is 0.802870090634441\n",
      "at random state 57, confusion matrix is [[86  0  0 ...  0  0  0]\n",
      " [ 0 77  0 ...  0  0  1]\n",
      " [ 0  0 81 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 42  0  0]\n",
      " [ 0  0  0 ...  0 72  0]\n",
      " [ 0  0  1 ...  0  0 88]]\n",
      "at random state 57, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.86      0.76       100\n",
      "           1       0.66      0.71      0.69       108\n",
      "           2       0.81      0.94      0.87        86\n",
      "           3       0.88      0.75      0.81       122\n",
      "           4       0.81      0.91      0.86        97\n",
      "           5       0.72      0.72      0.72       109\n",
      "           6       0.90      0.96      0.92        90\n",
      "           7       0.72      0.83      0.77        84\n",
      "           8       0.94      0.95      0.94        92\n",
      "           9       0.74      0.70      0.72       103\n",
      "          10       0.61      0.70      0.65        91\n",
      "          11       0.77      0.87      0.82        95\n",
      "          12       0.88      0.93      0.90        94\n",
      "          13       0.72      0.69      0.71       113\n",
      "          14       0.87      0.91      0.89       100\n",
      "          15       0.95      0.89      0.92        79\n",
      "          16       0.85      0.95      0.90        91\n",
      "          17       0.90      0.80      0.85       115\n",
      "          18       0.81      0.83      0.82       103\n",
      "          19       0.74      0.77      0.76       102\n",
      "          20       0.88      0.87      0.87        99\n",
      "          21       0.68      0.70      0.69       103\n",
      "          22       0.96      0.94      0.95        97\n",
      "          23       0.73      0.75      0.74        99\n",
      "          24       0.55      0.60      0.58        96\n",
      "          25       0.80      0.96      0.88       103\n",
      "          26       0.67      0.83      0.74        92\n",
      "          27       0.91      0.80      0.85       102\n",
      "          28       0.95      0.85      0.90        96\n",
      "          29       0.92      0.88      0.90        97\n",
      "          30       0.92      0.82      0.87       103\n",
      "          31       0.57      0.58      0.58       113\n",
      "          32       0.85      0.81      0.83       114\n",
      "          33       0.66      0.77      0.71        97\n",
      "          34       0.92      0.92      0.92       106\n",
      "          35       0.90      0.94      0.92        89\n",
      "          36       0.83      0.68      0.75       125\n",
      "          37       0.74      0.78      0.76        93\n",
      "          38       0.74      0.78      0.76        93\n",
      "          39       0.81      0.83      0.82       110\n",
      "          40       0.82      0.83      0.82        99\n",
      "          41       0.71      0.67      0.69        82\n",
      "          42       0.92      0.80      0.86        89\n",
      "          43       0.82      0.72      0.77        92\n",
      "          44       0.87      0.83      0.85        99\n",
      "          45       0.92      0.91      0.92        89\n",
      "          46       0.96      0.88      0.92        78\n",
      "          47       0.81      0.79      0.80       100\n",
      "          48       0.82      0.74      0.78       101\n",
      "          49       0.83      0.72      0.77       109\n",
      "          50       0.56      0.48      0.52       115\n",
      "          51       1.00      1.00      1.00        42\n",
      "          52       0.96      0.91      0.94        79\n",
      "          53       0.85      0.73      0.79       121\n",
      "\n",
      "    accuracy                           0.80      5296\n",
      "   macro avg       0.81      0.81      0.81      5296\n",
      "weighted avg       0.81      0.80      0.80      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 58,  accuracy score is 0.8175981873111783\n",
      "at random state 58, confusion matrix is [[111   0   0 ...   0   0   0]\n",
      " [  0  70   0 ...   0   0   2]\n",
      " [  0   0  95 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  45   0   0]\n",
      " [  0   0   0 ...   0  75   0]\n",
      " [  0   0   1 ...   0   0  70]]\n",
      "at random state 58, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.95      0.88       117\n",
      "           1       0.59      0.71      0.65        98\n",
      "           2       0.86      0.90      0.88       105\n",
      "           3       0.89      0.82      0.85       107\n",
      "           4       0.84      0.85      0.84        89\n",
      "           5       0.82      0.78      0.80       100\n",
      "           6       0.95      0.94      0.95        88\n",
      "           7       0.78      0.87      0.82       101\n",
      "           8       0.94      0.91      0.93       112\n",
      "           9       0.64      0.73      0.68        92\n",
      "          10       0.56      0.73      0.64        86\n",
      "          11       0.82      0.91      0.86       100\n",
      "          12       0.93      0.88      0.90       102\n",
      "          13       0.67      0.66      0.66        90\n",
      "          14       0.91      0.92      0.92       112\n",
      "          15       1.00      0.94      0.97        90\n",
      "          16       0.93      0.89      0.91        93\n",
      "          17       0.90      0.83      0.87       112\n",
      "          18       0.87      0.83      0.85       111\n",
      "          19       0.73      0.80      0.76        93\n",
      "          20       0.95      0.87      0.91       105\n",
      "          21       0.67      0.71      0.69       100\n",
      "          22       0.94      0.95      0.94        92\n",
      "          23       0.81      0.83      0.82       103\n",
      "          24       0.57      0.65      0.61       104\n",
      "          25       0.90      0.98      0.94       108\n",
      "          26       0.75      0.75      0.75       106\n",
      "          27       0.84      0.79      0.81       110\n",
      "          28       0.97      0.90      0.93       102\n",
      "          29       0.94      0.97      0.96        99\n",
      "          30       0.90      0.89      0.90        94\n",
      "          31       0.53      0.58      0.55        95\n",
      "          32       0.91      0.81      0.86       106\n",
      "          33       0.73      0.78      0.76       105\n",
      "          34       0.91      0.86      0.88       115\n",
      "          35       0.89      0.91      0.90       100\n",
      "          36       0.82      0.66      0.73       104\n",
      "          37       0.74      0.79      0.77        87\n",
      "          38       0.80      0.74      0.77        99\n",
      "          39       0.78      0.82      0.80        98\n",
      "          40       0.76      0.93      0.84        99\n",
      "          41       0.81      0.67      0.73        87\n",
      "          42       0.87      0.79      0.83        95\n",
      "          43       0.77      0.78      0.77        92\n",
      "          44       0.85      0.82      0.83       105\n",
      "          45       0.91      0.92      0.92        89\n",
      "          46       0.93      0.90      0.92        92\n",
      "          47       0.78      0.66      0.72        95\n",
      "          48       0.83      0.60      0.70       103\n",
      "          49       0.86      0.83      0.84        98\n",
      "          50       0.53      0.47      0.50        94\n",
      "          51       1.00      1.00      1.00        45\n",
      "          52       0.96      0.93      0.94        81\n",
      "          53       0.78      0.77      0.77        91\n",
      "\n",
      "    accuracy                           0.82      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.82      0.82      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 59,  accuracy score is 0.8096676737160121\n",
      "at random state 59, confusion matrix is [[ 90   0   0 ...   0   0   0]\n",
      " [  0  70   0 ...   0   0   1]\n",
      " [  0   0 103 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  63   0   0]\n",
      " [  0   0   0 ...   0  77   0]\n",
      " [  0   0   1 ...   0   0  82]]\n",
      "at random state 59, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.91      0.85        99\n",
      "           1       0.74      0.67      0.70       104\n",
      "           2       0.85      0.96      0.90       107\n",
      "           3       0.89      0.78      0.83        83\n",
      "           4       0.90      0.88      0.89       129\n",
      "           5       0.76      0.76      0.76        97\n",
      "           6       0.89      0.98      0.93        86\n",
      "           7       0.67      0.82      0.74        97\n",
      "           8       0.91      0.91      0.91        95\n",
      "           9       0.73      0.76      0.75       105\n",
      "          10       0.65      0.65      0.65       109\n",
      "          11       0.82      0.91      0.86        99\n",
      "          12       0.96      0.90      0.93        98\n",
      "          13       0.61      0.72      0.66        98\n",
      "          14       0.88      0.93      0.90        90\n",
      "          15       0.94      0.93      0.94        88\n",
      "          16       0.86      0.91      0.88        91\n",
      "          17       0.91      0.81      0.86       106\n",
      "          18       0.79      0.85      0.82        99\n",
      "          19       0.79      0.80      0.80        97\n",
      "          20       0.90      0.75      0.82        96\n",
      "          21       0.65      0.62      0.64       104\n",
      "          22       0.97      0.91      0.94       100\n",
      "          23       0.77      0.85      0.81        97\n",
      "          24       0.61      0.59      0.60        94\n",
      "          25       0.85      0.92      0.88       104\n",
      "          26       0.76      0.83      0.79       106\n",
      "          27       0.85      0.77      0.81        95\n",
      "          28       0.95      0.93      0.94        92\n",
      "          29       0.98      0.91      0.94       102\n",
      "          30       0.89      0.90      0.90       104\n",
      "          31       0.49      0.59      0.53        90\n",
      "          32       0.80      0.81      0.80       101\n",
      "          33       0.67      0.81      0.73        99\n",
      "          34       0.93      0.92      0.92       112\n",
      "          35       0.91      0.88      0.89       113\n",
      "          36       0.77      0.68      0.72        91\n",
      "          37       0.71      0.71      0.71       103\n",
      "          38       0.81      0.71      0.76        84\n",
      "          39       0.74      0.78      0.76       103\n",
      "          40       0.74      0.83      0.78        96\n",
      "          41       0.80      0.66      0.72       108\n",
      "          42       0.88      0.81      0.84        89\n",
      "          43       0.86      0.77      0.81        90\n",
      "          44       0.83      0.88      0.85        96\n",
      "          45       0.92      0.89      0.90        87\n",
      "          46       0.82      0.96      0.88        89\n",
      "          47       0.79      0.78      0.79        88\n",
      "          48       0.91      0.70      0.79       122\n",
      "          49       0.82      0.78      0.80       102\n",
      "          50       0.53      0.48      0.51        93\n",
      "          51       1.00      1.00      1.00        63\n",
      "          52       0.96      0.94      0.95        82\n",
      "          53       0.85      0.66      0.74       124\n",
      "\n",
      "    accuracy                           0.81      5296\n",
      "   macro avg       0.82      0.81      0.81      5296\n",
      "weighted avg       0.81      0.81      0.81      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 60,  accuracy score is 0.8121223564954683\n",
      "at random state 60, confusion matrix is [[90  0  0 ...  0  0  0]\n",
      " [ 0 75  0 ...  0  0  1]\n",
      " [ 0  0 94 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 42  0  0]\n",
      " [ 0  0  0 ...  0 75  0]\n",
      " [ 0  0  3 ...  0  0 79]]\n",
      "at random state 60, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.89      0.81       101\n",
      "           1       0.60      0.78      0.68        96\n",
      "           2       0.82      0.94      0.87       100\n",
      "           3       0.87      0.80      0.83       104\n",
      "           4       0.86      0.87      0.87       101\n",
      "           5       0.77      0.77      0.77       106\n",
      "           6       0.94      0.94      0.94        86\n",
      "           7       0.74      0.87      0.80       100\n",
      "           8       0.92      0.92      0.92       102\n",
      "           9       0.70      0.83      0.76       104\n",
      "          10       0.72      0.71      0.71       114\n",
      "          11       0.85      0.92      0.88        97\n",
      "          12       0.88      0.85      0.86        97\n",
      "          13       0.67      0.54      0.60       101\n",
      "          14       0.88      0.93      0.91        90\n",
      "          15       0.98      0.95      0.96        91\n",
      "          16       0.87      0.91      0.89       104\n",
      "          17       0.90      0.78      0.84       105\n",
      "          18       0.83      0.80      0.81        89\n",
      "          19       0.78      0.83      0.80       104\n",
      "          20       0.89      0.76      0.82        96\n",
      "          21       0.63      0.70      0.66        94\n",
      "          22       0.94      0.94      0.94       103\n",
      "          23       0.78      0.78      0.78       106\n",
      "          24       0.60      0.63      0.62       104\n",
      "          25       0.93      0.99      0.96        93\n",
      "          26       0.74      0.72      0.73       111\n",
      "          27       0.79      0.77      0.78        88\n",
      "          28       0.99      0.90      0.94        93\n",
      "          29       0.97      0.96      0.96        94\n",
      "          30       0.93      0.83      0.88       109\n",
      "          31       0.64      0.57      0.60       111\n",
      "          32       0.91      0.83      0.87       115\n",
      "          33       0.75      0.78      0.77        97\n",
      "          34       0.90      0.89      0.90       101\n",
      "          35       0.92      0.96      0.94       123\n",
      "          36       0.72      0.77      0.74        95\n",
      "          37       0.74      0.74      0.74        82\n",
      "          38       0.74      0.87      0.80        90\n",
      "          39       0.73      0.81      0.77       101\n",
      "          40       0.79      0.87      0.83       107\n",
      "          41       0.70      0.67      0.68       106\n",
      "          42       0.85      0.82      0.84        84\n",
      "          43       0.86      0.69      0.77       109\n",
      "          44       0.86      0.81      0.83        95\n",
      "          45       0.94      0.95      0.94        94\n",
      "          46       0.91      0.97      0.94        94\n",
      "          47       0.76      0.75      0.75        91\n",
      "          48       0.84      0.56      0.67       104\n",
      "          49       0.85      0.81      0.83        96\n",
      "          50       0.47      0.43      0.45        88\n",
      "          51       0.98      1.00      0.99        42\n",
      "          52       1.00      0.94      0.97        80\n",
      "          53       0.78      0.73      0.76       108\n",
      "\n",
      "    accuracy                           0.81      5296\n",
      "   macro avg       0.82      0.82      0.81      5296\n",
      "weighted avg       0.82      0.81      0.81      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 61,  accuracy score is 0.8115558912386707\n",
      "at random state 61, confusion matrix is [[97  0  0 ...  0  0  0]\n",
      " [ 0 79  0 ...  0  0  1]\n",
      " [ 0  0 98 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 51  0  0]\n",
      " [ 0  0  0 ...  0 74  0]\n",
      " [ 0  0  4 ...  0  0 77]]\n",
      "at random state 61, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.92      0.83       106\n",
      "           1       0.71      0.70      0.70       113\n",
      "           2       0.85      0.94      0.89       104\n",
      "           3       0.82      0.82      0.82        89\n",
      "           4       0.85      0.86      0.85        98\n",
      "           5       0.80      0.77      0.78       112\n",
      "           6       0.92      0.96      0.94        84\n",
      "           7       0.71      0.85      0.77        96\n",
      "           8       0.97      0.91      0.94       102\n",
      "           9       0.70      0.81      0.75       113\n",
      "          10       0.62      0.72      0.67       118\n",
      "          11       0.79      0.92      0.85        96\n",
      "          12       0.93      0.89      0.91       108\n",
      "          13       0.59      0.64      0.62        81\n",
      "          14       0.93      0.91      0.92       106\n",
      "          15       0.91      0.92      0.92        78\n",
      "          16       0.90      0.81      0.85       111\n",
      "          17       0.87      0.81      0.84       105\n",
      "          18       0.90      0.80      0.85       110\n",
      "          19       0.78      0.74      0.76       108\n",
      "          20       0.87      0.80      0.83        98\n",
      "          21       0.67      0.76      0.71        93\n",
      "          22       0.92      0.94      0.93        93\n",
      "          23       0.80      0.70      0.75        94\n",
      "          24       0.59      0.69      0.64        93\n",
      "          25       0.88      0.94      0.91       104\n",
      "          26       0.74      0.67      0.70       103\n",
      "          27       0.81      0.84      0.82       104\n",
      "          28       0.93      0.91      0.92       101\n",
      "          29       0.92      0.95      0.94        86\n",
      "          30       0.87      0.85      0.86       104\n",
      "          31       0.60      0.60      0.60       110\n",
      "          32       0.87      0.82      0.84       102\n",
      "          33       0.75      0.74      0.74        95\n",
      "          34       0.96      0.89      0.92        99\n",
      "          35       0.91      0.92      0.91        98\n",
      "          36       0.70      0.72      0.71        93\n",
      "          37       0.77      0.77      0.77        99\n",
      "          38       0.82      0.73      0.77       104\n",
      "          39       0.71      0.76      0.74        99\n",
      "          40       0.80      0.81      0.81       106\n",
      "          41       0.75      0.75      0.75       106\n",
      "          42       0.82      0.82      0.82        85\n",
      "          43       0.89      0.77      0.82        98\n",
      "          44       0.81      0.84      0.82        95\n",
      "          45       0.99      0.93      0.96        89\n",
      "          46       0.91      0.91      0.91        94\n",
      "          47       0.82      0.74      0.78        94\n",
      "          48       0.82      0.69      0.75        90\n",
      "          49       0.85      0.85      0.85       110\n",
      "          50       0.54      0.46      0.50        89\n",
      "          51       1.00      1.00      1.00        51\n",
      "          52       0.97      0.94      0.95        79\n",
      "          53       0.83      0.77      0.80       100\n",
      "\n",
      "    accuracy                           0.81      5296\n",
      "   macro avg       0.82      0.81      0.81      5296\n",
      "weighted avg       0.82      0.81      0.81      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 62,  accuracy score is 0.8162764350453172\n",
      "at random state 62, confusion matrix is [[104   0   0 ...   0   0   0]\n",
      " [  0  67   0 ...   0   0   2]\n",
      " [  0   0  91 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  43   0   0]\n",
      " [  0   0   0 ...   0  71   0]\n",
      " [  0   0   2 ...   0   0  63]]\n",
      "at random state 62, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.91      0.82       114\n",
      "           1       0.66      0.66      0.66       101\n",
      "           2       0.85      0.91      0.88       100\n",
      "           3       0.81      0.78      0.80       101\n",
      "           4       0.89      0.90      0.90        84\n",
      "           5       0.71      0.71      0.71        91\n",
      "           6       0.96      0.99      0.97        88\n",
      "           7       0.78      0.79      0.79       101\n",
      "           8       0.93      0.92      0.93       115\n",
      "           9       0.70      0.80      0.75       104\n",
      "          10       0.60      0.74      0.66        96\n",
      "          11       0.82      0.95      0.88       104\n",
      "          12       0.86      0.92      0.89        91\n",
      "          13       0.65      0.71      0.68        91\n",
      "          14       0.90      0.92      0.91       104\n",
      "          15       0.94      0.92      0.93        86\n",
      "          16       0.91      0.88      0.90       108\n",
      "          17       0.89      0.92      0.90        72\n",
      "          18       0.87      0.81      0.84        96\n",
      "          19       0.75      0.81      0.78       105\n",
      "          20       0.87      0.84      0.85        98\n",
      "          21       0.62      0.67      0.64        99\n",
      "          22       0.94      0.95      0.95       104\n",
      "          23       0.78      0.83      0.80       109\n",
      "          24       0.53      0.52      0.52        94\n",
      "          25       0.90      0.98      0.94       105\n",
      "          26       0.75      0.75      0.75       105\n",
      "          27       0.84      0.83      0.83       100\n",
      "          28       0.97      0.93      0.95       118\n",
      "          29       0.97      0.95      0.96        98\n",
      "          30       0.90      0.91      0.90       106\n",
      "          31       0.55      0.62      0.59        88\n",
      "          32       0.86      0.82      0.84       109\n",
      "          33       0.72      0.79      0.75        96\n",
      "          34       0.87      0.91      0.89       101\n",
      "          35       0.92      0.92      0.92       105\n",
      "          36       0.77      0.73      0.75       103\n",
      "          37       0.68      0.76      0.72        83\n",
      "          38       0.77      0.77      0.77       103\n",
      "          39       0.88      0.77      0.82       115\n",
      "          40       0.87      0.76      0.81       105\n",
      "          41       0.76      0.65      0.70       111\n",
      "          42       0.88      0.82      0.85        99\n",
      "          43       0.82      0.78      0.80        91\n",
      "          44       0.81      0.76      0.78        99\n",
      "          45       0.94      0.93      0.94        91\n",
      "          46       0.95      0.97      0.96        89\n",
      "          47       0.86      0.79      0.82       101\n",
      "          48       0.89      0.64      0.75       104\n",
      "          49       0.85      0.84      0.84       106\n",
      "          50       0.60      0.42      0.49        96\n",
      "          51       1.00      1.00      1.00        43\n",
      "          52       0.95      0.95      0.95        75\n",
      "          53       0.79      0.66      0.72        95\n",
      "\n",
      "    accuracy                           0.82      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.82      0.82      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 63,  accuracy score is 0.80702416918429\n",
      "at random state 63, confusion matrix is [[ 79   0   0 ...   0   0   0]\n",
      " [  0  79   0 ...   0   0   1]\n",
      " [  0   0 105 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  42   0   0]\n",
      " [  0   0   0 ...   0  66   0]\n",
      " [  0   0   3 ...   0   0  67]]\n",
      "at random state 63, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.84      0.80        94\n",
      "           1       0.64      0.75      0.69       105\n",
      "           2       0.86      0.95      0.90       111\n",
      "           3       0.82      0.78      0.80       101\n",
      "           4       0.92      0.92      0.92        95\n",
      "           5       0.67      0.72      0.70        97\n",
      "           6       0.91      0.99      0.95        83\n",
      "           7       0.83      0.79      0.81       126\n",
      "           8       0.92      0.92      0.92       102\n",
      "           9       0.73      0.77      0.75       109\n",
      "          10       0.64      0.73      0.69       101\n",
      "          11       0.82      0.92      0.87       106\n",
      "          12       0.94      0.85      0.89        89\n",
      "          13       0.73      0.72      0.72       114\n",
      "          14       0.86      0.91      0.88       100\n",
      "          15       0.95      0.88      0.91        66\n",
      "          16       0.90      0.88      0.89       107\n",
      "          17       0.89      0.84      0.87        90\n",
      "          18       0.90      0.83      0.86       115\n",
      "          19       0.78      0.83      0.80       104\n",
      "          20       0.91      0.78      0.84       101\n",
      "          21       0.68      0.78      0.73       101\n",
      "          22       0.95      0.95      0.95        93\n",
      "          23       0.79      0.84      0.81        93\n",
      "          24       0.52      0.54      0.53       104\n",
      "          25       0.86      0.98      0.91        85\n",
      "          26       0.77      0.74      0.75       103\n",
      "          27       0.83      0.83      0.83       103\n",
      "          28       0.99      0.88      0.93       110\n",
      "          29       0.96      0.92      0.94       100\n",
      "          30       0.92      0.91      0.91        95\n",
      "          31       0.48      0.59      0.53        98\n",
      "          32       0.86      0.85      0.86       103\n",
      "          33       0.75      0.82      0.79       107\n",
      "          34       0.95      0.91      0.92        95\n",
      "          35       0.89      0.96      0.92       107\n",
      "          36       0.73      0.69      0.71        95\n",
      "          37       0.70      0.83      0.76        92\n",
      "          38       0.71      0.69      0.70        95\n",
      "          39       0.83      0.76      0.79       104\n",
      "          40       0.70      0.84      0.76        96\n",
      "          41       0.67      0.64      0.66        90\n",
      "          42       0.82      0.78      0.80        97\n",
      "          43       0.83      0.75      0.79       113\n",
      "          44       0.81      0.88      0.84        89\n",
      "          45       0.96      0.92      0.94        72\n",
      "          46       0.95      0.94      0.94        96\n",
      "          47       0.77      0.74      0.75       112\n",
      "          48       0.83      0.60      0.70       103\n",
      "          49       0.82      0.72      0.77       107\n",
      "          50       0.59      0.41      0.48       110\n",
      "          51       1.00      1.00      1.00        42\n",
      "          52       0.97      0.93      0.95        71\n",
      "          53       0.82      0.68      0.74        99\n",
      "\n",
      "    accuracy                           0.81      5296\n",
      "   macro avg       0.82      0.81      0.81      5296\n",
      "weighted avg       0.81      0.81      0.81      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 64,  accuracy score is 0.8141993957703928\n",
      "at random state 64, confusion matrix is [[ 89   0   0 ...   0   0   0]\n",
      " [  0  73   0 ...   0   0   1]\n",
      " [  0   0 109 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  41   0   0]\n",
      " [  0   0   0 ...   0  68   0]\n",
      " [  0   0   1 ...   0   0  71]]\n",
      "at random state 64, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.90      0.87        99\n",
      "           1       0.70      0.67      0.69       109\n",
      "           2       0.89      0.99      0.94       110\n",
      "           3       0.81      0.81      0.81        97\n",
      "           4       0.79      0.90      0.84        86\n",
      "           5       0.75      0.78      0.76        91\n",
      "           6       0.96      0.97      0.96        88\n",
      "           7       0.79      0.83      0.81       115\n",
      "           8       0.95      0.94      0.94       108\n",
      "           9       0.64      0.77      0.70        79\n",
      "          10       0.78      0.73      0.75       113\n",
      "          11       0.84      0.91      0.87        96\n",
      "          12       0.89      0.83      0.86        96\n",
      "          13       0.65      0.64      0.65       104\n",
      "          14       0.95      0.94      0.94       111\n",
      "          15       0.97      0.96      0.96        95\n",
      "          16       0.94      0.85      0.89       106\n",
      "          17       0.83      0.83      0.83        89\n",
      "          18       0.86      0.82      0.84        99\n",
      "          19       0.81      0.85      0.83       106\n",
      "          20       0.83      0.85      0.84        82\n",
      "          21       0.63      0.76      0.69       100\n",
      "          22       0.97      0.92      0.94       111\n",
      "          23       0.78      0.78      0.78        93\n",
      "          24       0.55      0.62      0.59        88\n",
      "          25       0.87      0.92      0.89       107\n",
      "          26       0.76      0.77      0.77       102\n",
      "          27       0.86      0.85      0.85        97\n",
      "          28       0.97      0.96      0.96        96\n",
      "          29       0.92      0.94      0.93        89\n",
      "          30       0.97      0.83      0.89       111\n",
      "          31       0.55      0.62      0.59        93\n",
      "          32       0.89      0.84      0.86       101\n",
      "          33       0.72      0.74      0.73       112\n",
      "          34       0.90      0.92      0.91        95\n",
      "          35       0.88      0.91      0.89       107\n",
      "          36       0.65      0.69      0.67       104\n",
      "          37       0.77      0.75      0.76       102\n",
      "          38       0.76      0.77      0.76       103\n",
      "          39       0.76      0.93      0.84        82\n",
      "          40       0.76      0.83      0.79       116\n",
      "          41       0.65      0.65      0.65        83\n",
      "          42       0.89      0.77      0.83        96\n",
      "          43       0.86      0.69      0.76       113\n",
      "          44       0.75      0.78      0.76        97\n",
      "          45       0.90      0.94      0.92        87\n",
      "          46       0.91      0.93      0.92        97\n",
      "          47       0.90      0.79      0.84       110\n",
      "          48       0.81      0.58      0.67       104\n",
      "          49       0.84      0.85      0.84       104\n",
      "          50       0.60      0.43      0.50       105\n",
      "          51       1.00      1.00      1.00        41\n",
      "          52       1.00      0.92      0.96        74\n",
      "          53       0.70      0.73      0.72        97\n",
      "\n",
      "    accuracy                           0.81      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.81      0.81      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 65,  accuracy score is 0.8092900302114804\n",
      "at random state 65, confusion matrix is [[95  0  0 ...  0  0  0]\n",
      " [ 0 59  0 ...  0  0  1]\n",
      " [ 0  0 86 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 52  0  0]\n",
      " [ 0  0  0 ...  0 78  0]\n",
      " [ 0  0  1 ...  0  0 78]]\n",
      "at random state 65, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.90      0.82       106\n",
      "           1       0.58      0.63      0.61        93\n",
      "           2       0.92      0.88      0.90        98\n",
      "           3       0.87      0.81      0.84        97\n",
      "           4       0.82      0.90      0.86       108\n",
      "           5       0.80      0.71      0.75       107\n",
      "           6       0.95      0.93      0.94        75\n",
      "           7       0.79      0.85      0.82       103\n",
      "           8       0.88      0.93      0.91       107\n",
      "           9       0.77      0.79      0.78       108\n",
      "          10       0.63      0.65      0.64        95\n",
      "          11       0.83      0.91      0.86       106\n",
      "          12       0.88      0.86      0.87        95\n",
      "          13       0.68      0.68      0.68       101\n",
      "          14       0.86      0.95      0.90       102\n",
      "          15       0.93      0.94      0.94        88\n",
      "          16       0.90      0.96      0.93       114\n",
      "          17       0.91      0.76      0.83        91\n",
      "          18       0.83      0.81      0.82       111\n",
      "          19       0.82      0.82      0.82        98\n",
      "          20       0.91      0.80      0.85       108\n",
      "          21       0.65      0.69      0.67        95\n",
      "          22       0.97      0.93      0.95        99\n",
      "          23       0.74      0.85      0.79        86\n",
      "          24       0.61      0.61      0.61       111\n",
      "          25       0.83      0.97      0.89        96\n",
      "          26       0.67      0.79      0.72        96\n",
      "          27       0.80      0.82      0.81       110\n",
      "          28       0.95      0.97      0.96       100\n",
      "          29       0.97      0.94      0.95       110\n",
      "          30       0.85      0.85      0.85        97\n",
      "          31       0.54      0.63      0.58        92\n",
      "          32       0.90      0.77      0.83       109\n",
      "          33       0.74      0.82      0.78       104\n",
      "          34       0.92      0.84      0.88        98\n",
      "          35       0.84      0.92      0.88       106\n",
      "          36       0.76      0.66      0.71       104\n",
      "          37       0.73      0.78      0.75        89\n",
      "          38       0.73      0.68      0.70        96\n",
      "          39       0.79      0.73      0.76        95\n",
      "          40       0.83      0.84      0.83       104\n",
      "          41       0.70      0.75      0.73        93\n",
      "          42       0.85      0.88      0.86        72\n",
      "          43       0.86      0.70      0.77       110\n",
      "          44       0.77      0.81      0.79        98\n",
      "          45       0.95      0.94      0.94        79\n",
      "          46       0.97      0.85      0.90       100\n",
      "          47       0.80      0.73      0.77       101\n",
      "          48       0.83      0.67      0.74        90\n",
      "          49       0.79      0.78      0.79       102\n",
      "          50       0.56      0.50      0.53       105\n",
      "          51       1.00      1.00      1.00        52\n",
      "          52       0.97      0.96      0.97        81\n",
      "          53       0.84      0.74      0.79       105\n",
      "\n",
      "    accuracy                           0.81      5296\n",
      "   macro avg       0.82      0.81      0.81      5296\n",
      "weighted avg       0.81      0.81      0.81      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 66,  accuracy score is 0.8149546827794562\n",
      "at random state 66, confusion matrix is [[85  0  0 ...  0  0  0]\n",
      " [ 0 70  0 ...  0  0  2]\n",
      " [ 0  0 89 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 51  0  0]\n",
      " [ 0  0  0 ...  0 72  0]\n",
      " [ 0  0  1 ...  0  0 66]]\n",
      "at random state 66, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.91      0.87        93\n",
      "           1       0.60      0.69      0.65       101\n",
      "           2       0.84      0.92      0.88        97\n",
      "           3       0.86      0.82      0.84        88\n",
      "           4       0.82      0.93      0.87        89\n",
      "           5       0.77      0.73      0.75        84\n",
      "           6       0.94      0.94      0.94        86\n",
      "           7       0.78      0.88      0.83        91\n",
      "           8       0.91      0.93      0.92       102\n",
      "           9       0.62      0.79      0.70       101\n",
      "          10       0.76      0.71      0.74       118\n",
      "          11       0.87      0.90      0.88       127\n",
      "          12       0.93      0.92      0.92       111\n",
      "          13       0.66      0.63      0.64       106\n",
      "          14       0.86      0.93      0.89       100\n",
      "          15       0.93      0.93      0.93        81\n",
      "          16       0.91      0.96      0.93       101\n",
      "          17       0.92      0.85      0.89       100\n",
      "          18       0.83      0.88      0.85        86\n",
      "          19       0.84      0.75      0.79       108\n",
      "          20       0.94      0.86      0.90       111\n",
      "          21       0.71      0.75      0.73       110\n",
      "          22       0.98      0.91      0.94        98\n",
      "          23       0.83      0.80      0.81       120\n",
      "          24       0.59      0.52      0.55        99\n",
      "          25       0.84      0.94      0.88        98\n",
      "          26       0.70      0.74      0.72       114\n",
      "          27       0.86      0.88      0.87       109\n",
      "          28       0.97      0.88      0.92        96\n",
      "          29       0.95      0.96      0.95        92\n",
      "          30       0.94      0.87      0.91        93\n",
      "          31       0.56      0.56      0.56       108\n",
      "          32       0.89      0.80      0.84        95\n",
      "          33       0.75      0.77      0.76       102\n",
      "          34       0.92      0.90      0.91       101\n",
      "          35       0.94      0.87      0.90       107\n",
      "          36       0.76      0.76      0.76        91\n",
      "          37       0.67      0.80      0.73        82\n",
      "          38       0.88      0.74      0.80       108\n",
      "          39       0.75      0.83      0.79        96\n",
      "          40       0.79      0.86      0.82        98\n",
      "          41       0.75      0.74      0.75       103\n",
      "          42       0.85      0.79      0.82        78\n",
      "          43       0.76      0.70      0.73        91\n",
      "          44       0.81      0.87      0.84       106\n",
      "          45       0.96      0.90      0.93       105\n",
      "          46       0.86      0.93      0.90        87\n",
      "          47       0.80      0.79      0.79       100\n",
      "          48       0.87      0.65      0.74       112\n",
      "          49       0.77      0.87      0.82        92\n",
      "          50       0.41      0.40      0.41        97\n",
      "          51       1.00      1.00      1.00        51\n",
      "          52       0.99      0.95      0.97        76\n",
      "          53       0.80      0.66      0.72       100\n",
      "\n",
      "    accuracy                           0.81      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.81      0.81      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 67,  accuracy score is 0.8123111782477341\n",
      "at random state 67, confusion matrix is [[ 84   0   0 ...   0   0   0]\n",
      " [  0  69   0 ...   0   0   0]\n",
      " [  0   0 100 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  44   0   0]\n",
      " [  0   0   0 ...   0  83   0]\n",
      " [  0   0   2 ...   0   0  78]]\n",
      "at random state 67, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.84      0.84       100\n",
      "           1       0.63      0.64      0.64       108\n",
      "           2       0.88      0.96      0.92       104\n",
      "           3       0.89      0.80      0.84       116\n",
      "           4       0.87      0.95      0.91        99\n",
      "           5       0.66      0.72      0.69        95\n",
      "           6       0.98      0.94      0.96        87\n",
      "           7       0.73      0.80      0.76       102\n",
      "           8       0.92      0.92      0.92        97\n",
      "           9       0.77      0.80      0.78       106\n",
      "          10       0.63      0.78      0.70        87\n",
      "          11       0.88      0.93      0.90        98\n",
      "          12       0.89      0.91      0.90        95\n",
      "          13       0.70      0.75      0.73        92\n",
      "          14       0.87      0.92      0.90       102\n",
      "          15       0.96      0.93      0.94        73\n",
      "          16       0.89      0.89      0.89       104\n",
      "          17       0.96      0.81      0.88       100\n",
      "          18       0.89      0.83      0.86       103\n",
      "          19       0.78      0.82      0.80        99\n",
      "          20       0.87      0.86      0.86        91\n",
      "          21       0.52      0.76      0.62        88\n",
      "          22       0.89      0.92      0.90        92\n",
      "          23       0.80      0.85      0.82       106\n",
      "          24       0.66      0.63      0.64       109\n",
      "          25       0.84      0.94      0.89        95\n",
      "          26       0.80      0.76      0.78       119\n",
      "          27       0.85      0.79      0.81        98\n",
      "          28       0.96      0.88      0.92       103\n",
      "          29       0.93      0.87      0.90       108\n",
      "          30       0.96      0.83      0.89       103\n",
      "          31       0.60      0.70      0.65       104\n",
      "          32       0.86      0.86      0.86       104\n",
      "          33       0.72      0.84      0.77        92\n",
      "          34       0.90      0.87      0.88        89\n",
      "          35       0.85      0.94      0.89        98\n",
      "          36       0.66      0.71      0.68       109\n",
      "          37       0.71      0.75      0.73       102\n",
      "          38       0.76      0.72      0.74       105\n",
      "          39       0.78      0.82      0.80        88\n",
      "          40       0.73      0.80      0.76        92\n",
      "          41       0.75      0.62      0.68       110\n",
      "          42       0.92      0.84      0.88        93\n",
      "          43       0.80      0.68      0.74       101\n",
      "          44       0.80      0.85      0.83       101\n",
      "          45       0.90      0.96      0.93        85\n",
      "          46       0.92      0.89      0.90       100\n",
      "          47       0.85      0.69      0.76       106\n",
      "          48       0.88      0.72      0.79        98\n",
      "          49       0.81      0.82      0.82        95\n",
      "          50       0.54      0.42      0.48       102\n",
      "          51       0.98      1.00      0.99        44\n",
      "          52       1.00      0.97      0.98        86\n",
      "          53       0.94      0.69      0.80       113\n",
      "\n",
      "    accuracy                           0.81      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.81      0.81      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 68,  accuracy score is 0.8219410876132931\n",
      "at random state 68, confusion matrix is [[96  0  0 ...  0  0  0]\n",
      " [ 0 75  0 ...  0  0  1]\n",
      " [ 0  0 95 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 55  0  0]\n",
      " [ 0  0  0 ...  0 75  0]\n",
      " [ 0  0  0 ...  0  0 81]]\n",
      "at random state 68, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.94      0.89       102\n",
      "           1       0.68      0.75      0.71       100\n",
      "           2       0.90      0.97      0.93        98\n",
      "           3       0.91      0.80      0.85       108\n",
      "           4       0.87      0.91      0.89       106\n",
      "           5       0.78      0.70      0.74       113\n",
      "           6       0.92      0.95      0.94        88\n",
      "           7       0.69      0.92      0.79        83\n",
      "           8       0.93      0.93      0.93        85\n",
      "           9       0.74      0.77      0.75       115\n",
      "          10       0.69      0.74      0.71        97\n",
      "          11       0.80      0.92      0.86       105\n",
      "          12       0.90      0.83      0.87       108\n",
      "          13       0.65      0.65      0.65       101\n",
      "          14       0.93      0.93      0.93       106\n",
      "          15       0.94      0.95      0.95        86\n",
      "          16       0.87      0.86      0.87        96\n",
      "          17       0.88      0.87      0.88        77\n",
      "          18       0.87      0.80      0.84       128\n",
      "          19       0.82      0.89      0.85       100\n",
      "          20       0.83      0.79      0.81        89\n",
      "          21       0.64      0.73      0.68        93\n",
      "          22       0.97      0.97      0.97        86\n",
      "          23       0.78      0.82      0.80        98\n",
      "          24       0.58      0.65      0.61        95\n",
      "          25       0.92      0.95      0.94       103\n",
      "          26       0.77      0.76      0.77        99\n",
      "          27       0.75      0.81      0.78       101\n",
      "          28       0.96      0.97      0.96        96\n",
      "          29       0.93      0.96      0.95        90\n",
      "          30       0.89      0.82      0.85        99\n",
      "          31       0.55      0.56      0.55       115\n",
      "          32       0.86      0.80      0.83        87\n",
      "          33       0.75      0.76      0.76       102\n",
      "          34       0.92      0.90      0.91       112\n",
      "          35       0.92      0.97      0.95        98\n",
      "          36       0.67      0.69      0.68        98\n",
      "          37       0.87      0.68      0.76       107\n",
      "          38       0.79      0.78      0.79        93\n",
      "          39       0.82      0.82      0.82       114\n",
      "          40       0.82      0.84      0.83       111\n",
      "          41       0.76      0.76      0.76        97\n",
      "          42       0.93      0.85      0.89        87\n",
      "          43       0.88      0.74      0.80       105\n",
      "          44       0.88      0.85      0.87        99\n",
      "          45       0.95      0.92      0.93        93\n",
      "          46       0.92      0.91      0.91        95\n",
      "          47       0.85      0.81      0.83       103\n",
      "          48       0.82      0.63      0.71        99\n",
      "          49       0.77      0.83      0.80        98\n",
      "          50       0.53      0.45      0.49        99\n",
      "          51       1.00      1.00      1.00        55\n",
      "          52       0.97      0.95      0.96        79\n",
      "          53       0.84      0.82      0.83        99\n",
      "\n",
      "    accuracy                           0.82      5296\n",
      "   macro avg       0.83      0.83      0.83      5296\n",
      "weighted avg       0.82      0.82      0.82      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 69,  accuracy score is 0.81797583081571\n",
      "at random state 69, confusion matrix is [[107   0   0 ...   0   0   0]\n",
      " [  0  80   0 ...   0   0   2]\n",
      " [  0   0  92 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  53   0   0]\n",
      " [  0   0   0 ...   1  75   0]\n",
      " [  0   1   1 ...   0   0  80]]\n",
      "at random state 69, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.92      0.85       116\n",
      "           1       0.69      0.67      0.68       119\n",
      "           2       0.88      0.95      0.91        97\n",
      "           3       0.82      0.82      0.82        97\n",
      "           4       0.85      0.90      0.87       100\n",
      "           5       0.83      0.77      0.80        88\n",
      "           6       0.93      0.98      0.96        87\n",
      "           7       0.73      0.88      0.80        91\n",
      "           8       0.95      0.90      0.92       115\n",
      "           9       0.80      0.83      0.82       103\n",
      "          10       0.63      0.73      0.67        88\n",
      "          11       0.83      0.89      0.86       108\n",
      "          12       0.95      0.92      0.93        99\n",
      "          13       0.68      0.73      0.71        94\n",
      "          14       0.89      0.87      0.88       106\n",
      "          15       0.92      0.93      0.92        85\n",
      "          16       0.94      0.89      0.91       116\n",
      "          17       0.93      0.86      0.89       113\n",
      "          18       0.86      0.86      0.86        98\n",
      "          19       0.71      0.76      0.73        86\n",
      "          20       0.93      0.81      0.87        97\n",
      "          21       0.61      0.69      0.65        98\n",
      "          22       0.94      0.95      0.94       113\n",
      "          23       0.85      0.83      0.84       102\n",
      "          24       0.59      0.57      0.58       101\n",
      "          25       0.89      0.96      0.93        85\n",
      "          26       0.79      0.76      0.78       102\n",
      "          27       0.85      0.79      0.82       106\n",
      "          28       0.98      0.86      0.92       102\n",
      "          29       0.93      0.97      0.95        86\n",
      "          30       0.94      0.83      0.88       109\n",
      "          31       0.58      0.63      0.60       105\n",
      "          32       0.85      0.82      0.83        94\n",
      "          33       0.73      0.75      0.74       101\n",
      "          34       0.85      0.86      0.85       115\n",
      "          35       0.89      0.93      0.91       100\n",
      "          36       0.69      0.76      0.72        99\n",
      "          37       0.76      0.74      0.75       100\n",
      "          38       0.71      0.73      0.72        84\n",
      "          39       0.84      0.86      0.85        96\n",
      "          40       0.72      0.88      0.79        88\n",
      "          41       0.74      0.61      0.67       110\n",
      "          42       0.89      0.77      0.82        91\n",
      "          43       0.81      0.72      0.76        99\n",
      "          44       0.85      0.85      0.85        86\n",
      "          45       0.98      0.93      0.95        89\n",
      "          46       0.93      0.95      0.94        93\n",
      "          47       0.78      0.75      0.76       101\n",
      "          48       0.79      0.70      0.74       102\n",
      "          49       0.87      0.83      0.85       103\n",
      "          50       0.51      0.49      0.50        90\n",
      "          51       0.98      1.00      0.99        53\n",
      "          52       0.97      0.91      0.94        82\n",
      "          53       0.83      0.74      0.78       108\n",
      "\n",
      "    accuracy                           0.82      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.82      0.82      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 70,  accuracy score is 0.8081570996978852\n",
      "at random state 70, confusion matrix is [[94  0  0 ...  0  0  0]\n",
      " [ 0 71  0 ...  0  0  0]\n",
      " [ 0  0 93 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 46  0  0]\n",
      " [ 0  0  0 ...  0 68  0]\n",
      " [ 0  1  2 ...  0  0 65]]\n",
      "at random state 70, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.90      0.86       104\n",
      "           1       0.62      0.70      0.66       101\n",
      "           2       0.86      0.97      0.91        96\n",
      "           3       0.80      0.82      0.81        96\n",
      "           4       0.84      0.94      0.89       102\n",
      "           5       0.82      0.68      0.74       111\n",
      "           6       0.92      0.89      0.91        82\n",
      "           7       0.74      0.93      0.83        92\n",
      "           8       0.95      0.92      0.93       109\n",
      "           9       0.73      0.87      0.79       104\n",
      "          10       0.69      0.73      0.71       119\n",
      "          11       0.86      0.91      0.88        96\n",
      "          12       0.89      0.83      0.86       110\n",
      "          13       0.74      0.66      0.70       111\n",
      "          14       0.92      0.87      0.89       111\n",
      "          15       0.92      0.92      0.92        79\n",
      "          16       0.94      0.91      0.92       112\n",
      "          17       0.91      0.83      0.87        99\n",
      "          18       0.88      0.88      0.88        90\n",
      "          19       0.74      0.80      0.77       102\n",
      "          20       0.91      0.81      0.86       106\n",
      "          21       0.66      0.72      0.69        98\n",
      "          22       0.95      0.90      0.92        99\n",
      "          23       0.74      0.78      0.76        93\n",
      "          24       0.61      0.62      0.61       104\n",
      "          25       0.86      0.97      0.91        86\n",
      "          26       0.72      0.73      0.72       104\n",
      "          27       0.86      0.86      0.86       107\n",
      "          28       0.97      0.92      0.94        92\n",
      "          29       0.98      0.96      0.97        97\n",
      "          30       0.93      0.85      0.88       104\n",
      "          31       0.45      0.54      0.49        93\n",
      "          32       0.84      0.86      0.85        94\n",
      "          33       0.74      0.78      0.76        94\n",
      "          34       0.88      0.87      0.88       101\n",
      "          35       0.84      0.94      0.89       107\n",
      "          36       0.71      0.77      0.74        99\n",
      "          37       0.80      0.69      0.74       101\n",
      "          38       0.74      0.73      0.74        94\n",
      "          39       0.77      0.81      0.79        94\n",
      "          40       0.83      0.83      0.83       102\n",
      "          41       0.75      0.69      0.72       118\n",
      "          42       0.81      0.86      0.83        79\n",
      "          43       0.88      0.72      0.79       101\n",
      "          44       0.86      0.83      0.85        96\n",
      "          45       0.89      0.92      0.90        83\n",
      "          46       0.91      0.81      0.86        90\n",
      "          47       0.80      0.75      0.78       106\n",
      "          48       0.83      0.62      0.71       100\n",
      "          49       0.75      0.83      0.79       102\n",
      "          50       0.48      0.40      0.43       111\n",
      "          51       1.00      1.00      1.00        46\n",
      "          52       0.92      0.89      0.91        76\n",
      "          53       0.81      0.70      0.75        93\n",
      "\n",
      "    accuracy                           0.81      5296\n",
      "   macro avg       0.82      0.81      0.81      5296\n",
      "weighted avg       0.81      0.81      0.81      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 71,  accuracy score is 0.8168429003021148\n",
      "at random state 71, confusion matrix is [[100   0   0 ...   0   0   0]\n",
      " [  0  69   0 ...   0   0   1]\n",
      " [  0   0  94 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  57   0   0]\n",
      " [  0   0   0 ...   0  73   0]\n",
      " [  0   0   0 ...   0   0  80]]\n",
      "at random state 71, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.83      0.81       120\n",
      "           1       0.64      0.70      0.67        99\n",
      "           2       0.88      0.91      0.90       103\n",
      "           3       0.81      0.75      0.78       102\n",
      "           4       0.87      0.92      0.89       107\n",
      "           5       0.73      0.83      0.78       103\n",
      "           6       0.96      0.94      0.95        87\n",
      "           7       0.75      0.84      0.79       102\n",
      "           8       0.93      0.94      0.94       104\n",
      "           9       0.79      0.85      0.82       118\n",
      "          10       0.61      0.72      0.66        88\n",
      "          11       0.82      0.90      0.86       112\n",
      "          12       0.89      0.84      0.86        91\n",
      "          13       0.74      0.72      0.73       103\n",
      "          14       0.88      0.91      0.90       109\n",
      "          15       0.87      0.97      0.92        70\n",
      "          16       0.93      0.94      0.94       103\n",
      "          17       0.97      0.79      0.88        97\n",
      "          18       0.82      0.82      0.82       108\n",
      "          19       0.80      0.80      0.80       102\n",
      "          20       0.92      0.88      0.90       104\n",
      "          21       0.65      0.74      0.69       105\n",
      "          22       0.97      0.94      0.95       101\n",
      "          23       0.76      0.81      0.78       101\n",
      "          24       0.56      0.51      0.53        97\n",
      "          25       0.90      0.94      0.92       108\n",
      "          26       0.74      0.80      0.77        92\n",
      "          27       0.82      0.81      0.81       103\n",
      "          28       0.98      0.93      0.95       110\n",
      "          29       0.95      0.97      0.96       101\n",
      "          30       0.90      0.89      0.89       100\n",
      "          31       0.62      0.67      0.64        99\n",
      "          32       0.83      0.84      0.84        89\n",
      "          33       0.65      0.67      0.66       104\n",
      "          34       0.91      0.85      0.88       101\n",
      "          35       0.89      0.94      0.91        96\n",
      "          36       0.76      0.71      0.74        90\n",
      "          37       0.76      0.72      0.74        90\n",
      "          38       0.80      0.81      0.80        88\n",
      "          39       0.80      0.89      0.84        79\n",
      "          40       0.79      0.82      0.80        99\n",
      "          41       0.74      0.63      0.68        95\n",
      "          42       0.93      0.84      0.88        95\n",
      "          43       0.87      0.79      0.83        99\n",
      "          44       0.81      0.78      0.80       100\n",
      "          45       0.90      0.94      0.92        89\n",
      "          46       0.95      0.87      0.91        93\n",
      "          47       0.87      0.69      0.77       118\n",
      "          48       0.86      0.72      0.78        92\n",
      "          49       0.75      0.73      0.74       102\n",
      "          50       0.42      0.45      0.44        86\n",
      "          51       1.00      1.00      1.00        57\n",
      "          52       0.97      0.96      0.97        76\n",
      "          53       0.83      0.73      0.78       109\n",
      "\n",
      "    accuracy                           0.82      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.82      0.82      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 72,  accuracy score is 0.8191087613293051\n",
      "at random state 72, confusion matrix is [[85  0  0 ...  0  0  0]\n",
      " [ 0 82  0 ...  0  0  0]\n",
      " [ 0  0 89 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 51  0  0]\n",
      " [ 0  0  0 ...  1 73  0]\n",
      " [ 0  1  1 ...  0  0 73]]\n",
      "at random state 72, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.88      0.84        97\n",
      "           1       0.67      0.79      0.73       104\n",
      "           2       0.86      0.97      0.91        92\n",
      "           3       0.83      0.82      0.82       111\n",
      "           4       0.88      0.89      0.88       105\n",
      "           5       0.83      0.76      0.80       105\n",
      "           6       0.88      0.99      0.93        74\n",
      "           7       0.84      0.77      0.81       115\n",
      "           8       0.98      0.89      0.93        99\n",
      "           9       0.68      0.83      0.75        99\n",
      "          10       0.67      0.73      0.70       106\n",
      "          11       0.82      0.96      0.88       100\n",
      "          12       0.87      0.88      0.87       112\n",
      "          13       0.69      0.64      0.66       100\n",
      "          14       0.94      0.89      0.91       107\n",
      "          15       0.95      0.94      0.94        78\n",
      "          16       0.92      0.93      0.93       106\n",
      "          17       0.94      0.85      0.89        88\n",
      "          18       0.86      0.82      0.84       108\n",
      "          19       0.78      0.88      0.83       111\n",
      "          20       0.85      0.79      0.82       104\n",
      "          21       0.72      0.73      0.73       107\n",
      "          22       0.96      0.95      0.95        91\n",
      "          23       0.77      0.72      0.75       104\n",
      "          24       0.65      0.62      0.64       100\n",
      "          25       0.89      0.94      0.91        83\n",
      "          26       0.77      0.78      0.78       102\n",
      "          27       0.85      0.87      0.86        95\n",
      "          28       0.98      0.93      0.95        96\n",
      "          29       0.96      0.94      0.95       103\n",
      "          30       0.88      0.82      0.85        97\n",
      "          31       0.49      0.63      0.55        84\n",
      "          32       0.88      0.89      0.88       102\n",
      "          33       0.83      0.77      0.80       123\n",
      "          34       0.90      0.89      0.89       100\n",
      "          35       0.86      0.96      0.91       104\n",
      "          36       0.72      0.66      0.69        95\n",
      "          37       0.65      0.80      0.72        96\n",
      "          38       0.73      0.72      0.72        95\n",
      "          39       0.75      0.82      0.79        94\n",
      "          40       0.78      0.87      0.82        94\n",
      "          41       0.68      0.70      0.69        88\n",
      "          42       0.93      0.85      0.89        89\n",
      "          43       0.83      0.70      0.76       106\n",
      "          44       0.82      0.78      0.80       108\n",
      "          45       0.93      0.94      0.94        89\n",
      "          46       0.97      0.88      0.92       108\n",
      "          47       0.78      0.77      0.78       102\n",
      "          48       0.84      0.63      0.72        86\n",
      "          49       0.85      0.87      0.86       106\n",
      "          50       0.54      0.43      0.48        92\n",
      "          51       0.98      1.00      0.99        51\n",
      "          52       0.99      0.91      0.95        80\n",
      "          53       0.81      0.70      0.75       105\n",
      "\n",
      "    accuracy                           0.82      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.82      0.82      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 73,  accuracy score is 0.8158987915407855\n",
      "at random state 73, confusion matrix is [[96  0  0 ...  0  0  0]\n",
      " [ 0 82  0 ...  0  0  2]\n",
      " [ 0  0 81 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 46  0  0]\n",
      " [ 0  0  0 ...  0 66  0]\n",
      " [ 0  0  0 ...  0  0 65]]\n",
      "at random state 73, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.88      0.86       109\n",
      "           1       0.70      0.70      0.70       117\n",
      "           2       0.84      0.91      0.87        89\n",
      "           3       0.85      0.81      0.83        97\n",
      "           4       0.82      0.85      0.84       111\n",
      "           5       0.75      0.77      0.76        99\n",
      "           6       0.95      0.97      0.96        96\n",
      "           7       0.81      0.84      0.82       103\n",
      "           8       0.94      0.94      0.94       103\n",
      "           9       0.68      0.77      0.72        99\n",
      "          10       0.69      0.72      0.71       116\n",
      "          11       0.84      0.90      0.87        97\n",
      "          12       0.90      0.87      0.88       108\n",
      "          13       0.67      0.62      0.64       104\n",
      "          14       0.94      0.93      0.93        99\n",
      "          15       0.97      0.96      0.96        89\n",
      "          16       0.88      0.94      0.91        98\n",
      "          17       0.88      0.85      0.86       104\n",
      "          18       0.84      0.80      0.82       104\n",
      "          19       0.74      0.79      0.77        97\n",
      "          20       0.92      0.87      0.89        97\n",
      "          21       0.61      0.71      0.65       101\n",
      "          22       0.96      0.93      0.95        92\n",
      "          23       0.81      0.82      0.81       101\n",
      "          24       0.61      0.51      0.56       105\n",
      "          25       0.87      0.99      0.93        97\n",
      "          26       0.80      0.78      0.79       110\n",
      "          27       0.81      0.76      0.79        97\n",
      "          28       0.99      0.87      0.92        89\n",
      "          29       0.93      0.90      0.91       100\n",
      "          30       0.92      0.77      0.84       103\n",
      "          31       0.57      0.65      0.61       100\n",
      "          32       0.85      0.83      0.84       102\n",
      "          33       0.74      0.76      0.75       114\n",
      "          34       0.91      0.89      0.90       111\n",
      "          35       0.90      0.94      0.92        84\n",
      "          36       0.70      0.67      0.68       106\n",
      "          37       0.75      0.81      0.78        88\n",
      "          38       0.76      0.79      0.78       102\n",
      "          39       0.78      0.91      0.84        91\n",
      "          40       0.81      0.82      0.81       114\n",
      "          41       0.84      0.71      0.77       109\n",
      "          42       0.82      0.86      0.84        87\n",
      "          43       0.86      0.79      0.82       100\n",
      "          44       0.76      0.85      0.80        97\n",
      "          45       0.98      0.97      0.97        86\n",
      "          46       0.93      0.94      0.94        70\n",
      "          47       0.82      0.75      0.78        99\n",
      "          48       0.82      0.76      0.79        87\n",
      "          49       0.85      0.82      0.83       113\n",
      "          50       0.52      0.52      0.52        91\n",
      "          51       1.00      1.00      1.00        46\n",
      "          52       1.00      0.92      0.96        72\n",
      "          53       0.81      0.68      0.74        96\n",
      "\n",
      "    accuracy                           0.82      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.82      0.82      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 74,  accuracy score is 0.8062688821752266\n",
      "at random state 74, confusion matrix is [[77  0  0 ...  0  0  0]\n",
      " [ 0 78  0 ...  0  0  1]\n",
      " [ 0  0 95 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 47  0  0]\n",
      " [ 0  0  0 ...  0 74  0]\n",
      " [ 0  1  0 ...  0  0 70]]\n",
      "at random state 74, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.96      0.84        80\n",
      "           1       0.67      0.63      0.65       123\n",
      "           2       0.85      0.96      0.90        99\n",
      "           3       0.90      0.79      0.84       105\n",
      "           4       0.85      0.90      0.87       105\n",
      "           5       0.75      0.70      0.72       107\n",
      "           6       0.93      0.96      0.95        81\n",
      "           7       0.81      0.82      0.81       104\n",
      "           8       0.93      0.91      0.92        98\n",
      "           9       0.70      0.77      0.74       101\n",
      "          10       0.60      0.78      0.68        89\n",
      "          11       0.79      0.91      0.85       106\n",
      "          12       0.91      0.87      0.89        94\n",
      "          13       0.64      0.72      0.68        97\n",
      "          14       0.88      0.85      0.86        91\n",
      "          15       0.96      0.95      0.96        79\n",
      "          16       0.84      0.88      0.86        91\n",
      "          17       0.91      0.78      0.84       121\n",
      "          18       0.84      0.81      0.83        97\n",
      "          19       0.81      0.83      0.82       106\n",
      "          20       0.86      0.87      0.86       120\n",
      "          21       0.58      0.72      0.64       104\n",
      "          22       0.99      0.94      0.97        89\n",
      "          23       0.77      0.77      0.77        93\n",
      "          24       0.62      0.56      0.59        99\n",
      "          25       0.88      0.99      0.93        99\n",
      "          26       0.71      0.71      0.71       110\n",
      "          27       0.85      0.81      0.83       100\n",
      "          28       0.95      0.94      0.94        96\n",
      "          29       0.98      0.94      0.96        94\n",
      "          30       0.93      0.89      0.91       117\n",
      "          31       0.58      0.62      0.60       101\n",
      "          32       0.85      0.88      0.86        96\n",
      "          33       0.76      0.75      0.75       107\n",
      "          34       0.88      0.88      0.88       109\n",
      "          35       0.91      0.92      0.91       107\n",
      "          36       0.74      0.69      0.71       107\n",
      "          37       0.73      0.76      0.75        91\n",
      "          38       0.77      0.77      0.77        94\n",
      "          39       0.79      0.75      0.77       103\n",
      "          40       0.76      0.82      0.79        95\n",
      "          41       0.68      0.70      0.69        82\n",
      "          42       0.84      0.84      0.84        93\n",
      "          43       0.87      0.70      0.77       120\n",
      "          44       0.79      0.82      0.80       102\n",
      "          45       0.90      0.97      0.93        80\n",
      "          46       0.91      0.88      0.90        93\n",
      "          47       0.76      0.78      0.77        98\n",
      "          48       0.84      0.62      0.72        93\n",
      "          49       0.82      0.77      0.79       108\n",
      "          50       0.44      0.42      0.43        89\n",
      "          51       1.00      1.00      1.00        47\n",
      "          52       0.99      0.88      0.93        84\n",
      "          53       0.86      0.69      0.77       102\n",
      "\n",
      "    accuracy                           0.81      5296\n",
      "   macro avg       0.81      0.81      0.81      5296\n",
      "weighted avg       0.81      0.81      0.81      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 75,  accuracy score is 0.815143504531722\n",
      "at random state 75, confusion matrix is [[ 91   0   0 ...   0   0   0]\n",
      " [  0  72   0 ...   0   0   2]\n",
      " [  0   0 109 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  48   0   0]\n",
      " [  0   0   0 ...   0  52   0]\n",
      " [  0   1   3 ...   0   0  70]]\n",
      "at random state 75, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.91      0.82       100\n",
      "           1       0.69      0.72      0.71       100\n",
      "           2       0.87      0.94      0.90       116\n",
      "           3       0.88      0.73      0.80        92\n",
      "           4       0.87      0.93      0.90       101\n",
      "           5       0.71      0.69      0.70       104\n",
      "           6       0.96      0.96      0.96        80\n",
      "           7       0.76      0.85      0.80       114\n",
      "           8       0.95      0.91      0.93        98\n",
      "           9       0.66      0.78      0.72       101\n",
      "          10       0.66      0.77      0.71       112\n",
      "          11       0.85      0.91      0.88       115\n",
      "          12       0.91      0.87      0.89       105\n",
      "          13       0.70      0.70      0.70       103\n",
      "          14       0.93      0.90      0.92       108\n",
      "          15       0.97      0.87      0.92        78\n",
      "          16       0.87      0.89      0.88       110\n",
      "          17       0.94      0.85      0.89       111\n",
      "          18       0.84      0.80      0.82        92\n",
      "          19       0.88      0.81      0.84       105\n",
      "          20       0.89      0.89      0.89        99\n",
      "          21       0.60      0.78      0.68        92\n",
      "          22       0.94      0.97      0.95        94\n",
      "          23       0.79      0.84      0.81       109\n",
      "          24       0.60      0.64      0.62        91\n",
      "          25       0.79      0.99      0.88        94\n",
      "          26       0.72      0.76      0.74        94\n",
      "          27       0.80      0.78      0.79        85\n",
      "          28       0.99      0.90      0.94        99\n",
      "          29       0.97      0.92      0.94       111\n",
      "          30       0.91      0.86      0.88       113\n",
      "          31       0.57      0.55      0.56        99\n",
      "          32       0.87      0.75      0.81       102\n",
      "          33       0.82      0.71      0.76        94\n",
      "          34       0.88      0.89      0.88        89\n",
      "          35       0.93      0.93      0.93       106\n",
      "          36       0.65      0.64      0.64        96\n",
      "          37       0.79      0.77      0.78       105\n",
      "          38       0.74      0.80      0.77       106\n",
      "          39       0.81      0.81      0.81       108\n",
      "          40       0.80      0.84      0.82       103\n",
      "          41       0.73      0.69      0.71        87\n",
      "          42       0.88      0.85      0.86        98\n",
      "          43       0.75      0.72      0.74        94\n",
      "          44       0.88      0.83      0.85        94\n",
      "          45       0.97      0.91      0.93        95\n",
      "          46       0.93      0.97      0.95       104\n",
      "          47       0.88      0.83      0.85        99\n",
      "          48       0.80      0.66      0.72        87\n",
      "          49       0.84      0.77      0.80       113\n",
      "          50       0.42      0.39      0.40        83\n",
      "          51       1.00      1.00      1.00        48\n",
      "          52       0.93      0.95      0.94        55\n",
      "          53       0.80      0.67      0.73       105\n",
      "\n",
      "    accuracy                           0.82      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.82      0.82      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 76,  accuracy score is 0.8138217522658611\n",
      "at random state 76, confusion matrix is [[103   0   0 ...   0   0   0]\n",
      " [  0  78   0 ...   0   0   1]\n",
      " [  0   0  85 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  52   0   0]\n",
      " [  0   0   0 ...   0  68   0]\n",
      " [  0   0   1 ...   0   0  73]]\n",
      "at random state 76, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.87      0.82       118\n",
      "           1       0.74      0.80      0.77        98\n",
      "           2       0.79      0.93      0.86        91\n",
      "           3       0.88      0.74      0.80       106\n",
      "           4       0.87      0.91      0.89       111\n",
      "           5       0.72      0.71      0.72        89\n",
      "           6       0.89      0.97      0.93        91\n",
      "           7       0.78      0.84      0.81       111\n",
      "           8       0.93      0.90      0.91        99\n",
      "           9       0.62      0.79      0.69        96\n",
      "          10       0.63      0.78      0.70        94\n",
      "          11       0.93      0.93      0.93       111\n",
      "          12       0.92      0.88      0.90       105\n",
      "          13       0.76      0.73      0.75        97\n",
      "          14       0.84      0.94      0.89        97\n",
      "          15       0.92      0.92      0.92        83\n",
      "          16       0.90      0.92      0.91        90\n",
      "          17       0.91      0.83      0.87        84\n",
      "          18       0.86      0.88      0.87       103\n",
      "          19       0.79      0.74      0.76       110\n",
      "          20       0.93      0.79      0.85       122\n",
      "          21       0.64      0.75      0.69        93\n",
      "          22       0.99      0.87      0.92       113\n",
      "          23       0.79      0.79      0.79       103\n",
      "          24       0.57      0.53      0.55       106\n",
      "          25       0.90      0.96      0.93       106\n",
      "          26       0.73      0.73      0.73       102\n",
      "          27       0.86      0.79      0.82       114\n",
      "          28       0.99      0.88      0.93       115\n",
      "          29       0.96      0.95      0.95        94\n",
      "          30       0.92      0.85      0.88        84\n",
      "          31       0.50      0.61      0.55        97\n",
      "          32       0.90      0.85      0.88       100\n",
      "          33       0.73      0.80      0.76       100\n",
      "          34       0.90      0.88      0.89       109\n",
      "          35       0.88      0.97      0.92        95\n",
      "          36       0.77      0.68      0.72        90\n",
      "          37       0.72      0.76      0.74        96\n",
      "          38       0.77      0.77      0.77        95\n",
      "          39       0.76      0.75      0.76       105\n",
      "          40       0.77      0.81      0.79       100\n",
      "          41       0.77      0.64      0.70       106\n",
      "          42       0.91      0.88      0.90        92\n",
      "          43       0.85      0.76      0.80       105\n",
      "          44       0.84      0.87      0.85        93\n",
      "          45       0.87      0.97      0.92        80\n",
      "          46       0.91      0.92      0.91        98\n",
      "          47       0.76      0.77      0.76        95\n",
      "          48       0.84      0.65      0.73        99\n",
      "          49       0.75      0.82      0.78        84\n",
      "          50       0.52      0.43      0.47        96\n",
      "          51       1.00      1.00      1.00        52\n",
      "          52       1.00      0.88      0.94        77\n",
      "          53       0.79      0.76      0.78        96\n",
      "\n",
      "    accuracy                           0.81      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.81      0.81      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 77,  accuracy score is 0.8123111782477341\n",
      "at random state 77, confusion matrix is [[ 91   0   0 ...   0   0   0]\n",
      " [  0  70   0 ...   0   0   0]\n",
      " [  0   0 103 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  57   0   0]\n",
      " [  0   0   0 ...   0  73   0]\n",
      " [  0   0   2 ...   0   0  67]]\n",
      "at random state 77, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.88      0.83       103\n",
      "           1       0.55      0.74      0.63        94\n",
      "           2       0.91      0.98      0.94       105\n",
      "           3       0.85      0.80      0.82       111\n",
      "           4       0.85      0.95      0.90        93\n",
      "           5       0.73      0.66      0.70       104\n",
      "           6       0.92      0.94      0.93        89\n",
      "           7       0.72      0.84      0.78       100\n",
      "           8       0.89      0.95      0.92        96\n",
      "           9       0.77      0.74      0.76       119\n",
      "          10       0.64      0.66      0.65       110\n",
      "          11       0.84      0.91      0.88       108\n",
      "          12       0.91      0.83      0.87       103\n",
      "          13       0.65      0.63      0.64       101\n",
      "          14       0.90      0.90      0.90        90\n",
      "          15       0.92      0.92      0.92        93\n",
      "          16       0.94      0.90      0.92       104\n",
      "          17       0.94      0.80      0.87       102\n",
      "          18       0.82      0.87      0.84        91\n",
      "          19       0.81      0.82      0.82       108\n",
      "          20       0.91      0.87      0.89       110\n",
      "          21       0.65      0.76      0.70       104\n",
      "          22       0.95      0.95      0.95       101\n",
      "          23       0.75      0.84      0.79        99\n",
      "          24       0.57      0.50      0.54       103\n",
      "          25       0.93      0.97      0.95       101\n",
      "          26       0.67      0.80      0.73       105\n",
      "          27       0.75      0.84      0.79        92\n",
      "          28       0.97      0.94      0.96        83\n",
      "          29       0.96      0.95      0.95        98\n",
      "          30       0.93      0.85      0.89        82\n",
      "          31       0.58      0.58      0.58        98\n",
      "          32       0.90      0.80      0.85       102\n",
      "          33       0.74      0.77      0.76       109\n",
      "          34       0.91      0.85      0.88       103\n",
      "          35       0.91      0.98      0.94       104\n",
      "          36       0.72      0.72      0.72       110\n",
      "          37       0.71      0.75      0.73       100\n",
      "          38       0.76      0.78      0.77        87\n",
      "          39       0.80      0.81      0.80        99\n",
      "          40       0.78      0.80      0.79        97\n",
      "          41       0.70      0.62      0.66       106\n",
      "          42       0.89      0.85      0.87        84\n",
      "          43       0.79      0.70      0.74        93\n",
      "          44       0.91      0.79      0.84       100\n",
      "          45       0.96      0.95      0.96        80\n",
      "          46       0.94      0.95      0.95        86\n",
      "          47       0.89      0.74      0.81       129\n",
      "          48       0.83      0.63      0.72        95\n",
      "          49       0.75      0.81      0.78        97\n",
      "          50       0.54      0.47      0.50        94\n",
      "          51       1.00      1.00      1.00        57\n",
      "          52       0.97      0.92      0.95        79\n",
      "          53       0.82      0.79      0.80        85\n",
      "\n",
      "    accuracy                           0.81      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.81      0.81      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 78,  accuracy score is 0.8119335347432024\n",
      "at random state 78, confusion matrix is [[94  0  0 ...  0  0  0]\n",
      " [ 0 71  0 ...  0  0  1]\n",
      " [ 0  0 91 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 55  0  0]\n",
      " [ 0  0  0 ...  0 63  0]\n",
      " [ 0  0  2 ...  0  0 64]]\n",
      "at random state 78, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.86      0.82       109\n",
      "           1       0.66      0.70      0.68       102\n",
      "           2       0.78      0.90      0.84       101\n",
      "           3       0.84      0.84      0.84       106\n",
      "           4       0.83      0.94      0.88        85\n",
      "           5       0.71      0.72      0.72        90\n",
      "           6       0.93      0.94      0.93        95\n",
      "           7       0.71      0.84      0.77        95\n",
      "           8       0.92      0.89      0.90       122\n",
      "           9       0.77      0.77      0.77       104\n",
      "          10       0.76      0.76      0.76        96\n",
      "          11       0.77      0.93      0.84       107\n",
      "          12       0.96      0.91      0.93       109\n",
      "          13       0.64      0.71      0.67        99\n",
      "          14       0.87      0.92      0.89        84\n",
      "          15       0.89      0.88      0.88        82\n",
      "          16       0.93      0.90      0.92       110\n",
      "          17       0.93      0.83      0.88        90\n",
      "          18       0.87      0.80      0.83       108\n",
      "          19       0.73      0.83      0.77        89\n",
      "          20       0.94      0.90      0.92        97\n",
      "          21       0.59      0.72      0.65        94\n",
      "          22       0.98      0.92      0.95       101\n",
      "          23       0.82      0.78      0.80       109\n",
      "          24       0.61      0.55      0.58       118\n",
      "          25       0.90      0.93      0.91        98\n",
      "          26       0.82      0.81      0.82        99\n",
      "          27       0.86      0.78      0.82       112\n",
      "          28       0.95      0.91      0.93       105\n",
      "          29       0.96      0.98      0.97        82\n",
      "          30       0.95      0.85      0.90       106\n",
      "          31       0.59      0.56      0.58       105\n",
      "          32       0.93      0.79      0.85        96\n",
      "          33       0.74      0.82      0.78        94\n",
      "          34       0.88      0.85      0.86        99\n",
      "          35       0.89      0.94      0.92        89\n",
      "          36       0.64      0.74      0.69        82\n",
      "          37       0.78      0.73      0.75       113\n",
      "          38       0.79      0.71      0.75       109\n",
      "          39       0.83      0.83      0.83       108\n",
      "          40       0.75      0.83      0.79       104\n",
      "          41       0.77      0.71      0.74       101\n",
      "          42       0.87      0.83      0.85        96\n",
      "          43       0.92      0.68      0.78       105\n",
      "          44       0.77      0.89      0.83        91\n",
      "          45       0.92      0.93      0.92        98\n",
      "          46       0.96      0.92      0.94        96\n",
      "          47       0.85      0.79      0.82       117\n",
      "          48       0.74      0.70      0.72        98\n",
      "          49       0.71      0.75      0.73        77\n",
      "          50       0.46      0.47      0.47        99\n",
      "          51       1.00      1.00      1.00        55\n",
      "          52       0.93      0.97      0.95        65\n",
      "          53       0.80      0.67      0.73        95\n",
      "\n",
      "    accuracy                           0.81      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.81      0.81      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 79,  accuracy score is 0.8119335347432024\n",
      "at random state 79, confusion matrix is [[105   0   0 ...   0   0   0]\n",
      " [  0  62   0 ...   0   0   1]\n",
      " [  0   0 110 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  52   0   0]\n",
      " [  0   0   0 ...   0  75   0]\n",
      " [  0   0   0 ...   0   0  75]]\n",
      "at random state 79, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.95      0.86       111\n",
      "           1       0.57      0.77      0.65        81\n",
      "           2       0.89      0.92      0.91       119\n",
      "           3       0.84      0.78      0.80        98\n",
      "           4       0.86      0.84      0.85       105\n",
      "           5       0.76      0.64      0.70       104\n",
      "           6       0.92      0.99      0.95        78\n",
      "           7       0.76      0.87      0.81        95\n",
      "           8       0.89      0.90      0.90        90\n",
      "           9       0.74      0.65      0.69       100\n",
      "          10       0.65      0.81      0.72       101\n",
      "          11       0.82      0.90      0.86       105\n",
      "          12       0.93      0.86      0.89       102\n",
      "          13       0.65      0.71      0.68        98\n",
      "          14       0.90      0.92      0.91       103\n",
      "          15       0.93      0.93      0.93        73\n",
      "          16       0.85      0.90      0.88        83\n",
      "          17       0.91      0.81      0.86       102\n",
      "          18       0.94      0.79      0.86       117\n",
      "          19       0.71      0.80      0.75        90\n",
      "          20       0.85      0.86      0.86        88\n",
      "          21       0.61      0.72      0.66        99\n",
      "          22       0.98      0.99      0.99       106\n",
      "          23       0.77      0.80      0.79       102\n",
      "          24       0.63      0.55      0.59       112\n",
      "          25       0.89      0.98      0.93        91\n",
      "          26       0.75      0.71      0.73       103\n",
      "          27       0.85      0.82      0.83       109\n",
      "          28       1.00      0.95      0.97        98\n",
      "          29       0.98      0.97      0.98       114\n",
      "          30       0.90      0.89      0.89        98\n",
      "          31       0.57      0.61      0.59        99\n",
      "          32       0.84      0.84      0.84        97\n",
      "          33       0.69      0.82      0.75        98\n",
      "          34       0.92      0.87      0.90        94\n",
      "          35       0.86      0.90      0.88        97\n",
      "          36       0.62      0.57      0.59        96\n",
      "          37       0.80      0.74      0.77       100\n",
      "          38       0.83      0.74      0.78       115\n",
      "          39       0.81      0.78      0.79        98\n",
      "          40       0.86      0.75      0.80       106\n",
      "          41       0.81      0.74      0.77       107\n",
      "          42       0.88      0.91      0.89        97\n",
      "          43       0.83      0.75      0.79        93\n",
      "          44       0.81      0.87      0.84       109\n",
      "          45       0.93      0.89      0.91        88\n",
      "          46       0.88      0.90      0.89       107\n",
      "          47       0.81      0.77      0.79        98\n",
      "          48       0.89      0.65      0.75        98\n",
      "          49       0.75      0.83      0.79        94\n",
      "          50       0.47      0.44      0.45        96\n",
      "          51       1.00      1.00      1.00        52\n",
      "          52       0.99      0.88      0.93        85\n",
      "          53       0.77      0.77      0.77        97\n",
      "\n",
      "    accuracy                           0.81      5296\n",
      "   macro avg       0.82      0.82      0.81      5296\n",
      "weighted avg       0.82      0.81      0.81      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 80,  accuracy score is 0.8209969788519638\n",
      "at random state 80, confusion matrix is [[97  0  0 ...  0  0  0]\n",
      " [ 0 72  0 ...  0  0  1]\n",
      " [ 0  0 88 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 53  0  0]\n",
      " [ 0  0  0 ...  0 71  0]\n",
      " [ 0  0  3 ...  0  0 64]]\n",
      "at random state 80, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.87      0.85       111\n",
      "           1       0.60      0.71      0.65       101\n",
      "           2       0.81      0.97      0.88        91\n",
      "           3       0.88      0.81      0.85        91\n",
      "           4       0.84      0.90      0.87        96\n",
      "           5       0.81      0.71      0.76       111\n",
      "           6       0.95      0.96      0.95        92\n",
      "           7       0.76      0.84      0.80       101\n",
      "           8       0.90      0.95      0.92        97\n",
      "           9       0.77      0.78      0.78       106\n",
      "          10       0.71      0.73      0.72       100\n",
      "          11       0.84      0.94      0.89       112\n",
      "          12       0.92      0.87      0.90       108\n",
      "          13       0.71      0.81      0.76        97\n",
      "          14       0.86      0.95      0.90        94\n",
      "          15       0.96      0.94      0.95        84\n",
      "          16       0.90      0.91      0.90       103\n",
      "          17       0.89      0.82      0.85        94\n",
      "          18       0.81      0.90      0.85        98\n",
      "          19       0.80      0.83      0.81        99\n",
      "          20       0.91      0.84      0.88       109\n",
      "          21       0.66      0.75      0.70        95\n",
      "          22       0.97      0.96      0.96        98\n",
      "          23       0.78      0.80      0.79        90\n",
      "          24       0.64      0.55      0.59       105\n",
      "          25       0.89      0.97      0.93        86\n",
      "          26       0.75      0.80      0.78       107\n",
      "          27       0.80      0.81      0.81       101\n",
      "          28       0.97      0.88      0.93        86\n",
      "          29       0.94      0.95      0.94       115\n",
      "          30       0.92      0.89      0.90        98\n",
      "          31       0.57      0.59      0.58       102\n",
      "          32       0.91      0.80      0.85       103\n",
      "          33       0.75      0.83      0.79        92\n",
      "          34       0.85      0.90      0.88       101\n",
      "          35       0.85      0.92      0.88        87\n",
      "          36       0.76      0.71      0.73       109\n",
      "          37       0.80      0.77      0.79       115\n",
      "          38       0.74      0.76      0.75       102\n",
      "          39       0.93      0.75      0.83       101\n",
      "          40       0.79      0.91      0.84        95\n",
      "          41       0.76      0.68      0.71       111\n",
      "          42       0.89      0.83      0.86        77\n",
      "          43       0.87      0.73      0.80       109\n",
      "          44       0.81      0.76      0.79       114\n",
      "          45       0.92      0.95      0.93        82\n",
      "          46       0.94      0.90      0.92       109\n",
      "          47       0.79      0.76      0.77        87\n",
      "          48       0.79      0.63      0.70       105\n",
      "          49       0.75      0.88      0.81       101\n",
      "          50       0.57      0.52      0.54        93\n",
      "          51       1.00      1.00      1.00        53\n",
      "          52       0.95      0.91      0.93        78\n",
      "          53       0.85      0.68      0.76        94\n",
      "\n",
      "    accuracy                           0.82      5296\n",
      "   macro avg       0.83      0.83      0.82      5296\n",
      "weighted avg       0.82      0.82      0.82      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 81,  accuracy score is 0.8157099697885196\n",
      "at random state 81, confusion matrix is [[ 92   0   0 ...   0   0   0]\n",
      " [  0  79   0 ...   0   0   1]\n",
      " [  0   0 107 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  48   0   0]\n",
      " [  0   0   0 ...   0  68   0]\n",
      " [  0   1   1 ...   0   0  85]]\n",
      "at random state 81, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.92      0.85       100\n",
      "           1       0.67      0.73      0.70       108\n",
      "           2       0.84      0.96      0.90       111\n",
      "           3       0.84      0.77      0.80        96\n",
      "           4       0.90      0.90      0.90        97\n",
      "           5       0.82      0.72      0.77       104\n",
      "           6       0.96      0.96      0.96        82\n",
      "           7       0.73      0.90      0.81       100\n",
      "           8       0.91      0.92      0.91        99\n",
      "           9       0.76      0.86      0.81       104\n",
      "          10       0.68      0.77      0.72       104\n",
      "          11       0.79      0.89      0.84       109\n",
      "          12       0.88      0.87      0.88        95\n",
      "          13       0.76      0.72      0.74       112\n",
      "          14       0.89      0.92      0.91        90\n",
      "          15       0.93      0.87      0.90        82\n",
      "          16       0.92      0.87      0.89       105\n",
      "          17       0.93      0.84      0.88        93\n",
      "          18       0.86      0.81      0.83       115\n",
      "          19       0.80      0.86      0.83        95\n",
      "          20       0.85      0.86      0.86        88\n",
      "          21       0.57      0.73      0.64        92\n",
      "          22       0.93      0.98      0.95        97\n",
      "          23       0.83      0.85      0.84        91\n",
      "          24       0.60      0.54      0.57       101\n",
      "          25       0.88      0.91      0.89       100\n",
      "          26       0.73      0.81      0.77        99\n",
      "          27       0.87      0.82      0.84       110\n",
      "          28       0.98      0.90      0.94       100\n",
      "          29       0.95      0.98      0.96        93\n",
      "          30       0.93      0.89      0.91       104\n",
      "          31       0.52      0.59      0.55        96\n",
      "          32       0.82      0.85      0.84        94\n",
      "          33       0.73      0.72      0.72       116\n",
      "          34       0.86      0.87      0.87       101\n",
      "          35       0.89      0.93      0.91        85\n",
      "          36       0.74      0.67      0.70       110\n",
      "          37       0.80      0.74      0.77        96\n",
      "          38       0.81      0.81      0.81       103\n",
      "          39       0.85      0.83      0.84       106\n",
      "          40       0.80      0.80      0.80       113\n",
      "          41       0.80      0.62      0.70       120\n",
      "          42       0.86      0.82      0.84        93\n",
      "          43       0.78      0.74      0.76        97\n",
      "          44       0.84      0.82      0.83        95\n",
      "          45       0.97      0.89      0.93        94\n",
      "          46       0.85      0.94      0.89        72\n",
      "          47       0.82      0.76      0.79       101\n",
      "          48       0.87      0.63      0.73        94\n",
      "          49       0.82      0.87      0.85       108\n",
      "          50       0.43      0.36      0.40        99\n",
      "          51       1.00      1.00      1.00        48\n",
      "          52       0.97      0.96      0.96        71\n",
      "          53       0.83      0.79      0.81       108\n",
      "\n",
      "    accuracy                           0.82      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.82      0.82      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 82,  accuracy score is 0.8162764350453172\n",
      "at random state 82, confusion matrix is [[89  0  0 ...  0  0  0]\n",
      " [ 0 80  0 ...  0  0  0]\n",
      " [ 0  0 83 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 45  0  0]\n",
      " [ 0  0  0 ...  0 76  0]\n",
      " [ 0  1  0 ...  0  0 70]]\n",
      "at random state 82, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.88      0.84       101\n",
      "           1       0.71      0.73      0.72       109\n",
      "           2       0.85      0.89      0.87        93\n",
      "           3       0.83      0.77      0.80        88\n",
      "           4       0.83      0.92      0.87       100\n",
      "           5       0.73      0.76      0.74        94\n",
      "           6       0.94      0.97      0.96       100\n",
      "           7       0.70      0.81      0.75       109\n",
      "           8       0.98      0.96      0.97        98\n",
      "           9       0.73      0.80      0.76       104\n",
      "          10       0.65      0.67      0.66       105\n",
      "          11       0.88      0.92      0.90       117\n",
      "          12       0.90      0.88      0.89        97\n",
      "          13       0.73      0.71      0.72       109\n",
      "          14       0.85      0.88      0.86       108\n",
      "          15       0.96      0.92      0.94        84\n",
      "          16       0.96      0.87      0.91       101\n",
      "          17       0.93      0.90      0.92        91\n",
      "          18       0.81      0.88      0.85        94\n",
      "          19       0.78      0.83      0.80        98\n",
      "          20       0.92      0.84      0.88       102\n",
      "          21       0.67      0.71      0.69       109\n",
      "          22       0.94      0.98      0.96        87\n",
      "          23       0.80      0.82      0.81        91\n",
      "          24       0.62      0.65      0.64        85\n",
      "          25       0.90      0.98      0.94        96\n",
      "          26       0.70      0.73      0.71       107\n",
      "          27       0.88      0.81      0.84        96\n",
      "          28       0.98      0.90      0.94       116\n",
      "          29       0.97      0.89      0.93        94\n",
      "          30       0.92      0.83      0.88       109\n",
      "          31       0.54      0.58      0.56        98\n",
      "          32       0.88      0.82      0.85       112\n",
      "          33       0.64      0.70      0.67       106\n",
      "          34       0.92      0.90      0.91        91\n",
      "          35       0.89      0.95      0.92        98\n",
      "          36       0.84      0.71      0.77       106\n",
      "          37       0.71      0.78      0.74       101\n",
      "          38       0.79      0.66      0.72        99\n",
      "          39       0.76      0.81      0.79        91\n",
      "          40       0.74      0.81      0.78       100\n",
      "          41       0.70      0.63      0.66       103\n",
      "          42       0.87      0.85      0.86        95\n",
      "          43       0.85      0.79      0.82       108\n",
      "          44       0.80      0.83      0.82       101\n",
      "          45       0.91      0.96      0.93        90\n",
      "          46       0.93      0.91      0.92        95\n",
      "          47       0.81      0.75      0.78        99\n",
      "          48       0.85      0.75      0.80       105\n",
      "          49       0.79      0.85      0.82        99\n",
      "          50       0.50      0.45      0.47        85\n",
      "          51       1.00      1.00      1.00        45\n",
      "          52       0.96      0.95      0.96        80\n",
      "          53       0.81      0.72      0.77        97\n",
      "\n",
      "    accuracy                           0.82      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.82      0.82      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 83,  accuracy score is 0.8085347432024169\n",
      "at random state 83, confusion matrix is [[ 88   0   0 ...   0   0   0]\n",
      " [  0  60   0 ...   0   0   0]\n",
      " [  0   0 103 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  56   0   0]\n",
      " [  0   0   0 ...   0  58   0]\n",
      " [  0   0   2 ...   0   0  68]]\n",
      "at random state 83, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.88      0.85       100\n",
      "           1       0.57      0.70      0.63        86\n",
      "           2       0.81      0.97      0.88       106\n",
      "           3       0.78      0.81      0.79        95\n",
      "           4       0.86      0.90      0.88       103\n",
      "           5       0.78      0.75      0.77       110\n",
      "           6       0.92      0.95      0.93        96\n",
      "           7       0.75      0.81      0.78        95\n",
      "           8       0.95      0.92      0.94        91\n",
      "           9       0.68      0.89      0.77        96\n",
      "          10       0.68      0.73      0.70       106\n",
      "          11       0.81      0.88      0.85       102\n",
      "          12       0.89      0.85      0.87        99\n",
      "          13       0.73      0.62      0.67        99\n",
      "          14       0.89      0.94      0.92        98\n",
      "          15       0.94      0.93      0.93        84\n",
      "          16       0.84      0.92      0.88        90\n",
      "          17       0.91      0.87      0.89       103\n",
      "          18       0.85      0.85      0.85        96\n",
      "          19       0.79      0.83      0.81       103\n",
      "          20       0.90      0.85      0.88        99\n",
      "          21       0.62      0.75      0.68        99\n",
      "          22       0.96      0.94      0.95       109\n",
      "          23       0.79      0.83      0.81       102\n",
      "          24       0.64      0.54      0.59       101\n",
      "          25       0.89      0.98      0.94       103\n",
      "          26       0.69      0.67      0.68       104\n",
      "          27       0.88      0.79      0.83        97\n",
      "          28       0.96      0.89      0.93       113\n",
      "          29       0.92      0.98      0.95        87\n",
      "          30       0.84      0.83      0.84        92\n",
      "          31       0.54      0.58      0.56        97\n",
      "          32       0.87      0.74      0.80       102\n",
      "          33       0.64      0.72      0.68        90\n",
      "          34       0.94      0.88      0.91       112\n",
      "          35       0.92      0.93      0.93       100\n",
      "          36       0.72      0.69      0.71       104\n",
      "          37       0.69      0.71      0.70        96\n",
      "          38       0.80      0.68      0.73       106\n",
      "          39       0.79      0.83      0.81       102\n",
      "          40       0.81      0.83      0.82       100\n",
      "          41       0.83      0.69      0.75       102\n",
      "          42       0.92      0.87      0.90       109\n",
      "          43       0.80      0.71      0.75        94\n",
      "          44       0.79      0.83      0.81       101\n",
      "          45       0.92      0.93      0.92        95\n",
      "          46       0.94      0.86      0.90       103\n",
      "          47       0.76      0.66      0.71        98\n",
      "          48       0.88      0.63      0.73       111\n",
      "          49       0.77      0.87      0.82        93\n",
      "          50       0.49      0.51      0.50        99\n",
      "          51       1.00      1.00      1.00        56\n",
      "          52       0.95      0.94      0.94        62\n",
      "          53       0.82      0.68      0.74       100\n",
      "\n",
      "    accuracy                           0.81      5296\n",
      "   macro avg       0.81      0.81      0.81      5296\n",
      "weighted avg       0.81      0.81      0.81      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 84,  accuracy score is 0.8213746223564955\n",
      "at random state 84, confusion matrix is [[73  0  0 ...  0  0  0]\n",
      " [ 0 68  0 ...  0  0  0]\n",
      " [ 0  0 87 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 52  0  0]\n",
      " [ 0  0  0 ...  0 86  0]\n",
      " [ 0  0  2 ...  0  0 78]]\n",
      "at random state 84, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.91      0.80        80\n",
      "           1       0.61      0.76      0.68        90\n",
      "           2       0.84      0.93      0.88        94\n",
      "           3       0.89      0.72      0.80       108\n",
      "           4       0.90      0.85      0.87       101\n",
      "           5       0.68      0.75      0.71        91\n",
      "           6       0.92      0.94      0.93        96\n",
      "           7       0.80      0.85      0.82       101\n",
      "           8       0.90      0.94      0.92       101\n",
      "           9       0.74      0.81      0.77       109\n",
      "          10       0.57      0.71      0.63        93\n",
      "          11       0.85      0.91      0.88        93\n",
      "          12       0.93      0.84      0.88       100\n",
      "          13       0.66      0.65      0.66        95\n",
      "          14       0.93      0.90      0.91       101\n",
      "          15       0.98      0.93      0.95        85\n",
      "          16       0.94      0.94      0.94       112\n",
      "          17       0.97      0.86      0.91       103\n",
      "          18       0.84      0.83      0.84        96\n",
      "          19       0.78      0.86      0.82       111\n",
      "          20       0.93      0.80      0.86       102\n",
      "          21       0.63      0.64      0.63        89\n",
      "          22       0.92      0.96      0.94       111\n",
      "          23       0.76      0.85      0.80        96\n",
      "          24       0.70      0.59      0.64        98\n",
      "          25       0.88      0.97      0.93       102\n",
      "          26       0.78      0.76      0.77       100\n",
      "          27       0.91      0.80      0.85       103\n",
      "          28       0.97      0.90      0.93       103\n",
      "          29       0.96      0.91      0.94       103\n",
      "          30       0.91      0.90      0.91       112\n",
      "          31       0.43      0.62      0.51        81\n",
      "          32       0.90      0.84      0.87       100\n",
      "          33       0.74      0.74      0.74       108\n",
      "          34       0.88      0.93      0.91        98\n",
      "          35       0.88      0.96      0.92       101\n",
      "          36       0.75      0.75      0.75        89\n",
      "          37       0.71      0.77      0.74       100\n",
      "          38       0.84      0.83      0.83       100\n",
      "          39       0.80      0.75      0.77       114\n",
      "          40       0.82      0.82      0.82       102\n",
      "          41       0.80      0.71      0.75       100\n",
      "          42       0.92      0.88      0.90       106\n",
      "          43       0.89      0.70      0.78       101\n",
      "          44       0.79      0.83      0.81       115\n",
      "          45       0.99      0.90      0.94        88\n",
      "          46       0.91      0.95      0.93        79\n",
      "          47       0.77      0.81      0.79        94\n",
      "          48       0.83      0.62      0.71       101\n",
      "          49       0.84      0.75      0.80       106\n",
      "          50       0.58      0.48      0.53        96\n",
      "          51       1.00      1.00      1.00        52\n",
      "          52       1.00      0.97      0.98        89\n",
      "          53       0.80      0.80      0.80        97\n",
      "\n",
      "    accuracy                           0.82      5296\n",
      "   macro avg       0.83      0.82      0.82      5296\n",
      "weighted avg       0.83      0.82      0.82      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 85,  accuracy score is 0.8177870090634441\n",
      "at random state 85, confusion matrix is [[106   0   0 ...   0   0   0]\n",
      " [  0  74   0 ...   0   0   2]\n",
      " [  0   0  96 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  52   0   0]\n",
      " [  0   0   0 ...   0  69   0]\n",
      " [  0   0   1 ...   0   0  83]]\n",
      "at random state 85, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.93      0.90       114\n",
      "           1       0.66      0.78      0.71        95\n",
      "           2       0.89      0.92      0.91       104\n",
      "           3       0.82      0.75      0.78        96\n",
      "           4       0.84      0.90      0.87        99\n",
      "           5       0.77      0.65      0.71       104\n",
      "           6       0.95      0.99      0.97        90\n",
      "           7       0.71      0.91      0.80        97\n",
      "           8       0.93      0.91      0.92       108\n",
      "           9       0.77      0.74      0.75       104\n",
      "          10       0.64      0.80      0.71        95\n",
      "          11       0.82      0.95      0.88        99\n",
      "          12       0.94      0.83      0.88       109\n",
      "          13       0.72      0.76      0.74       102\n",
      "          14       0.90      0.92      0.91       107\n",
      "          15       0.95      0.95      0.95        76\n",
      "          16       0.88      0.91      0.89        93\n",
      "          17       0.91      0.83      0.87       103\n",
      "          18       0.84      0.75      0.79       110\n",
      "          19       0.73      0.78      0.75        88\n",
      "          20       0.85      0.74      0.79       108\n",
      "          21       0.69      0.76      0.72        91\n",
      "          22       0.98      0.93      0.95        88\n",
      "          23       0.78      0.78      0.78        98\n",
      "          24       0.51      0.57      0.54        91\n",
      "          25       0.85      0.98      0.91       106\n",
      "          26       0.72      0.77      0.74       103\n",
      "          27       0.89      0.81      0.85       102\n",
      "          28       0.97      0.91      0.94       103\n",
      "          29       0.99      0.88      0.93        94\n",
      "          30       0.94      0.87      0.90       104\n",
      "          31       0.49      0.57      0.53       101\n",
      "          32       0.93      0.89      0.91        95\n",
      "          33       0.67      0.77      0.72        87\n",
      "          34       0.92      0.93      0.93       102\n",
      "          35       0.96      0.92      0.94       118\n",
      "          36       0.80      0.77      0.78       107\n",
      "          37       0.81      0.72      0.76        98\n",
      "          38       0.83      0.73      0.78       109\n",
      "          39       0.78      0.80      0.79        91\n",
      "          40       0.83      0.85      0.84       100\n",
      "          41       0.72      0.66      0.69        99\n",
      "          42       0.89      0.85      0.87        95\n",
      "          43       0.81      0.82      0.81        94\n",
      "          44       0.79      0.83      0.81        93\n",
      "          45       0.93      0.93      0.93        86\n",
      "          46       0.91      0.93      0.92        90\n",
      "          47       0.80      0.73      0.76       113\n",
      "          48       0.85      0.73      0.79       101\n",
      "          49       0.76      0.86      0.81        86\n",
      "          50       0.55      0.42      0.48       113\n",
      "          51       1.00      1.00      1.00        52\n",
      "          52       0.92      0.96      0.94        72\n",
      "          53       0.81      0.73      0.77       113\n",
      "\n",
      "    accuracy                           0.82      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.82      0.82      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 86,  accuracy score is 0.8108006042296072\n",
      "at random state 86, confusion matrix is [[95  0  0 ...  0  0  0]\n",
      " [ 0 68  0 ...  0  0  2]\n",
      " [ 0  0 88 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 54  0  0]\n",
      " [ 0  0  0 ...  0 63  0]\n",
      " [ 0  0  1 ...  0  0 65]]\n",
      "at random state 86, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.89      0.86       107\n",
      "           1       0.65      0.69      0.67        99\n",
      "           2       0.82      0.96      0.88        92\n",
      "           3       0.88      0.77      0.82       104\n",
      "           4       0.93      0.86      0.89       105\n",
      "           5       0.76      0.67      0.71        99\n",
      "           6       0.93      0.97      0.95        77\n",
      "           7       0.80      0.84      0.82       109\n",
      "           8       0.87      0.95      0.91       101\n",
      "           9       0.68      0.77      0.72        90\n",
      "          10       0.75      0.79      0.77       111\n",
      "          11       0.78      0.92      0.84       100\n",
      "          12       0.90      0.89      0.90        93\n",
      "          13       0.74      0.69      0.72       101\n",
      "          14       0.92      0.92      0.92        91\n",
      "          15       1.00      0.97      0.99        79\n",
      "          16       0.91      0.90      0.91       103\n",
      "          17       0.90      0.87      0.89        93\n",
      "          18       0.84      0.80      0.82        99\n",
      "          19       0.79      0.82      0.80       103\n",
      "          20       0.87      0.82      0.85        97\n",
      "          21       0.60      0.74      0.66       103\n",
      "          22       0.96      0.97      0.97       111\n",
      "          23       0.75      0.89      0.81        98\n",
      "          24       0.51      0.55      0.53        96\n",
      "          25       0.88      0.97      0.92        91\n",
      "          26       0.75      0.73      0.74       123\n",
      "          27       0.83      0.80      0.82       107\n",
      "          28       0.99      0.87      0.93       116\n",
      "          29       0.93      0.94      0.94        87\n",
      "          30       0.92      0.88      0.90        98\n",
      "          31       0.51      0.56      0.54        95\n",
      "          32       0.81      0.78      0.80        97\n",
      "          33       0.68      0.75      0.71       104\n",
      "          34       0.92      0.90      0.91       108\n",
      "          35       0.86      0.85      0.86        96\n",
      "          36       0.74      0.70      0.72        91\n",
      "          37       0.71      0.73      0.72       106\n",
      "          38       0.73      0.82      0.78        97\n",
      "          39       0.82      0.80      0.81        95\n",
      "          40       0.76      0.80      0.78        89\n",
      "          41       0.76      0.61      0.68       120\n",
      "          42       0.88      0.85      0.86        93\n",
      "          43       0.81      0.73      0.77        92\n",
      "          44       0.88      0.84      0.86       109\n",
      "          45       0.96      0.93      0.95       113\n",
      "          46       0.89      0.89      0.89        99\n",
      "          47       0.81      0.74      0.77       104\n",
      "          48       0.77      0.69      0.73        99\n",
      "          49       0.80      0.86      0.83        94\n",
      "          50       0.49      0.40      0.44       103\n",
      "          51       1.00      1.00      1.00        54\n",
      "          52       0.97      0.95      0.96        66\n",
      "          53       0.83      0.73      0.78        89\n",
      "\n",
      "    accuracy                           0.81      5296\n",
      "   macro avg       0.82      0.82      0.81      5296\n",
      "weighted avg       0.81      0.81      0.81      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 87,  accuracy score is 0.8091012084592145\n",
      "at random state 87, confusion matrix is [[ 90   0   0 ...   0   0   0]\n",
      " [  0  77   0 ...   0   0   3]\n",
      " [  0   0 107 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  48   0   0]\n",
      " [  0   0   0 ...   0  63   0]\n",
      " [  0   1   1 ...   0   0  50]]\n",
      "at random state 87, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.91      0.82        99\n",
      "           1       0.64      0.73      0.68       105\n",
      "           2       0.86      0.96      0.91       111\n",
      "           3       0.88      0.75      0.81       102\n",
      "           4       0.87      0.90      0.88       111\n",
      "           5       0.75      0.67      0.71       117\n",
      "           6       0.94      0.97      0.95        91\n",
      "           7       0.73      0.81      0.77       105\n",
      "           8       0.90      0.93      0.92        88\n",
      "           9       0.73      0.74      0.74       113\n",
      "          10       0.63      0.70      0.66       107\n",
      "          11       0.82      0.87      0.84       107\n",
      "          12       0.92      0.87      0.90        93\n",
      "          13       0.66      0.62      0.64        98\n",
      "          14       0.92      0.88      0.90        99\n",
      "          15       0.97      0.92      0.94        83\n",
      "          16       0.85      0.90      0.88        91\n",
      "          17       0.92      0.90      0.91       105\n",
      "          18       0.80      0.81      0.81       101\n",
      "          19       0.75      0.81      0.78       102\n",
      "          20       0.91      0.84      0.87       102\n",
      "          21       0.68      0.76      0.72       103\n",
      "          22       0.97      0.94      0.96        83\n",
      "          23       0.82      0.88      0.85        92\n",
      "          24       0.59      0.60      0.59        92\n",
      "          25       0.90      0.95      0.92       101\n",
      "          26       0.71      0.73      0.72       102\n",
      "          27       0.81      0.82      0.81       101\n",
      "          28       0.96      0.84      0.90        97\n",
      "          29       0.94      0.99      0.96        92\n",
      "          30       0.94      0.84      0.89       101\n",
      "          31       0.52      0.69      0.59        95\n",
      "          32       0.87      0.80      0.84       112\n",
      "          33       0.69      0.75      0.72        99\n",
      "          34       0.91      0.89      0.90        99\n",
      "          35       0.89      0.91      0.90        99\n",
      "          36       0.74      0.67      0.70       102\n",
      "          37       0.69      0.71      0.70        83\n",
      "          38       0.75      0.76      0.76        92\n",
      "          39       0.80      0.85      0.82       106\n",
      "          40       0.84      0.83      0.83       110\n",
      "          41       0.81      0.69      0.75       108\n",
      "          42       0.82      0.83      0.82        88\n",
      "          43       0.76      0.76      0.76        92\n",
      "          44       0.83      0.82      0.83       102\n",
      "          45       0.96      0.92      0.94        95\n",
      "          46       0.91      0.96      0.93       101\n",
      "          47       0.79      0.72      0.76        98\n",
      "          48       0.87      0.68      0.77       101\n",
      "          49       0.82      0.80      0.81       116\n",
      "          50       0.64      0.44      0.52       111\n",
      "          51       1.00      1.00      1.00        48\n",
      "          52       0.93      0.93      0.93        68\n",
      "          53       0.74      0.65      0.69        77\n",
      "\n",
      "    accuracy                           0.81      5296\n",
      "   macro avg       0.82      0.81      0.81      5296\n",
      "weighted avg       0.81      0.81      0.81      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 88,  accuracy score is 0.8189199395770392\n",
      "at random state 88, confusion matrix is [[87  0  0 ...  0  0  0]\n",
      " [ 0 86  0 ...  0  0  1]\n",
      " [ 0  0 79 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 64  0  0]\n",
      " [ 0  0  0 ...  1 70  0]\n",
      " [ 0  1  2 ...  0  0 75]]\n",
      "at random state 88, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.89      0.83        98\n",
      "           1       0.75      0.73      0.74       118\n",
      "           2       0.77      0.94      0.85        84\n",
      "           3       0.81      0.86      0.83        91\n",
      "           4       0.87      0.92      0.89       110\n",
      "           5       0.74      0.75      0.74        91\n",
      "           6       0.95      0.96      0.95        73\n",
      "           7       0.72      0.84      0.78        85\n",
      "           8       0.96      0.92      0.94        99\n",
      "           9       0.73      0.76      0.74       113\n",
      "          10       0.69      0.73      0.71       105\n",
      "          11       0.85      0.92      0.88       103\n",
      "          12       0.90      0.86      0.88        98\n",
      "          13       0.66      0.72      0.69        90\n",
      "          14       0.87      0.89      0.88       103\n",
      "          15       0.94      0.90      0.92        87\n",
      "          16       0.91      0.92      0.91        85\n",
      "          17       0.93      0.86      0.89       107\n",
      "          18       0.86      0.86      0.86        91\n",
      "          19       0.81      0.82      0.82       106\n",
      "          20       0.90      0.82      0.86        89\n",
      "          21       0.71      0.69      0.70       124\n",
      "          22       1.00      0.98      0.99       116\n",
      "          23       0.84      0.80      0.82       110\n",
      "          24       0.55      0.58      0.57        96\n",
      "          25       0.87      0.99      0.92        80\n",
      "          26       0.78      0.78      0.78       101\n",
      "          27       0.84      0.85      0.84        99\n",
      "          28       0.96      0.87      0.91        99\n",
      "          29       0.95      0.94      0.94        97\n",
      "          30       0.92      0.82      0.87       106\n",
      "          31       0.57      0.61      0.59       102\n",
      "          32       0.86      0.82      0.84       108\n",
      "          33       0.65      0.80      0.72        87\n",
      "          34       0.89      0.84      0.87       103\n",
      "          35       0.91      0.96      0.93        97\n",
      "          36       0.61      0.79      0.69        86\n",
      "          37       0.67      0.72      0.70        87\n",
      "          38       0.77      0.69      0.73       106\n",
      "          39       0.87      0.85      0.86        97\n",
      "          40       0.81      0.88      0.84        91\n",
      "          41       0.72      0.66      0.69       108\n",
      "          42       0.87      0.91      0.89        91\n",
      "          43       0.90      0.65      0.76       117\n",
      "          44       0.85      0.84      0.85       124\n",
      "          45       1.00      0.95      0.97        82\n",
      "          46       0.91      0.96      0.93        93\n",
      "          47       0.83      0.83      0.83        99\n",
      "          48       0.80      0.69      0.74       105\n",
      "          49       0.84      0.85      0.85       116\n",
      "          50       0.49      0.43      0.46        97\n",
      "          51       0.98      1.00      0.99        64\n",
      "          52       1.00      0.92      0.96        76\n",
      "          53       0.90      0.71      0.79       106\n",
      "\n",
      "    accuracy                           0.82      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.82      0.82      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 89,  accuracy score is 0.8126888217522659\n",
      "at random state 89, confusion matrix is [[ 98   0   0 ...   0   0   0]\n",
      " [  0  77   0 ...   0   0   0]\n",
      " [  0   0 108 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  55   0   0]\n",
      " [  0   0   0 ...   1  77   0]\n",
      " [  0   1   1 ...   0   0  76]]\n",
      "at random state 89, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.88      0.86       112\n",
      "           1       0.65      0.73      0.69       105\n",
      "           2       0.84      0.96      0.89       113\n",
      "           3       0.88      0.79      0.83        99\n",
      "           4       0.86      0.87      0.86        95\n",
      "           5       0.78      0.79      0.78       105\n",
      "           6       0.98      0.97      0.97        93\n",
      "           7       0.70      0.91      0.80        94\n",
      "           8       0.93      0.93      0.93       108\n",
      "           9       0.69      0.83      0.75        83\n",
      "          10       0.55      0.65      0.60        94\n",
      "          11       0.79      0.86      0.82       104\n",
      "          12       0.92      0.83      0.87        95\n",
      "          13       0.71      0.73      0.72        99\n",
      "          14       0.88      0.90      0.89        94\n",
      "          15       0.97      0.87      0.92        75\n",
      "          16       0.87      0.92      0.89       108\n",
      "          17       0.93      0.77      0.84        97\n",
      "          18       0.77      0.83      0.80       106\n",
      "          19       0.73      0.76      0.74        92\n",
      "          20       0.91      0.91      0.91       102\n",
      "          21       0.57      0.72      0.64        85\n",
      "          22       0.94      0.97      0.96       105\n",
      "          23       0.80      0.81      0.80       111\n",
      "          24       0.69      0.61      0.64        97\n",
      "          25       0.90      0.96      0.93       100\n",
      "          26       0.69      0.80      0.74        93\n",
      "          27       0.91      0.79      0.84       100\n",
      "          28       1.00      0.91      0.95       111\n",
      "          29       0.93      0.97      0.95        91\n",
      "          30       0.91      0.86      0.89       108\n",
      "          31       0.56      0.65      0.60       102\n",
      "          32       0.92      0.78      0.84        98\n",
      "          33       0.72      0.74      0.73       102\n",
      "          34       0.88      0.85      0.87       103\n",
      "          35       0.93      0.92      0.92       111\n",
      "          36       0.75      0.71      0.73       111\n",
      "          37       0.75      0.73      0.74        91\n",
      "          38       0.77      0.71      0.74        99\n",
      "          39       0.81      0.85      0.83        94\n",
      "          40       0.83      0.76      0.79       121\n",
      "          41       0.65      0.61      0.63        92\n",
      "          42       0.89      0.89      0.89        82\n",
      "          43       0.83      0.75      0.79       107\n",
      "          44       0.88      0.89      0.89        95\n",
      "          45       0.91      0.91      0.91        86\n",
      "          46       0.88      0.90      0.89        88\n",
      "          47       0.77      0.86      0.81        84\n",
      "          48       0.86      0.57      0.69       115\n",
      "          49       0.80      0.82      0.81       104\n",
      "          50       0.52      0.40      0.45        92\n",
      "          51       0.98      1.00      0.99        55\n",
      "          52       0.96      0.92      0.94        84\n",
      "          53       0.83      0.72      0.77       106\n",
      "\n",
      "    accuracy                           0.81      5296\n",
      "   macro avg       0.82      0.81      0.81      5296\n",
      "weighted avg       0.82      0.81      0.81      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 90,  accuracy score is 0.8172205438066465\n",
      "at random state 90, confusion matrix is [[93  0  0 ...  0  0  0]\n",
      " [ 0 73  0 ...  0  0  2]\n",
      " [ 0  0 83 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 51  0  0]\n",
      " [ 0  0  0 ...  0 70  0]\n",
      " [ 0  0  2 ...  0  0 63]]\n",
      "at random state 90, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.89      0.84       104\n",
      "           1       0.65      0.68      0.66       108\n",
      "           2       0.83      0.93      0.88        89\n",
      "           3       0.83      0.75      0.79       115\n",
      "           4       0.89      0.91      0.90       102\n",
      "           5       0.80      0.63      0.71       109\n",
      "           6       0.95      0.99      0.97        89\n",
      "           7       0.71      0.88      0.78        93\n",
      "           8       0.95      0.92      0.93       109\n",
      "           9       0.65      0.78      0.71        92\n",
      "          10       0.70      0.83      0.76        95\n",
      "          11       0.85      0.90      0.88       101\n",
      "          12       0.93      0.88      0.91       110\n",
      "          13       0.71      0.70      0.70       106\n",
      "          14       0.90      0.95      0.92        93\n",
      "          15       0.96      0.99      0.97        90\n",
      "          16       0.86      0.93      0.90        87\n",
      "          17       0.94      0.73      0.82        81\n",
      "          18       0.82      0.86      0.84       105\n",
      "          19       0.86      0.85      0.86       108\n",
      "          20       0.90      0.84      0.87       101\n",
      "          21       0.69      0.71      0.70       108\n",
      "          22       0.96      0.95      0.96       110\n",
      "          23       0.73      0.77      0.75       111\n",
      "          24       0.53      0.57      0.55        96\n",
      "          25       0.92      0.96      0.94       104\n",
      "          26       0.69      0.70      0.70        88\n",
      "          27       0.82      0.79      0.80        86\n",
      "          28       0.98      0.93      0.95       100\n",
      "          29       0.97      0.92      0.94       108\n",
      "          30       0.94      0.90      0.92        98\n",
      "          31       0.52      0.59      0.55       108\n",
      "          32       0.91      0.84      0.87       100\n",
      "          33       0.70      0.77      0.73        93\n",
      "          34       0.89      0.90      0.89       103\n",
      "          35       0.95      0.92      0.93       119\n",
      "          36       0.68      0.72      0.70        95\n",
      "          37       0.73      0.80      0.76        91\n",
      "          38       0.85      0.74      0.79       102\n",
      "          39       0.88      0.80      0.84        96\n",
      "          40       0.84      0.84      0.84       117\n",
      "          41       0.80      0.64      0.71       104\n",
      "          42       0.93      0.87      0.90        87\n",
      "          43       0.87      0.69      0.77        96\n",
      "          44       0.86      0.81      0.83       102\n",
      "          45       0.93      0.95      0.94        75\n",
      "          46       0.94      0.95      0.95        86\n",
      "          47       0.79      0.80      0.79       104\n",
      "          48       0.83      0.68      0.75        98\n",
      "          49       0.80      0.83      0.81       106\n",
      "          50       0.48      0.45      0.46       116\n",
      "          51       1.00      1.00      1.00        51\n",
      "          52       0.97      0.96      0.97        73\n",
      "          53       0.72      0.81      0.76        78\n",
      "\n",
      "    accuracy                           0.82      5296\n",
      "   macro avg       0.83      0.82      0.82      5296\n",
      "weighted avg       0.82      0.82      0.82      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 91,  accuracy score is 0.8185422960725075\n",
      "at random state 91, confusion matrix is [[89  0  0 ...  0  0  0]\n",
      " [ 0 78  0 ...  0  0  2]\n",
      " [ 0  0 92 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 55  0  0]\n",
      " [ 0  0  0 ...  0 68  0]\n",
      " [ 0  0  3 ...  0  0 69]]\n",
      "at random state 91, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.85      0.82       105\n",
      "           1       0.64      0.72      0.68       108\n",
      "           2       0.84      0.95      0.89        97\n",
      "           3       0.90      0.79      0.84       115\n",
      "           4       0.87      0.88      0.88       104\n",
      "           5       0.81      0.71      0.75       102\n",
      "           6       0.91      0.94      0.93        90\n",
      "           7       0.72      0.86      0.78        93\n",
      "           8       0.91      0.94      0.93        89\n",
      "           9       0.68      0.82      0.74        98\n",
      "          10       0.59      0.78      0.67        98\n",
      "          11       0.84      0.94      0.89       105\n",
      "          12       0.95      0.91      0.93       109\n",
      "          13       0.73      0.69      0.71       105\n",
      "          14       0.90      0.95      0.92        94\n",
      "          15       0.92      0.94      0.93        90\n",
      "          16       0.88      0.89      0.88        96\n",
      "          17       0.92      0.85      0.89       114\n",
      "          18       0.82      0.83      0.83        95\n",
      "          19       0.82      0.81      0.81       105\n",
      "          20       0.85      0.84      0.84        97\n",
      "          21       0.63      0.74      0.68        95\n",
      "          22       0.97      0.94      0.95        94\n",
      "          23       0.71      0.76      0.73        95\n",
      "          24       0.68      0.57      0.62       102\n",
      "          25       0.87      0.96      0.91       105\n",
      "          26       0.72      0.80      0.76        94\n",
      "          27       0.83      0.83      0.83        83\n",
      "          28       0.98      0.90      0.94       110\n",
      "          29       0.98      0.91      0.94       106\n",
      "          30       0.86      0.90      0.88       101\n",
      "          31       0.58      0.70      0.63        96\n",
      "          32       0.84      0.76      0.80        98\n",
      "          33       0.72      0.76      0.74       103\n",
      "          34       0.87      0.90      0.89       100\n",
      "          35       0.89      0.93      0.91        94\n",
      "          36       0.73      0.71      0.72       102\n",
      "          37       0.75      0.79      0.77        91\n",
      "          38       0.80      0.73      0.76        95\n",
      "          39       0.83      0.76      0.79       100\n",
      "          40       0.84      0.82      0.83       102\n",
      "          41       0.72      0.65      0.69        84\n",
      "          42       0.92      0.82      0.87       104\n",
      "          43       0.88      0.75      0.81       103\n",
      "          44       0.89      0.86      0.87       108\n",
      "          45       0.91      0.95      0.93        88\n",
      "          46       0.94      0.90      0.92        98\n",
      "          47       0.78      0.81      0.79        98\n",
      "          48       0.81      0.59      0.69        96\n",
      "          49       0.83      0.86      0.85       110\n",
      "          50       0.61      0.50      0.55       105\n",
      "          51       1.00      1.00      1.00        55\n",
      "          52       0.96      0.89      0.93        76\n",
      "          53       0.82      0.72      0.77        96\n",
      "\n",
      "    accuracy                           0.82      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.82      0.82      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 92,  accuracy score is 0.8172205438066465\n",
      "at random state 92, confusion matrix is [[88  0  0 ...  0  0  0]\n",
      " [ 0 69  0 ...  0  0  0]\n",
      " [ 0  0 95 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 50  0  0]\n",
      " [ 0  0  0 ...  0 80  0]\n",
      " [ 0  0  1 ...  0  0 83]]\n",
      "at random state 92, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.90      0.83        98\n",
      "           1       0.63      0.74      0.68        93\n",
      "           2       0.85      0.96      0.90        99\n",
      "           3       0.90      0.79      0.84       107\n",
      "           4       0.92      0.83      0.87       103\n",
      "           5       0.73      0.63      0.68        98\n",
      "           6       0.89      0.95      0.92        79\n",
      "           7       0.76      0.85      0.80        88\n",
      "           8       0.94      0.92      0.93       109\n",
      "           9       0.75      0.82      0.78       104\n",
      "          10       0.67      0.78      0.72        97\n",
      "          11       0.86      0.94      0.90       107\n",
      "          12       0.92      0.90      0.91       108\n",
      "          13       0.68      0.72      0.70       100\n",
      "          14       0.89      0.90      0.89       103\n",
      "          15       0.90      0.97      0.93        86\n",
      "          16       0.88      0.86      0.87       101\n",
      "          17       0.92      0.85      0.88        99\n",
      "          18       0.89      0.86      0.87       101\n",
      "          19       0.82      0.83      0.82       105\n",
      "          20       0.84      0.88      0.86        95\n",
      "          21       0.67      0.70      0.69       103\n",
      "          22       0.99      0.88      0.93       124\n",
      "          23       0.81      0.84      0.82       108\n",
      "          24       0.59      0.57      0.58       103\n",
      "          25       0.83      0.99      0.90        93\n",
      "          26       0.73      0.80      0.77       100\n",
      "          27       0.82      0.85      0.83        99\n",
      "          28       0.96      0.94      0.95        85\n",
      "          29       0.97      0.90      0.94       114\n",
      "          30       0.84      0.90      0.87        83\n",
      "          31       0.67      0.58      0.62       104\n",
      "          32       0.85      0.80      0.82        91\n",
      "          33       0.75      0.77      0.76        98\n",
      "          34       0.90      0.92      0.91       107\n",
      "          35       0.92      0.94      0.93       102\n",
      "          36       0.66      0.60      0.63        95\n",
      "          37       0.76      0.80      0.78        98\n",
      "          38       0.68      0.66      0.67        96\n",
      "          39       0.75      0.81      0.78       102\n",
      "          40       0.88      0.87      0.87       104\n",
      "          41       0.72      0.64      0.68        89\n",
      "          42       0.92      0.85      0.88        93\n",
      "          43       0.79      0.77      0.78        95\n",
      "          44       0.79      0.74      0.77       101\n",
      "          45       0.88      0.93      0.90        97\n",
      "          46       0.99      0.87      0.93        95\n",
      "          47       0.81      0.72      0.76        95\n",
      "          48       0.86      0.72      0.78       100\n",
      "          49       0.81      0.78      0.79        98\n",
      "          50       0.49      0.56      0.52        99\n",
      "          51       1.00      1.00      1.00        50\n",
      "          52       0.98      0.98      0.98        82\n",
      "          53       0.87      0.73      0.80       113\n",
      "\n",
      "    accuracy                           0.82      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.82      0.82      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 93,  accuracy score is 0.8191087613293051\n",
      "at random state 93, confusion matrix is [[79  0  0 ...  0  0  0]\n",
      " [ 0 80  0 ...  0  0  1]\n",
      " [ 0  0 91 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 44  0  0]\n",
      " [ 0  0  0 ...  1 69  0]\n",
      " [ 0  3  1 ...  0  0 68]]\n",
      "at random state 93, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.86      0.80        92\n",
      "           1       0.71      0.76      0.74       105\n",
      "           2       0.83      0.95      0.88        96\n",
      "           3       0.87      0.74      0.80        99\n",
      "           4       0.86      0.91      0.88        99\n",
      "           5       0.73      0.80      0.76        97\n",
      "           6       0.93      0.97      0.95        89\n",
      "           7       0.80      0.85      0.83       109\n",
      "           8       0.96      0.94      0.95       101\n",
      "           9       0.66      0.85      0.74        87\n",
      "          10       0.58      0.71      0.64        93\n",
      "          11       0.82      0.90      0.86       114\n",
      "          12       0.90      0.92      0.91       106\n",
      "          13       0.75      0.77      0.76        96\n",
      "          14       0.89      0.88      0.89       106\n",
      "          15       0.98      0.98      0.98        88\n",
      "          16       0.89      0.90      0.89       105\n",
      "          17       0.93      0.80      0.86       104\n",
      "          18       0.80      0.83      0.82       102\n",
      "          19       0.79      0.79      0.79       104\n",
      "          20       0.88      0.84      0.86        95\n",
      "          21       0.60      0.70      0.64        96\n",
      "          22       0.97      0.95      0.96       103\n",
      "          23       0.75      0.81      0.78       101\n",
      "          24       0.60      0.58      0.59        93\n",
      "          25       0.96      0.96      0.96       117\n",
      "          26       0.74      0.79      0.76       112\n",
      "          27       0.87      0.82      0.84       118\n",
      "          28       0.98      0.91      0.94        88\n",
      "          29       0.93      0.97      0.95       104\n",
      "          30       0.87      0.88      0.87        89\n",
      "          31       0.61      0.70      0.65        99\n",
      "          32       0.89      0.77      0.82        86\n",
      "          33       0.73      0.76      0.74        99\n",
      "          34       0.89      0.88      0.88       104\n",
      "          35       0.92      0.94      0.93        89\n",
      "          36       0.75      0.71      0.73       105\n",
      "          37       0.72      0.76      0.74        96\n",
      "          38       0.76      0.79      0.77        95\n",
      "          39       0.76      0.76      0.76        90\n",
      "          40       0.81      0.84      0.83       108\n",
      "          41       0.77      0.71      0.74        98\n",
      "          42       0.87      0.89      0.88        93\n",
      "          43       0.84      0.70      0.76       101\n",
      "          44       0.87      0.80      0.84       117\n",
      "          45       0.94      0.97      0.95        86\n",
      "          46       0.95      0.88      0.91        88\n",
      "          47       0.76      0.80      0.78        91\n",
      "          48       0.91      0.59      0.72       108\n",
      "          49       0.88      0.78      0.83       114\n",
      "          50       0.60      0.44      0.51       107\n",
      "          51       0.98      1.00      0.99        44\n",
      "          52       0.97      0.93      0.95        74\n",
      "          53       0.82      0.71      0.76        96\n",
      "\n",
      "    accuracy                           0.82      5296\n",
      "   macro avg       0.83      0.82      0.82      5296\n",
      "weighted avg       0.82      0.82      0.82      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 94,  accuracy score is 0.8183534743202417\n",
      "at random state 94, confusion matrix is [[ 97   0   0 ...   0   0   0]\n",
      " [  0  69   0 ...   0   0   2]\n",
      " [  0   0 100 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  46   0   0]\n",
      " [  0   0   0 ...   0  61   0]\n",
      " [  0   0   1 ...   0   0  80]]\n",
      "at random state 94, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.92      0.86       105\n",
      "           1       0.70      0.77      0.73        90\n",
      "           2       0.87      0.93      0.90       107\n",
      "           3       0.82      0.77      0.79       100\n",
      "           4       0.88      0.97      0.92        99\n",
      "           5       0.79      0.68      0.73       114\n",
      "           6       0.90      0.96      0.93        98\n",
      "           7       0.76      0.87      0.81        95\n",
      "           8       0.95      0.87      0.91       117\n",
      "           9       0.66      0.85      0.74        86\n",
      "          10       0.59      0.77      0.67       103\n",
      "          11       0.82      0.92      0.87        92\n",
      "          12       0.84      0.83      0.84        98\n",
      "          13       0.73      0.67      0.70       109\n",
      "          14       0.91      0.84      0.88       114\n",
      "          15       0.95      0.94      0.94        81\n",
      "          16       0.89      0.90      0.90       103\n",
      "          17       0.96      0.89      0.92       103\n",
      "          18       0.81      0.81      0.81        89\n",
      "          19       0.79      0.79      0.79       113\n",
      "          20       0.92      0.83      0.87        96\n",
      "          21       0.61      0.66      0.64       101\n",
      "          22       0.96      0.96      0.96       113\n",
      "          23       0.72      0.82      0.76        89\n",
      "          24       0.60      0.60      0.60       103\n",
      "          25       0.90      0.94      0.92        86\n",
      "          26       0.76      0.76      0.76        99\n",
      "          27       0.84      0.84      0.84       110\n",
      "          28       0.96      0.96      0.96        99\n",
      "          29       0.96      0.96      0.96        89\n",
      "          30       0.94      0.89      0.92        94\n",
      "          31       0.46      0.66      0.54        79\n",
      "          32       0.91      0.91      0.91        95\n",
      "          33       0.75      0.74      0.75       101\n",
      "          34       0.93      0.86      0.90       111\n",
      "          35       0.92      0.93      0.92       111\n",
      "          36       0.69      0.72      0.70        88\n",
      "          37       0.74      0.76      0.75       100\n",
      "          38       0.82      0.70      0.76        98\n",
      "          39       0.89      0.77      0.83        97\n",
      "          40       0.79      0.83      0.81        96\n",
      "          41       0.72      0.73      0.72       102\n",
      "          42       0.88      0.88      0.88        96\n",
      "          43       0.92      0.66      0.77       122\n",
      "          44       0.83      0.85      0.84       114\n",
      "          45       0.97      0.93      0.95       101\n",
      "          46       0.94      0.92      0.93        85\n",
      "          47       0.84      0.70      0.76       103\n",
      "          48       0.86      0.70      0.77        90\n",
      "          49       0.81      0.85      0.83       101\n",
      "          50       0.48      0.37      0.42       100\n",
      "          51       1.00      1.00      1.00        46\n",
      "          52       0.98      0.94      0.96        65\n",
      "          53       0.79      0.80      0.80       100\n",
      "\n",
      "    accuracy                           0.82      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.82      0.82      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 95,  accuracy score is 0.8134441087613293\n",
      "at random state 95, confusion matrix is [[102   0   0 ...   0   0   0]\n",
      " [  0  79   0 ...   0   0   1]\n",
      " [  0   0 109 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  47   0   0]\n",
      " [  0   0   0 ...   0  79   0]\n",
      " [  0   0   1 ...   0   0  69]]\n",
      "at random state 95, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.95      0.90       107\n",
      "           1       0.65      0.72      0.68       110\n",
      "           2       0.88      0.97      0.92       112\n",
      "           3       0.81      0.70      0.75        98\n",
      "           4       0.85      0.95      0.89        98\n",
      "           5       0.78      0.76      0.77       103\n",
      "           6       0.92      0.97      0.94        86\n",
      "           7       0.75      0.81      0.78        94\n",
      "           8       0.91      0.93      0.92        87\n",
      "           9       0.70      0.73      0.71       102\n",
      "          10       0.63      0.71      0.67       100\n",
      "          11       0.78      0.86      0.82       109\n",
      "          12       0.95      0.87      0.91       123\n",
      "          13       0.61      0.74      0.67        88\n",
      "          14       0.89      0.91      0.90        95\n",
      "          15       0.91      0.93      0.92        84\n",
      "          16       0.91      0.88      0.89        99\n",
      "          17       0.92      0.83      0.88       102\n",
      "          18       0.87      0.83      0.85        96\n",
      "          19       0.78      0.81      0.79       107\n",
      "          20       0.88      0.85      0.87        99\n",
      "          21       0.69      0.72      0.71       109\n",
      "          22       0.96      0.91      0.93       106\n",
      "          23       0.78      0.78      0.78       107\n",
      "          24       0.58      0.60      0.59        94\n",
      "          25       0.86      0.96      0.91        93\n",
      "          26       0.72      0.76      0.74        83\n",
      "          27       0.89      0.83      0.86       102\n",
      "          28       1.00      0.92      0.96       104\n",
      "          29       0.99      0.90      0.94        79\n",
      "          30       0.94      0.84      0.89       103\n",
      "          31       0.48      0.59      0.53        99\n",
      "          32       0.88      0.82      0.85        97\n",
      "          33       0.68      0.75      0.71        99\n",
      "          34       0.88      0.82      0.85        95\n",
      "          35       0.86      0.98      0.91        91\n",
      "          36       0.81      0.67      0.73       106\n",
      "          37       0.76      0.78      0.77       101\n",
      "          38       0.78      0.78      0.78       107\n",
      "          39       0.77      0.80      0.79       106\n",
      "          40       0.80      0.86      0.83       112\n",
      "          41       0.78      0.71      0.74        99\n",
      "          42       0.88      0.84      0.86        87\n",
      "          43       0.85      0.78      0.81        98\n",
      "          44       0.83      0.82      0.82        98\n",
      "          45       0.95      0.95      0.95        81\n",
      "          46       0.95      0.88      0.91        90\n",
      "          47       0.76      0.78      0.77        99\n",
      "          48       0.84      0.72      0.77       100\n",
      "          49       0.86      0.88      0.87       105\n",
      "          50       0.57      0.43      0.49       119\n",
      "          51       1.00      1.00      1.00        47\n",
      "          52       0.98      0.96      0.97        82\n",
      "          53       0.78      0.70      0.74        99\n",
      "\n",
      "    accuracy                           0.81      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.81      0.81      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 96,  accuracy score is 0.8219410876132931\n",
      "at random state 96, confusion matrix is [[99  0  0 ...  0  0  0]\n",
      " [ 0 70  0 ...  0  0  0]\n",
      " [ 0  0 96 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 47  0  0]\n",
      " [ 0  0  0 ...  0 77  0]\n",
      " [ 0  0  0 ...  0  0 73]]\n",
      "at random state 96, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.91      0.86       109\n",
      "           1       0.68      0.71      0.70        98\n",
      "           2       0.88      0.92      0.90       104\n",
      "           3       0.82      0.76      0.79       109\n",
      "           4       0.89      0.91      0.90       102\n",
      "           5       0.73      0.68      0.70        99\n",
      "           6       0.95      0.99      0.97        82\n",
      "           7       0.77      0.85      0.80        97\n",
      "           8       0.94      0.97      0.95        97\n",
      "           9       0.73      0.79      0.76       101\n",
      "          10       0.67      0.72      0.69       107\n",
      "          11       0.88      0.91      0.89       116\n",
      "          12       0.94      0.91      0.93       111\n",
      "          13       0.68      0.70      0.69        97\n",
      "          14       0.93      0.94      0.94        86\n",
      "          15       1.00      0.93      0.96        82\n",
      "          16       0.96      0.90      0.93       103\n",
      "          17       0.91      0.88      0.90       103\n",
      "          18       0.85      0.88      0.87       106\n",
      "          19       0.72      0.81      0.76       104\n",
      "          20       0.87      0.88      0.87        95\n",
      "          21       0.66      0.78      0.72        99\n",
      "          22       0.96      0.98      0.97        83\n",
      "          23       0.75      0.76      0.75        99\n",
      "          24       0.63      0.56      0.59        98\n",
      "          25       0.89      0.98      0.93        96\n",
      "          26       0.68      0.76      0.72        91\n",
      "          27       0.87      0.87      0.87        87\n",
      "          28       0.96      0.92      0.94       102\n",
      "          29       0.96      0.92      0.94        96\n",
      "          30       0.90      0.88      0.89       108\n",
      "          31       0.53      0.65      0.58        96\n",
      "          32       0.90      0.74      0.81       103\n",
      "          33       0.67      0.79      0.73        99\n",
      "          34       0.88      0.96      0.92        92\n",
      "          35       0.93      0.96      0.94       112\n",
      "          36       0.81      0.74      0.77        97\n",
      "          37       0.78      0.77      0.78        92\n",
      "          38       0.76      0.68      0.72        99\n",
      "          39       0.82      0.79      0.81       113\n",
      "          40       0.78      0.89      0.83       101\n",
      "          41       0.77      0.69      0.73       110\n",
      "          42       0.89      0.88      0.88        80\n",
      "          43       0.82      0.77      0.80       101\n",
      "          44       0.87      0.81      0.84       110\n",
      "          45       0.96      0.91      0.93       100\n",
      "          46       0.92      0.93      0.92        96\n",
      "          47       0.81      0.71      0.76       112\n",
      "          48       0.88      0.67      0.76       102\n",
      "          49       0.74      0.80      0.77        91\n",
      "          50       0.48      0.37      0.42       105\n",
      "          51       1.00      1.00      1.00        47\n",
      "          52       0.97      0.95      0.96        81\n",
      "          53       0.81      0.81      0.81        90\n",
      "\n",
      "    accuracy                           0.82      5296\n",
      "   macro avg       0.83      0.83      0.83      5296\n",
      "weighted avg       0.82      0.82      0.82      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 97,  accuracy score is 0.8181646525679759\n",
      "at random state 97, confusion matrix is [[91  0  0 ...  0  0  0]\n",
      " [ 0 79  0 ...  0  0  2]\n",
      " [ 0  0 92 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 59  0  0]\n",
      " [ 0  0  0 ...  1 83  0]\n",
      " [ 0  0  1 ...  0  0 77]]\n",
      "at random state 97, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.94      0.84        97\n",
      "           1       0.71      0.74      0.72       107\n",
      "           2       0.85      0.92      0.88       100\n",
      "           3       0.84      0.76      0.79       107\n",
      "           4       0.82      0.93      0.87        99\n",
      "           5       0.81      0.71      0.76       111\n",
      "           6       0.95      0.92      0.93        77\n",
      "           7       0.73      0.85      0.78       110\n",
      "           8       0.91      0.92      0.91        93\n",
      "           9       0.69      0.85      0.77        82\n",
      "          10       0.74      0.71      0.72        94\n",
      "          11       0.83      0.90      0.87       118\n",
      "          12       0.96      0.82      0.88       114\n",
      "          13       0.72      0.74      0.73        99\n",
      "          14       0.85      0.90      0.87        88\n",
      "          15       0.93      0.90      0.91        97\n",
      "          16       0.92      0.91      0.91       107\n",
      "          17       0.90      0.86      0.88       111\n",
      "          18       0.81      0.85      0.83       101\n",
      "          19       0.85      0.88      0.86        94\n",
      "          20       0.85      0.81      0.83       103\n",
      "          21       0.62      0.68      0.65       103\n",
      "          22       0.96      0.95      0.95        95\n",
      "          23       0.80      0.78      0.79       103\n",
      "          24       0.57      0.61      0.59        88\n",
      "          25       0.81      0.96      0.88        91\n",
      "          26       0.71      0.82      0.76        89\n",
      "          27       0.88      0.83      0.86       109\n",
      "          28       1.00      0.86      0.93       103\n",
      "          29       0.94      0.90      0.92       104\n",
      "          30       0.92      0.83      0.87        99\n",
      "          31       0.60      0.64      0.62       106\n",
      "          32       0.87      0.82      0.84        88\n",
      "          33       0.75      0.81      0.78       115\n",
      "          34       0.86      0.90      0.88        91\n",
      "          35       0.88      0.95      0.91        96\n",
      "          36       0.73      0.74      0.73        95\n",
      "          37       0.72      0.72      0.72        95\n",
      "          38       0.82      0.70      0.76        98\n",
      "          39       0.83      0.85      0.84        88\n",
      "          40       0.77      0.83      0.80       106\n",
      "          41       0.71      0.56      0.63       112\n",
      "          42       0.89      0.81      0.85        84\n",
      "          43       0.88      0.77      0.82        94\n",
      "          44       0.83      0.87      0.85       111\n",
      "          45       0.97      0.95      0.96        95\n",
      "          46       0.95      0.95      0.95        88\n",
      "          47       0.78      0.82      0.80        90\n",
      "          48       0.86      0.72      0.78        98\n",
      "          49       0.85      0.83      0.84       106\n",
      "          50       0.53      0.38      0.44        99\n",
      "          51       0.98      1.00      0.99        59\n",
      "          52       0.93      0.93      0.93        89\n",
      "          53       0.79      0.77      0.78       100\n",
      "\n",
      "    accuracy                           0.82      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.82      0.82      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 98,  accuracy score is 0.8174093655589124\n",
      "at random state 98, confusion matrix is [[ 84   0   0 ...   0   0   0]\n",
      " [  0  70   0 ...   0   0   0]\n",
      " [  0   0 102 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  61   0   0]\n",
      " [  0   0   0 ...   0  78   0]\n",
      " [  0   1   2 ...   0   0  70]]\n",
      "at random state 98, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.94      0.84        89\n",
      "           1       0.62      0.80      0.70        87\n",
      "           2       0.88      0.94      0.91       108\n",
      "           3       0.85      0.75      0.79       103\n",
      "           4       0.90      0.88      0.89       120\n",
      "           5       0.77      0.75      0.76        99\n",
      "           6       0.90      0.91      0.91        81\n",
      "           7       0.74      0.71      0.73       110\n",
      "           8       0.95      0.95      0.95        93\n",
      "           9       0.66      0.71      0.69        97\n",
      "          10       0.71      0.76      0.73        99\n",
      "          11       0.85      0.89      0.87        92\n",
      "          12       0.91      0.90      0.91       107\n",
      "          13       0.66      0.75      0.71        81\n",
      "          14       0.88      0.93      0.90       108\n",
      "          15       0.96      0.98      0.97        88\n",
      "          16       0.92      0.87      0.89       112\n",
      "          17       0.91      0.87      0.89        93\n",
      "          18       0.83      0.89      0.86        96\n",
      "          19       0.83      0.77      0.80       131\n",
      "          20       0.85      0.84      0.85        98\n",
      "          21       0.64      0.74      0.69        88\n",
      "          22       0.94      0.95      0.94        93\n",
      "          23       0.77      0.80      0.78       114\n",
      "          24       0.62      0.61      0.61       103\n",
      "          25       0.89      0.98      0.93        87\n",
      "          26       0.72      0.76      0.74       106\n",
      "          27       0.81      0.76      0.78        96\n",
      "          28       0.97      0.92      0.94        96\n",
      "          29       0.98      0.97      0.98       110\n",
      "          30       0.87      0.91      0.89       106\n",
      "          31       0.53      0.63      0.57        86\n",
      "          32       0.91      0.88      0.90       104\n",
      "          33       0.76      0.73      0.75       101\n",
      "          34       0.89      0.87      0.88       109\n",
      "          35       0.91      0.93      0.92        85\n",
      "          36       0.78      0.78      0.78        88\n",
      "          37       0.66      0.74      0.70       102\n",
      "          38       0.79      0.77      0.78       105\n",
      "          39       0.81      0.73      0.77       104\n",
      "          40       0.84      0.77      0.80        96\n",
      "          41       0.76      0.71      0.74        98\n",
      "          42       0.90      0.92      0.91        95\n",
      "          43       0.89      0.73      0.80       105\n",
      "          44       0.80      0.84      0.82        97\n",
      "          45       0.98      0.91      0.94        97\n",
      "          46       0.92      0.90      0.91        84\n",
      "          47       0.67      0.70      0.68        89\n",
      "          48       0.86      0.66      0.75       110\n",
      "          49       0.84      0.84      0.84       110\n",
      "          50       0.52      0.40      0.45       102\n",
      "          51       1.00      1.00      1.00        61\n",
      "          52       0.95      0.94      0.95        83\n",
      "          53       0.78      0.74      0.76        94\n",
      "\n",
      "    accuracy                           0.82      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.82      0.82      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 99,  accuracy score is 0.8168429003021148\n",
      "at random state 99, confusion matrix is [[92  0  0 ...  0  0  0]\n",
      " [ 0 74  0 ...  0  0  0]\n",
      " [ 0  0 75 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 55  0  0]\n",
      " [ 0  0  0 ...  0 84  0]\n",
      " [ 0  0  0 ...  0  0 79]]\n",
      "at random state 99, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.90      0.83       102\n",
      "           1       0.63      0.74      0.68       100\n",
      "           2       0.88      0.95      0.91        79\n",
      "           3       0.87      0.84      0.85        97\n",
      "           4       0.90      0.85      0.88       109\n",
      "           5       0.78      0.85      0.81        92\n",
      "           6       0.94      0.98      0.96        91\n",
      "           7       0.75      0.79      0.77        90\n",
      "           8       0.96      0.90      0.93       112\n",
      "           9       0.77      0.77      0.77       101\n",
      "          10       0.59      0.73      0.65        95\n",
      "          11       0.87      0.89      0.88       105\n",
      "          12       0.90      0.89      0.90       107\n",
      "          13       0.66      0.72      0.69        98\n",
      "          14       0.93      0.90      0.92       109\n",
      "          15       0.95      0.96      0.96        81\n",
      "          16       0.91      0.91      0.91        91\n",
      "          17       0.90      0.85      0.87       104\n",
      "          18       0.80      0.88      0.84        94\n",
      "          19       0.76      0.86      0.81       102\n",
      "          20       0.88      0.83      0.86       109\n",
      "          21       0.69      0.70      0.69       103\n",
      "          22       0.97      0.92      0.94        97\n",
      "          23       0.79      0.82      0.81        96\n",
      "          24       0.68      0.56      0.62       108\n",
      "          25       0.92      0.95      0.93       103\n",
      "          26       0.64      0.77      0.70        93\n",
      "          27       0.88      0.77      0.82       108\n",
      "          28       0.99      0.95      0.97        82\n",
      "          29       0.94      0.95      0.94        96\n",
      "          30       0.86      0.86      0.86        98\n",
      "          31       0.48      0.66      0.55        82\n",
      "          32       0.80      0.79      0.80        94\n",
      "          33       0.71      0.72      0.72       107\n",
      "          34       0.95      0.85      0.90       114\n",
      "          35       0.91      0.95      0.93       111\n",
      "          36       0.71      0.67      0.69       105\n",
      "          37       0.71      0.82      0.76        88\n",
      "          38       0.82      0.75      0.79       105\n",
      "          39       0.74      0.73      0.74        93\n",
      "          40       0.85      0.81      0.83       105\n",
      "          41       0.76      0.74      0.75       109\n",
      "          42       0.87      0.89      0.88        83\n",
      "          43       0.76      0.71      0.73        92\n",
      "          44       0.82      0.76      0.79       118\n",
      "          45       0.91      0.93      0.92        87\n",
      "          46       0.95      0.89      0.91        97\n",
      "          47       0.83      0.74      0.79       113\n",
      "          48       0.80      0.61      0.70        93\n",
      "          49       0.86      0.83      0.84       105\n",
      "          50       0.57      0.48      0.52        96\n",
      "          51       1.00      1.00      1.00        55\n",
      "          52       1.00      0.99      0.99        85\n",
      "          53       0.83      0.74      0.78       107\n",
      "\n",
      "    accuracy                           0.82      5296\n",
      "   macro avg       0.82      0.82      0.82      5296\n",
      "weighted avg       0.82      0.82      0.82      5296\n",
      "\n",
      "\n",
      "\n",
      "Max accuracy at random state 22 = 0.8238293051359517\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn=KNeighborsClassifier(n_neighbors=5)\n",
    "model_selection(knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ea340b",
   "metadata": {},
   "source": [
    "CV Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "13d2e352",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ee6d6791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.68909276 0.63200815 0.55685875 0.47985722 0.58286588 0.57623661\n",
      " 0.54411015 0.45130036 0.54054054]\n",
      "0.5614300469761782\n",
      "0.06790327305823347\n"
     ]
    }
   ],
   "source": [
    "score=cross_val_score(knn,xx,yy,cv=9)\n",
    "print(score)\n",
    "print(score.mean())\n",
    "print(score.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894f5f04",
   "metadata": {},
   "source": [
    "## decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "22823422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 0,  accuracy score is 1.0\n",
      "at random state 0, confusion matrix is [[77  0  0 ...  0  0  0]\n",
      " [ 0 74  0 ...  0  0  2]\n",
      " [ 0  0 90 ...  0  0  1]\n",
      " ...\n",
      " [ 0  0  0 ... 48  0  0]\n",
      " [ 0  0  0 ...  0 73  0]\n",
      " [ 0  0  0 ...  0  0 58]]\n",
      "at random state 0, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.83      0.80        93\n",
      "           1       0.78      0.73      0.75       102\n",
      "           2       0.90      0.90      0.90       100\n",
      "           3       0.75      0.74      0.75       104\n",
      "           4       0.87      0.88      0.88       104\n",
      "           5       0.68      0.70      0.69        97\n",
      "           6       0.96      0.96      0.96        84\n",
      "           7       0.77      0.83      0.80       106\n",
      "           8       0.90      0.93      0.92       119\n",
      "           9       0.84      0.87      0.85        93\n",
      "          10       0.70      0.74      0.72       113\n",
      "          11       0.80      0.81      0.80       104\n",
      "          12       0.85      0.90      0.87        97\n",
      "          13       0.71      0.70      0.70        89\n",
      "          14       0.83      0.75      0.79        93\n",
      "          15       0.89      0.94      0.91        79\n",
      "          16       0.81      0.88      0.84        98\n",
      "          17       0.76      0.82      0.79        95\n",
      "          18       0.81      0.81      0.81        97\n",
      "          19       0.76      0.78      0.77        87\n",
      "          20       0.83      0.83      0.83       100\n",
      "          21       0.71      0.59      0.65       105\n",
      "          22       0.88      0.92      0.90       102\n",
      "          23       0.74      0.70      0.72       100\n",
      "          24       0.56      0.56      0.56       112\n",
      "          25       0.91      0.93      0.92        97\n",
      "          26       0.61      0.66      0.63       104\n",
      "          27       0.80      0.69      0.74        96\n",
      "          28       0.86      0.86      0.86        94\n",
      "          29       0.90      0.86      0.88       100\n",
      "          30       0.91      0.88      0.89       105\n",
      "          31       0.61      0.57      0.59       105\n",
      "          32       0.80      0.72      0.76       109\n",
      "          33       0.74      0.74      0.74       100\n",
      "          34       0.76      0.78      0.77       111\n",
      "          35       0.83      0.88      0.86        91\n",
      "          36       0.63      0.64      0.64       104\n",
      "          37       0.71      0.73      0.72        99\n",
      "          38       0.78      0.75      0.77       116\n",
      "          39       0.79      0.70      0.74       105\n",
      "          40       0.80      0.77      0.79       112\n",
      "          41       0.71      0.69      0.70       102\n",
      "          42       0.90      0.88      0.89        78\n",
      "          43       0.72      0.70      0.71        93\n",
      "          44       0.72      0.69      0.70       100\n",
      "          45       0.94      0.98      0.96        85\n",
      "          46       0.92      0.91      0.91        96\n",
      "          47       0.78      0.72      0.74       102\n",
      "          48       0.60      0.69      0.64        98\n",
      "          49       0.77      0.78      0.78        97\n",
      "          50       0.49      0.51      0.50       109\n",
      "          51       0.94      0.98      0.96        49\n",
      "          52       0.95      0.94      0.94        78\n",
      "          53       0.63      0.66      0.64        88\n",
      "\n",
      "    accuracy                           0.78      5296\n",
      "   macro avg       0.78      0.79      0.78      5296\n",
      "weighted avg       0.78      0.78      0.78      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 1,  accuracy score is 1.0\n",
      "at random state 1, confusion matrix is [[ 73   0   0 ...   0   0   0]\n",
      " [  0  64   0 ...   0   0   5]\n",
      " [  0   0 100 ...   0   0   4]\n",
      " ...\n",
      " [  0   0   0 ...  46   0   0]\n",
      " [  0   0   0 ...   0  61   0]\n",
      " [  0   2   0 ...   0   0  76]]\n",
      "at random state 1, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.74      0.73        98\n",
      "           1       0.70      0.59      0.64       108\n",
      "           2       0.88      0.82      0.85       122\n",
      "           3       0.86      0.87      0.86        92\n",
      "           4       0.90      0.82      0.86       114\n",
      "           5       0.63      0.69      0.66       107\n",
      "           6       0.97      0.93      0.95        91\n",
      "           7       0.79      0.69      0.73       102\n",
      "           8       0.91      0.94      0.92        98\n",
      "           9       0.89      0.83      0.86       102\n",
      "          10       0.66      0.67      0.66       114\n",
      "          11       0.84      0.88      0.86       104\n",
      "          12       0.92      0.82      0.87       118\n",
      "          13       0.72      0.75      0.73       101\n",
      "          14       0.81      0.74      0.77        96\n",
      "          15       0.89      0.92      0.91        78\n",
      "          16       0.83      0.86      0.85        96\n",
      "          17       0.79      0.81      0.80       106\n",
      "          18       0.84      0.84      0.84        97\n",
      "          19       0.85      0.83      0.84       106\n",
      "          20       0.84      0.89      0.86        90\n",
      "          21       0.69      0.71      0.70       101\n",
      "          22       0.90      0.96      0.93        97\n",
      "          23       0.81      0.83      0.82        99\n",
      "          24       0.63      0.66      0.65        94\n",
      "          25       0.89      0.97      0.93        96\n",
      "          26       0.66      0.71      0.69        97\n",
      "          27       0.80      0.79      0.79        98\n",
      "          28       0.85      0.82      0.84       102\n",
      "          29       0.90      0.90      0.90        84\n",
      "          30       0.90      0.88      0.89       104\n",
      "          31       0.53      0.65      0.58        96\n",
      "          32       0.70      0.73      0.72        94\n",
      "          33       0.78      0.78      0.78        96\n",
      "          34       0.80      0.72      0.76        99\n",
      "          35       0.87      0.91      0.89        99\n",
      "          36       0.69      0.75      0.72        97\n",
      "          37       0.64      0.70      0.67        99\n",
      "          38       0.77      0.78      0.78       114\n",
      "          39       0.87      0.91      0.89       113\n",
      "          40       0.86      0.82      0.84       109\n",
      "          41       0.79      0.71      0.75        96\n",
      "          42       0.89      0.89      0.89        90\n",
      "          43       0.74      0.66      0.70        98\n",
      "          44       0.74      0.76      0.75        85\n",
      "          45       0.96      0.93      0.94        70\n",
      "          46       0.89      0.89      0.89       101\n",
      "          47       0.69      0.70      0.69        94\n",
      "          48       0.69      0.71      0.70        90\n",
      "          49       0.74      0.71      0.73       110\n",
      "          50       0.55      0.45      0.50       115\n",
      "          51       1.00      1.00      1.00        46\n",
      "          52       0.92      0.91      0.92        67\n",
      "          53       0.63      0.72      0.67       106\n",
      "\n",
      "    accuracy                           0.79      5296\n",
      "   macro avg       0.80      0.80      0.80      5296\n",
      "weighted avg       0.79      0.79      0.79      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 2,  accuracy score is 1.0\n",
      "at random state 2, confusion matrix is [[87  0  0 ...  0  0  0]\n",
      " [ 0 61  0 ...  0  0  0]\n",
      " [ 0  0 85 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 43  0  0]\n",
      " [ 0  0  0 ...  1 77  0]\n",
      " [ 0  2  5 ...  0  0 60]]\n",
      "at random state 2, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.81      0.80       107\n",
      "           1       0.70      0.62      0.66        98\n",
      "           2       0.84      0.90      0.87        94\n",
      "           3       0.76      0.81      0.78        99\n",
      "           4       0.89      0.88      0.88       113\n",
      "           5       0.65      0.77      0.71        93\n",
      "           6       0.92      0.94      0.93        82\n",
      "           7       0.82      0.79      0.80       107\n",
      "           8       0.94      0.95      0.95        86\n",
      "           9       0.83      0.77      0.80        97\n",
      "          10       0.69      0.65      0.67       101\n",
      "          11       0.85      0.86      0.85       105\n",
      "          12       0.92      0.83      0.87       108\n",
      "          13       0.78      0.76      0.77       108\n",
      "          14       0.87      0.74      0.80       100\n",
      "          15       0.93      0.96      0.94        80\n",
      "          16       0.94      0.86      0.90       106\n",
      "          17       0.83      0.78      0.81       106\n",
      "          18       0.86      0.77      0.81        98\n",
      "          19       0.76      0.76      0.76        97\n",
      "          20       0.87      0.86      0.86       104\n",
      "          21       0.70      0.66      0.68       104\n",
      "          22       0.91      0.86      0.88        90\n",
      "          23       0.72      0.70      0.71       110\n",
      "          24       0.63      0.59      0.61       104\n",
      "          25       0.92      0.96      0.94       117\n",
      "          26       0.56      0.72      0.63        75\n",
      "          27       0.83      0.74      0.78       119\n",
      "          28       0.84      0.78      0.81        97\n",
      "          29       0.91      0.90      0.91       101\n",
      "          30       0.88      0.90      0.89       111\n",
      "          31       0.65      0.65      0.65       104\n",
      "          32       0.79      0.81      0.80       100\n",
      "          33       0.67      0.89      0.76        91\n",
      "          34       0.67      0.69      0.68        99\n",
      "          35       0.77      0.79      0.78        90\n",
      "          36       0.68      0.63      0.65       107\n",
      "          37       0.68      0.73      0.70       103\n",
      "          38       0.77      0.87      0.81       104\n",
      "          39       0.88      0.81      0.84       112\n",
      "          40       0.74      0.80      0.77        98\n",
      "          41       0.81      0.79      0.80       104\n",
      "          42       0.88      0.88      0.88        82\n",
      "          43       0.72      0.78      0.75        99\n",
      "          44       0.80      0.77      0.78        99\n",
      "          45       0.92      0.96      0.94        90\n",
      "          46       0.90      0.92      0.91        92\n",
      "          47       0.78      0.72      0.75        99\n",
      "          48       0.55      0.65      0.59        89\n",
      "          49       0.77      0.75      0.76       102\n",
      "          50       0.49      0.53      0.51        94\n",
      "          51       0.98      1.00      0.99        43\n",
      "          52       0.95      0.90      0.92        86\n",
      "          53       0.71      0.65      0.68        92\n",
      "\n",
      "    accuracy                           0.79      5296\n",
      "   macro avg       0.79      0.79      0.79      5296\n",
      "weighted avg       0.79      0.79      0.79      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 3,  accuracy score is 1.0\n",
      "at random state 3, confusion matrix is [[78  0  0 ...  0  0  0]\n",
      " [ 0 76  0 ...  0  0  0]\n",
      " [ 0  0 96 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 50  0  0]\n",
      " [ 0  0  0 ...  0 62  0]\n",
      " [ 0  2  3 ...  0  0 76]]\n",
      "at random state 3, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.77      0.79       101\n",
      "           1       0.82      0.71      0.76       107\n",
      "           2       0.91      0.89      0.90       108\n",
      "           3       0.84      0.85      0.85       107\n",
      "           4       0.90      0.90      0.90       103\n",
      "           5       0.66      0.78      0.71       108\n",
      "           6       0.98      1.00      0.99        79\n",
      "           7       0.84      0.83      0.83        82\n",
      "           8       0.93      0.92      0.93       108\n",
      "           9       0.74      0.87      0.80        98\n",
      "          10       0.61      0.71      0.65        99\n",
      "          11       0.83      0.83      0.83        87\n",
      "          12       0.87      0.84      0.85       104\n",
      "          13       0.67      0.84      0.75        91\n",
      "          14       0.76      0.73      0.74       101\n",
      "          15       0.94      0.91      0.93        89\n",
      "          16       0.84      0.88      0.86       105\n",
      "          17       0.88      0.85      0.86       100\n",
      "          18       0.88      0.81      0.84        95\n",
      "          19       0.80      0.72      0.76        94\n",
      "          20       0.82      0.81      0.81       105\n",
      "          21       0.62      0.62      0.62        97\n",
      "          22       0.87      0.95      0.90        95\n",
      "          23       0.77      0.75      0.76       106\n",
      "          24       0.59      0.61      0.60        97\n",
      "          25       0.91      0.96      0.94        84\n",
      "          26       0.68      0.64      0.66        98\n",
      "          27       0.77      0.84      0.80        99\n",
      "          28       0.91      0.85      0.88       113\n",
      "          29       0.89      0.94      0.91        93\n",
      "          30       0.87      0.84      0.86       107\n",
      "          31       0.63      0.64      0.63        99\n",
      "          32       0.80      0.70      0.75        98\n",
      "          33       0.74      0.74      0.74       113\n",
      "          34       0.76      0.73      0.75        86\n",
      "          35       0.89      0.90      0.90       103\n",
      "          36       0.77      0.76      0.76       108\n",
      "          37       0.80      0.83      0.81        95\n",
      "          38       0.74      0.77      0.75        91\n",
      "          39       0.81      0.84      0.83       109\n",
      "          40       0.83      0.80      0.81        95\n",
      "          41       0.82      0.76      0.79        99\n",
      "          42       0.95      0.93      0.94        84\n",
      "          43       0.77      0.77      0.77       107\n",
      "          44       0.75      0.78      0.77       101\n",
      "          45       0.93      0.89      0.91       102\n",
      "          46       0.92      0.92      0.92        92\n",
      "          47       0.75      0.72      0.74       104\n",
      "          48       0.79      0.72      0.75       109\n",
      "          49       0.85      0.76      0.80       111\n",
      "          50       0.47      0.45      0.46       110\n",
      "          51       1.00      1.00      1.00        50\n",
      "          52       0.95      0.87      0.91        71\n",
      "          53       0.69      0.77      0.73        99\n",
      "\n",
      "    accuracy                           0.80      5296\n",
      "   macro avg       0.81      0.81      0.81      5296\n",
      "weighted avg       0.80      0.80      0.80      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 4,  accuracy score is 1.0\n",
      "at random state 4, confusion matrix is [[63  0  0 ...  0  0  0]\n",
      " [ 0 69  0 ...  0  0  0]\n",
      " [ 0  0 97 ...  0  0  3]\n",
      " ...\n",
      " [ 0  0  0 ... 51  0  0]\n",
      " [ 0  0  0 ...  0 82  0]\n",
      " [ 0  2  3 ...  0  0 79]]\n",
      "at random state 4, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.66      0.74        95\n",
      "           1       0.70      0.73      0.72        94\n",
      "           2       0.87      0.93      0.90       104\n",
      "           3       0.83      0.77      0.80       111\n",
      "           4       0.84      0.89      0.87        94\n",
      "           5       0.71      0.73      0.72       113\n",
      "           6       0.92      0.94      0.93       100\n",
      "           7       0.76      0.72      0.74       100\n",
      "           8       0.92      0.88      0.90       101\n",
      "           9       0.80      0.72      0.76        97\n",
      "          10       0.69      0.70      0.69       107\n",
      "          11       0.87      0.84      0.86       106\n",
      "          12       0.82      0.76      0.79        91\n",
      "          13       0.73      0.77      0.75        99\n",
      "          14       0.78      0.75      0.77        85\n",
      "          15       0.99      0.95      0.97        82\n",
      "          16       0.88      0.85      0.87       107\n",
      "          17       0.80      0.85      0.82        94\n",
      "          18       0.81      0.82      0.81       110\n",
      "          19       0.81      0.81      0.81        90\n",
      "          20       0.87      0.92      0.89       108\n",
      "          21       0.65      0.65      0.65        98\n",
      "          22       0.89      0.88      0.89       101\n",
      "          23       0.79      0.84      0.81       100\n",
      "          24       0.63      0.63      0.63        87\n",
      "          25       0.91      0.98      0.95        98\n",
      "          26       0.66      0.65      0.66       103\n",
      "          27       0.83      0.75      0.79        96\n",
      "          28       0.88      0.82      0.85       105\n",
      "          29       0.89      0.87      0.88        82\n",
      "          30       0.88      0.91      0.89       110\n",
      "          31       0.65      0.66      0.65        93\n",
      "          32       0.86      0.66      0.75        96\n",
      "          33       0.74      0.81      0.77        90\n",
      "          34       0.81      0.81      0.81       112\n",
      "          35       0.84      0.88      0.86       104\n",
      "          36       0.70      0.70      0.70       108\n",
      "          37       0.68      0.76      0.72       101\n",
      "          38       0.75      0.73      0.74       103\n",
      "          39       0.80      0.81      0.81       102\n",
      "          40       0.86      0.86      0.86        93\n",
      "          41       0.74      0.77      0.75        95\n",
      "          42       0.89      0.89      0.89        95\n",
      "          43       0.67      0.65      0.66        94\n",
      "          44       0.75      0.73      0.74       115\n",
      "          45       0.96      0.90      0.93        84\n",
      "          46       0.89      0.92      0.90        84\n",
      "          47       0.75      0.77      0.76       105\n",
      "          48       0.72      0.68      0.70       104\n",
      "          49       0.69      0.74      0.72       105\n",
      "          50       0.45      0.52      0.48        91\n",
      "          51       0.98      0.96      0.97        53\n",
      "          52       0.94      0.94      0.94        87\n",
      "          53       0.65      0.69      0.67       114\n",
      "\n",
      "    accuracy                           0.79      5296\n",
      "   macro avg       0.80      0.79      0.79      5296\n",
      "weighted avg       0.79      0.79      0.79      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 5,  accuracy score is 1.0\n",
      "at random state 5, confusion matrix is [[74  0  0 ...  0  0  0]\n",
      " [ 0 73  0 ...  0  0  1]\n",
      " [ 0  0 87 ...  0  0  5]\n",
      " ...\n",
      " [ 0  0  0 ... 56  0  0]\n",
      " [ 0  0  0 ...  0 58  0]\n",
      " [ 0  0  1 ...  0  0 73]]\n",
      "at random state 5, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.80      0.80        92\n",
      "           1       0.68      0.72      0.70       102\n",
      "           2       0.86      0.81      0.84       107\n",
      "           3       0.84      0.77      0.80       105\n",
      "           4       0.81      0.87      0.84       109\n",
      "           5       0.72      0.67      0.69       102\n",
      "           6       0.93      0.94      0.94        87\n",
      "           7       0.78      0.81      0.79       102\n",
      "           8       0.95      0.97      0.96        97\n",
      "           9       0.79      0.88      0.83        85\n",
      "          10       0.64      0.69      0.66        98\n",
      "          11       0.85      0.84      0.85       107\n",
      "          12       0.81      0.94      0.87       106\n",
      "          13       0.80      0.69      0.74       104\n",
      "          14       0.84      0.76      0.80        99\n",
      "          15       0.91      0.96      0.93        73\n",
      "          16       0.87      0.82      0.85       114\n",
      "          17       0.81      0.81      0.81       100\n",
      "          18       0.78      0.78      0.78        98\n",
      "          19       0.75      0.77      0.76       103\n",
      "          20       0.86      0.85      0.85       100\n",
      "          21       0.59      0.51      0.55       107\n",
      "          22       0.91      0.94      0.93        87\n",
      "          23       0.70      0.81      0.75       102\n",
      "          24       0.61      0.65      0.63       101\n",
      "          25       0.92      0.91      0.92       105\n",
      "          26       0.69      0.59      0.64       113\n",
      "          27       0.79      0.82      0.80        98\n",
      "          28       0.84      0.81      0.83       102\n",
      "          29       0.85      0.90      0.88       103\n",
      "          30       0.80      0.89      0.84        99\n",
      "          31       0.74      0.72      0.73       108\n",
      "          32       0.67      0.66      0.66        91\n",
      "          33       0.79      0.84      0.81        97\n",
      "          34       0.80      0.79      0.80        97\n",
      "          35       0.89      0.77      0.83       110\n",
      "          36       0.68      0.69      0.69       106\n",
      "          37       0.77      0.80      0.78       109\n",
      "          38       0.80      0.79      0.80        96\n",
      "          39       0.81      0.80      0.81        86\n",
      "          40       0.88      0.81      0.84       109\n",
      "          41       0.74      0.74      0.74       105\n",
      "          42       0.89      0.90      0.89        97\n",
      "          43       0.63      0.69      0.66        93\n",
      "          44       0.76      0.73      0.74        88\n",
      "          45       0.93      0.90      0.92        93\n",
      "          46       0.89      0.93      0.91        90\n",
      "          47       0.81      0.68      0.74        91\n",
      "          48       0.58      0.65      0.61        93\n",
      "          49       0.74      0.70      0.72       105\n",
      "          50       0.55      0.61      0.58        93\n",
      "          51       1.00      1.00      1.00        56\n",
      "          52       0.92      0.83      0.87        70\n",
      "          53       0.72      0.69      0.71       106\n",
      "\n",
      "    accuracy                           0.79      5296\n",
      "   macro avg       0.79      0.79      0.79      5296\n",
      "weighted avg       0.79      0.79      0.79      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 6,  accuracy score is 1.0\n",
      "at random state 6, confusion matrix is [[ 72   0   0 ...   0   0   0]\n",
      " [  0  84   0 ...   0   0   1]\n",
      " [  0   0 106 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  55   0   0]\n",
      " [  0   0   0 ...   0  75   0]\n",
      " [  0   3   0 ...   0   0  68]]\n",
      "at random state 6, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.71      0.76       102\n",
      "           1       0.68      0.72      0.70       116\n",
      "           2       0.88      0.93      0.90       114\n",
      "           3       0.69      0.81      0.74        77\n",
      "           4       0.94      0.87      0.90        90\n",
      "           5       0.65      0.65      0.65        92\n",
      "           6       0.95      0.95      0.95        80\n",
      "           7       0.85      0.89      0.87       119\n",
      "           8       0.96      0.93      0.94       101\n",
      "           9       0.85      0.81      0.83       106\n",
      "          10       0.70      0.71      0.70       104\n",
      "          11       0.85      0.86      0.86       118\n",
      "          12       0.86      0.77      0.81        94\n",
      "          13       0.77      0.83      0.80        98\n",
      "          14       0.79      0.86      0.82        90\n",
      "          15       0.87      0.93      0.90        72\n",
      "          16       0.93      0.88      0.90       100\n",
      "          17       0.75      0.86      0.80        96\n",
      "          18       0.84      0.91      0.88       104\n",
      "          19       0.81      0.80      0.80        98\n",
      "          20       0.79      0.88      0.83       100\n",
      "          21       0.60      0.68      0.64       108\n",
      "          22       0.97      0.92      0.94        91\n",
      "          23       0.76      0.65      0.70       110\n",
      "          24       0.67      0.51      0.58       113\n",
      "          25       0.94      0.96      0.95        83\n",
      "          26       0.64      0.68      0.66        95\n",
      "          27       0.80      0.75      0.77        95\n",
      "          28       0.87      0.92      0.89        95\n",
      "          29       0.97      0.89      0.93        98\n",
      "          30       0.86      0.84      0.85        97\n",
      "          31       0.65      0.67      0.66       102\n",
      "          32       0.82      0.75      0.78        99\n",
      "          33       0.77      0.63      0.69       119\n",
      "          34       0.79      0.81      0.80       104\n",
      "          35       0.84      0.88      0.86        99\n",
      "          36       0.69      0.68      0.68       114\n",
      "          37       0.79      0.76      0.78       102\n",
      "          38       0.78      0.73      0.76       113\n",
      "          39       0.86      0.80      0.83        99\n",
      "          40       0.80      0.77      0.78        95\n",
      "          41       0.82      0.73      0.77       112\n",
      "          42       0.86      0.87      0.86        99\n",
      "          43       0.66      0.70      0.68        99\n",
      "          44       0.82      0.82      0.82        94\n",
      "          45       0.97      0.96      0.96        93\n",
      "          46       0.92      0.93      0.92        83\n",
      "          47       0.79      0.74      0.76       106\n",
      "          48       0.70      0.72      0.71        86\n",
      "          49       0.74      0.79      0.76       105\n",
      "          50       0.43      0.56      0.48        84\n",
      "          51       1.00      1.00      1.00        55\n",
      "          52       0.93      0.95      0.94        79\n",
      "          53       0.64      0.69      0.66        99\n",
      "\n",
      "    accuracy                           0.80      5296\n",
      "   macro avg       0.80      0.80      0.80      5296\n",
      "weighted avg       0.80      0.80      0.80      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 7,  accuracy score is 1.0\n",
      "at random state 7, confusion matrix is [[75  0  0 ...  0  0  0]\n",
      " [ 0 76  0 ...  0  0  3]\n",
      " [ 0  0 90 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 45  0  0]\n",
      " [ 0  0  0 ...  1 72  0]\n",
      " [ 0  2  1 ...  0  0 83]]\n",
      "at random state 7, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.77      0.74        97\n",
      "           1       0.69      0.68      0.68       112\n",
      "           2       0.92      0.83      0.87       108\n",
      "           3       0.86      0.79      0.82        97\n",
      "           4       0.86      0.92      0.89       104\n",
      "           5       0.75      0.63      0.68        94\n",
      "           6       0.96      0.92      0.94        85\n",
      "           7       0.79      0.79      0.79       102\n",
      "           8       0.93      0.95      0.94       110\n",
      "           9       0.80      0.81      0.80       114\n",
      "          10       0.63      0.62      0.62       104\n",
      "          11       0.89      0.87      0.88        95\n",
      "          12       0.86      0.86      0.86       116\n",
      "          13       0.66      0.68      0.67       103\n",
      "          14       0.76      0.72      0.74        97\n",
      "          15       0.95      0.95      0.95        80\n",
      "          16       0.86      0.86      0.86       104\n",
      "          17       0.78      0.86      0.82        85\n",
      "          18       0.89      0.89      0.89       109\n",
      "          19       0.81      0.75      0.78        99\n",
      "          20       0.80      0.88      0.84        97\n",
      "          21       0.67      0.63      0.65        97\n",
      "          22       0.93      0.93      0.93        99\n",
      "          23       0.73      0.82      0.77       103\n",
      "          24       0.63      0.62      0.63        92\n",
      "          25       0.93      0.93      0.93        98\n",
      "          26       0.58      0.59      0.58       100\n",
      "          27       0.74      0.84      0.79        89\n",
      "          28       0.84      0.86      0.85       101\n",
      "          29       0.92      0.86      0.89        88\n",
      "          30       0.93      0.90      0.91       107\n",
      "          31       0.71      0.70      0.70        93\n",
      "          32       0.81      0.79      0.80       105\n",
      "          33       0.67      0.78      0.72        85\n",
      "          34       0.84      0.75      0.80       122\n",
      "          35       0.84      0.84      0.84       110\n",
      "          36       0.60      0.70      0.65        89\n",
      "          37       0.69      0.78      0.73       104\n",
      "          38       0.71      0.72      0.72        98\n",
      "          39       0.77      0.74      0.76        98\n",
      "          40       0.85      0.88      0.87        95\n",
      "          41       0.76      0.68      0.71        96\n",
      "          42       0.91      0.87      0.89        83\n",
      "          43       0.71      0.70      0.71       104\n",
      "          44       0.76      0.74      0.75       106\n",
      "          45       0.95      0.95      0.95        92\n",
      "          46       0.90      0.89      0.90       102\n",
      "          47       0.75      0.66      0.70        95\n",
      "          48       0.70      0.76      0.73        91\n",
      "          49       0.73      0.80      0.77        96\n",
      "          50       0.53      0.48      0.50       104\n",
      "          51       0.98      1.00      0.99        45\n",
      "          52       0.91      0.90      0.91        80\n",
      "          53       0.72      0.71      0.72       117\n",
      "\n",
      "    accuracy                           0.79      5296\n",
      "   macro avg       0.79      0.79      0.79      5296\n",
      "weighted avg       0.79      0.79      0.79      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 8,  accuracy score is 1.0\n",
      "at random state 8, confusion matrix is [[66  0  0 ...  0  0  0]\n",
      " [ 0 85  0 ...  0  0  2]\n",
      " [ 0  0 92 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 61  0  0]\n",
      " [ 0  0  0 ...  0 72  0]\n",
      " [ 0  0  6 ...  0  0 75]]\n",
      "at random state 8, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.78      0.78        85\n",
      "           1       0.79      0.71      0.75       119\n",
      "           2       0.81      0.93      0.87        99\n",
      "           3       0.84      0.81      0.82       100\n",
      "           4       0.87      0.91      0.89       119\n",
      "           5       0.68      0.66      0.67        96\n",
      "           6       0.97      0.96      0.96        92\n",
      "           7       0.81      0.81      0.81        97\n",
      "           8       0.89      0.89      0.89        98\n",
      "           9       0.83      0.79      0.81        92\n",
      "          10       0.76      0.69      0.72       107\n",
      "          11       0.83      0.81      0.82       109\n",
      "          12       0.84      0.80      0.82        86\n",
      "          13       0.73      0.70      0.72       101\n",
      "          14       0.76      0.76      0.76       100\n",
      "          15       0.92      0.93      0.92        84\n",
      "          16       0.91      0.79      0.85       114\n",
      "          17       0.82      0.82      0.82       104\n",
      "          18       0.78      0.81      0.80        99\n",
      "          19       0.80      0.80      0.80       111\n",
      "          20       0.88      0.80      0.84        87\n",
      "          21       0.69      0.66      0.68        98\n",
      "          22       0.86      0.87      0.87       103\n",
      "          23       0.81      0.83      0.82        95\n",
      "          24       0.67      0.57      0.62       110\n",
      "          25       0.97      0.82      0.89       114\n",
      "          26       0.62      0.68      0.65        97\n",
      "          27       0.80      0.82      0.81       105\n",
      "          28       0.89      0.76      0.82       113\n",
      "          29       0.86      0.88      0.87       101\n",
      "          30       0.90      0.87      0.89        98\n",
      "          31       0.54      0.59      0.56        87\n",
      "          32       0.79      0.84      0.81        88\n",
      "          33       0.70      0.71      0.71        94\n",
      "          34       0.80      0.77      0.78        95\n",
      "          35       0.83      0.92      0.88        92\n",
      "          36       0.66      0.65      0.65       112\n",
      "          37       0.76      0.75      0.76       109\n",
      "          38       0.78      0.89      0.83       102\n",
      "          39       0.81      0.90      0.85       101\n",
      "          40       0.87      0.78      0.82       108\n",
      "          41       0.75      0.77      0.76       108\n",
      "          42       0.90      0.91      0.90        95\n",
      "          43       0.69      0.80      0.74        99\n",
      "          44       0.78      0.73      0.75       109\n",
      "          45       0.92      0.95      0.93        82\n",
      "          46       0.86      0.94      0.90        71\n",
      "          47       0.82      0.75      0.79        93\n",
      "          48       0.73      0.74      0.73        91\n",
      "          49       0.74      0.77      0.76        92\n",
      "          50       0.46      0.60      0.52        87\n",
      "          51       0.97      1.00      0.98        61\n",
      "          52       0.95      0.91      0.93        79\n",
      "          53       0.68      0.69      0.69       108\n",
      "\n",
      "    accuracy                           0.79      5296\n",
      "   macro avg       0.80      0.80      0.80      5296\n",
      "weighted avg       0.80      0.79      0.79      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 9,  accuracy score is 1.0\n",
      "at random state 9, confusion matrix is [[66  0  0 ...  0  0  0]\n",
      " [ 0 65  0 ...  0  0  4]\n",
      " [ 0  0 92 ...  0  0  3]\n",
      " ...\n",
      " [ 0  0  0 ... 47  0  0]\n",
      " [ 0  0  0 ...  0 75  0]\n",
      " [ 0  1  0 ...  0  0 76]]\n",
      "at random state 9, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.76      0.77        87\n",
      "           1       0.74      0.60      0.66       108\n",
      "           2       0.86      0.88      0.87       104\n",
      "           3       0.77      0.76      0.76        94\n",
      "           4       0.81      0.85      0.83       106\n",
      "           5       0.77      0.70      0.73       106\n",
      "           6       0.92      0.97      0.94        93\n",
      "           7       0.85      0.82      0.83        95\n",
      "           8       0.94      0.90      0.92        93\n",
      "           9       0.76      0.85      0.80        92\n",
      "          10       0.67      0.69      0.68       107\n",
      "          11       0.79      0.84      0.81       105\n",
      "          12       0.75      0.79      0.77       102\n",
      "          13       0.76      0.80      0.78       113\n",
      "          14       0.77      0.78      0.78       109\n",
      "          15       0.95      0.97      0.96        59\n",
      "          16       0.90      0.84      0.87       104\n",
      "          17       0.77      0.74      0.76        98\n",
      "          18       0.85      0.79      0.82       108\n",
      "          19       0.77      0.70      0.73        86\n",
      "          20       0.88      0.84      0.86       116\n",
      "          21       0.59      0.68      0.63        92\n",
      "          22       0.97      0.91      0.94       105\n",
      "          23       0.82      0.81      0.82       104\n",
      "          24       0.64      0.54      0.59        96\n",
      "          25       0.94      0.90      0.92       108\n",
      "          26       0.63      0.71      0.67        90\n",
      "          27       0.78      0.84      0.81       110\n",
      "          28       0.81      0.82      0.82        91\n",
      "          29       0.79      0.81      0.80        83\n",
      "          30       0.80      0.80      0.80        96\n",
      "          31       0.56      0.57      0.56       106\n",
      "          32       0.80      0.74      0.77        94\n",
      "          33       0.75      0.73      0.74       111\n",
      "          34       0.82      0.85      0.83       104\n",
      "          35       0.88      0.83      0.85       101\n",
      "          36       0.76      0.70      0.73       101\n",
      "          37       0.74      0.72      0.73       102\n",
      "          38       0.74      0.81      0.77       106\n",
      "          39       0.84      0.88      0.86        94\n",
      "          40       0.81      0.82      0.82       106\n",
      "          41       0.71      0.77      0.73       103\n",
      "          42       0.87      0.89      0.88        89\n",
      "          43       0.77      0.77      0.77        99\n",
      "          44       0.78      0.71      0.75       105\n",
      "          45       0.98      0.91      0.94        87\n",
      "          46       0.89      0.85      0.87        91\n",
      "          47       0.79      0.78      0.78        99\n",
      "          48       0.67      0.71      0.69        97\n",
      "          49       0.76      0.76      0.76       103\n",
      "          50       0.46      0.46      0.46       104\n",
      "          51       1.00      1.00      1.00        47\n",
      "          52       0.88      0.93      0.90        81\n",
      "          53       0.62      0.72      0.66       106\n",
      "\n",
      "    accuracy                           0.78      5296\n",
      "   macro avg       0.79      0.79      0.79      5296\n",
      "weighted avg       0.79      0.78      0.78      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 10,  accuracy score is 1.0\n",
      "at random state 10, confusion matrix is [[79  0  0 ...  0  0  0]\n",
      " [ 0 59  0 ...  0  0  2]\n",
      " [ 0  0 98 ...  0  0  5]\n",
      " ...\n",
      " [ 0  0  0 ... 51  0  0]\n",
      " [ 0  0  0 ...  0 65  0]\n",
      " [ 0  3  1 ...  0  0 43]]\n",
      "at random state 10, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.79      0.82       100\n",
      "           1       0.64      0.61      0.62        97\n",
      "           2       0.90      0.91      0.90       108\n",
      "           3       0.74      0.82      0.78       102\n",
      "           4       0.77      0.89      0.82        97\n",
      "           5       0.71      0.67      0.69       109\n",
      "           6       0.95      0.94      0.94        81\n",
      "           7       0.86      0.86      0.86        99\n",
      "           8       0.94      0.94      0.94       109\n",
      "           9       0.77      0.78      0.78        97\n",
      "          10       0.65      0.70      0.68       101\n",
      "          11       0.89      0.89      0.89       108\n",
      "          12       0.86      0.84      0.85        92\n",
      "          13       0.76      0.80      0.78       115\n",
      "          14       0.69      0.70      0.69        96\n",
      "          15       0.94      0.97      0.95        75\n",
      "          16       0.87      0.88      0.87       107\n",
      "          17       0.76      0.81      0.78        93\n",
      "          18       0.86      0.89      0.88       104\n",
      "          19       0.80      0.72      0.76       119\n",
      "          20       0.86      0.88      0.87       115\n",
      "          21       0.70      0.64      0.67       115\n",
      "          22       0.90      0.91      0.90       107\n",
      "          23       0.78      0.78      0.78       109\n",
      "          24       0.55      0.59      0.57        83\n",
      "          25       0.95      0.93      0.94        89\n",
      "          26       0.66      0.66      0.66        96\n",
      "          27       0.84      0.78      0.81       122\n",
      "          28       0.81      0.83      0.82       100\n",
      "          29       0.93      0.86      0.89       104\n",
      "          30       0.87      0.84      0.85       100\n",
      "          31       0.63      0.64      0.63       105\n",
      "          32       0.83      0.72      0.77       116\n",
      "          33       0.83      0.81      0.82       107\n",
      "          34       0.69      0.83      0.76        89\n",
      "          35       0.75      0.86      0.80        88\n",
      "          36       0.67      0.74      0.70        91\n",
      "          37       0.80      0.78      0.79        98\n",
      "          38       0.78      0.87      0.82       105\n",
      "          39       0.87      0.70      0.78       109\n",
      "          40       0.68      0.83      0.75        94\n",
      "          41       0.75      0.64      0.69       101\n",
      "          42       0.91      0.80      0.85        90\n",
      "          43       0.80      0.71      0.75        96\n",
      "          44       0.80      0.83      0.81        87\n",
      "          45       0.93      0.93      0.93        83\n",
      "          46       0.93      0.85      0.88       104\n",
      "          47       0.72      0.70      0.71        97\n",
      "          48       0.76      0.71      0.73       112\n",
      "          49       0.74      0.76      0.75        84\n",
      "          50       0.48      0.46      0.47       100\n",
      "          51       0.96      1.00      0.98        51\n",
      "          52       0.84      0.94      0.89        69\n",
      "          53       0.54      0.61      0.57        71\n",
      "\n",
      "    accuracy                           0.79      5296\n",
      "   macro avg       0.79      0.79      0.79      5296\n",
      "weighted avg       0.79      0.79      0.79      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 11,  accuracy score is 1.0\n",
      "at random state 11, confusion matrix is [[ 77   0   0 ...   0   0   0]\n",
      " [  0  62   0 ...   0   0   1]\n",
      " [  0   0 100 ...   0   0   2]\n",
      " ...\n",
      " [  0   0   0 ...  58   0   0]\n",
      " [  0   0   0 ...   1  79   0]\n",
      " [  0   1   0 ...   0   0  81]]\n",
      "at random state 11, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.75      0.79       102\n",
      "           1       0.67      0.71      0.69        87\n",
      "           2       0.89      0.88      0.89       113\n",
      "           3       0.85      0.83      0.84        95\n",
      "           4       0.92      0.94      0.93       115\n",
      "           5       0.72      0.75      0.73        88\n",
      "           6       0.93      0.90      0.91        77\n",
      "           7       0.80      0.75      0.77        93\n",
      "           8       0.90      0.90      0.90       103\n",
      "           9       0.88      0.83      0.85       118\n",
      "          10       0.71      0.72      0.72        98\n",
      "          11       0.85      0.87      0.86       107\n",
      "          12       0.92      0.80      0.86       105\n",
      "          13       0.73      0.78      0.75        99\n",
      "          14       0.83      0.78      0.81       101\n",
      "          15       0.95      0.90      0.92        79\n",
      "          16       0.84      0.86      0.85        99\n",
      "          17       0.84      0.84      0.84        93\n",
      "          18       0.78      0.76      0.77        94\n",
      "          19       0.78      0.84      0.81       100\n",
      "          20       0.82      0.86      0.84        92\n",
      "          21       0.60      0.67      0.63        92\n",
      "          22       0.85      0.93      0.89       101\n",
      "          23       0.80      0.80      0.80        93\n",
      "          24       0.55      0.56      0.56       102\n",
      "          25       0.93      0.92      0.93       102\n",
      "          26       0.66      0.68      0.67       102\n",
      "          27       0.84      0.86      0.85        91\n",
      "          28       0.80      0.86      0.83        94\n",
      "          29       0.89      0.89      0.89       109\n",
      "          30       0.93      0.89      0.91       103\n",
      "          31       0.62      0.59      0.60       108\n",
      "          32       0.77      0.69      0.73       107\n",
      "          33       0.73      0.76      0.74       112\n",
      "          34       0.85      0.81      0.83       107\n",
      "          35       0.87      0.88      0.88       100\n",
      "          36       0.64      0.58      0.61        96\n",
      "          37       0.79      0.81      0.80       105\n",
      "          38       0.75      0.82      0.78       100\n",
      "          39       0.77      0.88      0.82        89\n",
      "          40       0.84      0.81      0.82       109\n",
      "          41       0.75      0.74      0.75       111\n",
      "          42       0.83      0.83      0.83        78\n",
      "          43       0.72      0.73      0.72        97\n",
      "          44       0.78      0.79      0.78        96\n",
      "          45       0.89      0.93      0.91        91\n",
      "          46       0.96      0.89      0.93       114\n",
      "          47       0.78      0.71      0.74        87\n",
      "          48       0.72      0.62      0.67        90\n",
      "          49       0.73      0.80      0.76        91\n",
      "          50       0.55      0.52      0.54       103\n",
      "          51       0.95      0.98      0.97        59\n",
      "          52       0.82      0.95      0.88        83\n",
      "          53       0.74      0.70      0.72       116\n",
      "\n",
      "    accuracy                           0.80      5296\n",
      "   macro avg       0.80      0.80      0.80      5296\n",
      "weighted avg       0.80      0.80      0.80      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 12,  accuracy score is 1.0\n",
      "at random state 12, confusion matrix is [[78  0  0 ...  0  0  0]\n",
      " [ 0 69  0 ...  0  0  1]\n",
      " [ 0  0 79 ...  0  0  3]\n",
      " ...\n",
      " [ 0  0  0 ... 51  0  0]\n",
      " [ 0  0  0 ...  0 78  0]\n",
      " [ 0  2  5 ...  0  0 64]]\n",
      "at random state 12, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.77      0.78       101\n",
      "           1       0.66      0.73      0.69        94\n",
      "           2       0.86      0.86      0.86        92\n",
      "           3       0.81      0.76      0.78        98\n",
      "           4       0.85      0.95      0.89        98\n",
      "           5       0.69      0.67      0.68       107\n",
      "           6       1.00      0.83      0.91        88\n",
      "           7       0.80      0.85      0.82       111\n",
      "           8       0.89      0.94      0.92        99\n",
      "           9       0.77      0.68      0.72       104\n",
      "          10       0.61      0.65      0.63       104\n",
      "          11       0.85      0.86      0.86       108\n",
      "          12       0.90      0.86      0.88       107\n",
      "          13       0.74      0.75      0.75       109\n",
      "          14       0.81      0.73      0.77       105\n",
      "          15       0.91      0.94      0.92        83\n",
      "          16       0.88      0.91      0.89       102\n",
      "          17       0.81      0.84      0.82        89\n",
      "          18       0.91      0.81      0.86        96\n",
      "          19       0.73      0.84      0.78       100\n",
      "          20       0.86      0.83      0.84        98\n",
      "          21       0.66      0.66      0.66        92\n",
      "          22       0.93      0.88      0.90       101\n",
      "          23       0.74      0.82      0.77       109\n",
      "          24       0.67      0.69      0.68       104\n",
      "          25       0.90      0.90      0.90        99\n",
      "          26       0.68      0.65      0.66       110\n",
      "          27       0.86      0.85      0.85       114\n",
      "          28       0.82      0.86      0.84        94\n",
      "          29       0.85      0.80      0.82       105\n",
      "          30       0.97      0.90      0.93       103\n",
      "          31       0.59      0.62      0.60       105\n",
      "          32       0.74      0.68      0.71       102\n",
      "          33       0.76      0.75      0.76       110\n",
      "          34       0.77      0.78      0.77       101\n",
      "          35       0.79      0.82      0.80       102\n",
      "          36       0.61      0.72      0.66        88\n",
      "          37       0.80      0.70      0.74       106\n",
      "          38       0.70      0.81      0.75        88\n",
      "          39       0.73      0.78      0.75        89\n",
      "          40       0.80      0.86      0.83        96\n",
      "          41       0.73      0.82      0.77        98\n",
      "          42       0.87      0.82      0.84        92\n",
      "          43       0.74      0.71      0.73        97\n",
      "          44       0.75      0.76      0.75       103\n",
      "          45       0.91      0.99      0.95        72\n",
      "          46       0.91      0.90      0.91        94\n",
      "          47       0.78      0.70      0.74       103\n",
      "          48       0.73      0.68      0.70       100\n",
      "          49       0.79      0.76      0.77        97\n",
      "          50       0.50      0.48      0.49        91\n",
      "          51       1.00      1.00      1.00        51\n",
      "          52       0.91      0.96      0.93        81\n",
      "          53       0.74      0.60      0.67       106\n",
      "\n",
      "    accuracy                           0.79      5296\n",
      "   macro avg       0.79      0.79      0.79      5296\n",
      "weighted avg       0.79      0.79      0.79      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 13,  accuracy score is 1.0\n",
      "at random state 13, confusion matrix is [[ 82   0   0 ...   0   0   0]\n",
      " [  0  69   0 ...   0   0   0]\n",
      " [  0   0 106 ...   0   0   1]\n",
      " ...\n",
      " [  0   0   0 ...  45   0   0]\n",
      " [  0   0   0 ...   0  71   0]\n",
      " [  0   0   3 ...   0   0  63]]\n",
      "at random state 13, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.73      0.80       112\n",
      "           1       0.66      0.70      0.68        99\n",
      "           2       0.91      0.91      0.91       116\n",
      "           3       0.87      0.77      0.82       115\n",
      "           4       0.81      0.90      0.85        98\n",
      "           5       0.65      0.76      0.70        84\n",
      "           6       0.92      0.96      0.94       102\n",
      "           7       0.80      0.77      0.79        88\n",
      "           8       0.98      0.92      0.95       116\n",
      "           9       0.83      0.86      0.84        99\n",
      "          10       0.75      0.64      0.69       121\n",
      "          11       0.90      0.86      0.88        87\n",
      "          12       0.82      0.86      0.84        86\n",
      "          13       0.72      0.73      0.72        95\n",
      "          14       0.78      0.81      0.79        93\n",
      "          15       0.89      0.90      0.89        79\n",
      "          16       0.85      0.88      0.87        93\n",
      "          17       0.83      0.84      0.84       113\n",
      "          18       0.81      0.80      0.81        92\n",
      "          19       0.84      0.79      0.81       104\n",
      "          20       0.84      0.82      0.83        95\n",
      "          21       0.70      0.68      0.69       115\n",
      "          22       0.90      0.90      0.90       110\n",
      "          23       0.75      0.79      0.77       101\n",
      "          24       0.61      0.64      0.63        98\n",
      "          25       0.93      0.87      0.90       105\n",
      "          26       0.60      0.67      0.63        94\n",
      "          27       0.79      0.75      0.77       103\n",
      "          28       0.87      0.82      0.85       101\n",
      "          29       0.89      0.89      0.89        94\n",
      "          30       0.94      0.85      0.90        96\n",
      "          31       0.57      0.59      0.58       101\n",
      "          32       0.82      0.73      0.77       106\n",
      "          33       0.74      0.76      0.75        94\n",
      "          34       0.71      0.80      0.75        94\n",
      "          35       0.90      0.89      0.89       100\n",
      "          36       0.71      0.73      0.72        90\n",
      "          37       0.63      0.80      0.71        97\n",
      "          38       0.79      0.79      0.79       110\n",
      "          39       0.81      0.90      0.85       104\n",
      "          40       0.81      0.88      0.85       109\n",
      "          41       0.76      0.82      0.79        98\n",
      "          42       0.81      0.88      0.84        80\n",
      "          43       0.75      0.76      0.76        96\n",
      "          44       0.74      0.75      0.75       109\n",
      "          45       0.96      0.91      0.94        79\n",
      "          46       0.90      0.91      0.90        78\n",
      "          47       0.71      0.70      0.71       108\n",
      "          48       0.72      0.67      0.69       115\n",
      "          49       0.88      0.77      0.82       100\n",
      "          50       0.56      0.50      0.53       109\n",
      "          51       1.00      1.00      1.00        45\n",
      "          52       0.88      0.93      0.90        76\n",
      "          53       0.74      0.67      0.70        94\n",
      "\n",
      "    accuracy                           0.80      5296\n",
      "   macro avg       0.80      0.80      0.80      5296\n",
      "weighted avg       0.80      0.80      0.80      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 14,  accuracy score is 1.0\n",
      "at random state 14, confusion matrix is [[82  0  0 ...  0  0  0]\n",
      " [ 0 81  0 ...  0  0  4]\n",
      " [ 0  0 79 ...  0  0  1]\n",
      " ...\n",
      " [ 0  0  0 ... 57  0  0]\n",
      " [ 0  0  0 ...  0 68  0]\n",
      " [ 0  0  2 ...  0  1 60]]\n",
      "at random state 14, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.75      0.77       110\n",
      "           1       0.77      0.73      0.75       111\n",
      "           2       0.85      0.87      0.86        91\n",
      "           3       0.85      0.83      0.84        94\n",
      "           4       0.90      0.87      0.89       108\n",
      "           5       0.69      0.69      0.69        96\n",
      "           6       0.95      0.95      0.95        92\n",
      "           7       0.76      0.80      0.78        85\n",
      "           8       0.93      0.92      0.92       100\n",
      "           9       0.78      0.77      0.77       100\n",
      "          10       0.69      0.69      0.69       105\n",
      "          11       0.83      0.82      0.82        98\n",
      "          12       0.87      0.86      0.86        85\n",
      "          13       0.69      0.82      0.75       102\n",
      "          14       0.77      0.78      0.77       101\n",
      "          15       0.95      0.94      0.95        86\n",
      "          16       0.81      0.84      0.83        88\n",
      "          17       0.82      0.77      0.79        94\n",
      "          18       0.84      0.86      0.85       102\n",
      "          19       0.83      0.84      0.84       108\n",
      "          20       0.89      0.83      0.86        95\n",
      "          21       0.55      0.70      0.61        93\n",
      "          22       0.95      0.87      0.91       105\n",
      "          23       0.78      0.80      0.79        97\n",
      "          24       0.66      0.52      0.58       104\n",
      "          25       0.92      0.96      0.94        94\n",
      "          26       0.70      0.67      0.68       114\n",
      "          27       0.75      0.84      0.79        97\n",
      "          28       0.89      0.79      0.84       106\n",
      "          29       0.92      0.92      0.92        99\n",
      "          30       0.85      0.86      0.85        95\n",
      "          31       0.63      0.72      0.68       111\n",
      "          32       0.73      0.72      0.73        96\n",
      "          33       0.70      0.66      0.68       104\n",
      "          34       0.76      0.87      0.81        97\n",
      "          35       0.85      0.88      0.87       104\n",
      "          36       0.72      0.65      0.69       107\n",
      "          37       0.82      0.76      0.79       123\n",
      "          38       0.77      0.81      0.79       101\n",
      "          39       0.78      0.78      0.78       102\n",
      "          40       0.89      0.78      0.83       113\n",
      "          41       0.73      0.74      0.73        93\n",
      "          42       0.85      0.82      0.84        91\n",
      "          43       0.68      0.76      0.72       101\n",
      "          44       0.74      0.81      0.77       104\n",
      "          45       0.93      0.97      0.95        78\n",
      "          46       0.94      0.84      0.89        98\n",
      "          47       0.77      0.73      0.75        96\n",
      "          48       0.68      0.71      0.70        98\n",
      "          49       0.81      0.74      0.77        96\n",
      "          50       0.57      0.59      0.58       101\n",
      "          51       1.00      1.00      1.00        57\n",
      "          52       0.93      0.97      0.95        70\n",
      "          53       0.65      0.60      0.63       100\n",
      "\n",
      "    accuracy                           0.79      5296\n",
      "   macro avg       0.80      0.80      0.80      5296\n",
      "weighted avg       0.80      0.79      0.79      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 15,  accuracy score is 1.0\n",
      "at random state 15, confusion matrix is [[61  0  0 ...  0  0  0]\n",
      " [ 0 68  0 ...  0  0  1]\n",
      " [ 0  0 91 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 48  0  0]\n",
      " [ 0  0  0 ...  0 85  0]\n",
      " [ 0  1  1 ...  0  1 64]]\n",
      "at random state 15, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.70      0.71        87\n",
      "           1       0.70      0.63      0.66       108\n",
      "           2       0.92      0.88      0.90       104\n",
      "           3       0.79      0.74      0.77        98\n",
      "           4       0.89      0.84      0.86        92\n",
      "           5       0.63      0.57      0.60       105\n",
      "           6       1.00      0.94      0.97        94\n",
      "           7       0.87      0.75      0.80       103\n",
      "           8       0.89      0.90      0.89       113\n",
      "           9       0.83      0.79      0.81       109\n",
      "          10       0.64      0.71      0.67        96\n",
      "          11       0.80      0.87      0.84       108\n",
      "          12       0.87      0.80      0.83       118\n",
      "          13       0.73      0.78      0.76       105\n",
      "          14       0.79      0.80      0.79        93\n",
      "          15       0.96      0.97      0.97        75\n",
      "          16       0.82      0.92      0.87        91\n",
      "          17       0.75      0.90      0.82        94\n",
      "          18       0.78      0.77      0.77        94\n",
      "          19       0.77      0.86      0.81        95\n",
      "          20       0.88      0.73      0.80        93\n",
      "          21       0.57      0.62      0.59       104\n",
      "          22       0.96      0.91      0.93        97\n",
      "          23       0.77      0.77      0.77       111\n",
      "          24       0.53      0.61      0.57        90\n",
      "          25       0.93      1.00      0.97        98\n",
      "          26       0.57      0.55      0.56       100\n",
      "          27       0.68      0.77      0.72        97\n",
      "          28       0.86      0.91      0.88        98\n",
      "          29       0.90      0.90      0.90       106\n",
      "          30       0.84      0.91      0.87        95\n",
      "          31       0.68      0.60      0.64       106\n",
      "          32       0.89      0.74      0.81       115\n",
      "          33       0.74      0.68      0.71       110\n",
      "          34       0.80      0.74      0.77       109\n",
      "          35       0.91      0.90      0.90       105\n",
      "          36       0.63      0.75      0.69        97\n",
      "          37       0.71      0.78      0.75        88\n",
      "          38       0.77      0.72      0.74       109\n",
      "          39       0.86      0.82      0.84        89\n",
      "          40       0.81      0.84      0.83       102\n",
      "          41       0.70      0.76      0.73        96\n",
      "          42       0.90      0.85      0.87        84\n",
      "          43       0.81      0.73      0.77       112\n",
      "          44       0.81      0.81      0.81       103\n",
      "          45       0.99      0.93      0.96        82\n",
      "          46       0.94      0.97      0.95        93\n",
      "          47       0.73      0.76      0.74        92\n",
      "          48       0.55      0.63      0.58        99\n",
      "          49       0.69      0.66      0.68        98\n",
      "          50       0.48      0.51      0.49        95\n",
      "          51       1.00      1.00      1.00        48\n",
      "          52       0.86      0.99      0.92        86\n",
      "          53       0.74      0.60      0.66       107\n",
      "\n",
      "    accuracy                           0.78      5296\n",
      "   macro avg       0.79      0.79      0.79      5296\n",
      "weighted avg       0.79      0.78      0.78      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 16,  accuracy score is 1.0\n",
      "at random state 16, confusion matrix is [[64  0  0 ...  0  0  0]\n",
      " [ 0 55  0 ...  0  0  3]\n",
      " [ 0  0 92 ...  0  0  2]\n",
      " ...\n",
      " [ 0  0  0 ... 53  0  0]\n",
      " [ 0  0  0 ...  0 70  0]\n",
      " [ 0  2  0 ...  0  0 68]]\n",
      "at random state 16, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.78      0.74        82\n",
      "           1       0.65      0.62      0.64        89\n",
      "           2       0.90      0.88      0.89       105\n",
      "           3       0.79      0.77      0.78        84\n",
      "           4       0.94      0.92      0.93        99\n",
      "           5       0.71      0.68      0.69       114\n",
      "           6       0.91      0.93      0.92        83\n",
      "           7       0.78      0.79      0.78        96\n",
      "           8       0.95      0.90      0.92        98\n",
      "           9       0.86      0.83      0.84       111\n",
      "          10       0.60      0.64      0.62       112\n",
      "          11       0.81      0.90      0.85        97\n",
      "          12       0.95      0.77      0.85       116\n",
      "          13       0.74      0.77      0.75       105\n",
      "          14       0.76      0.80      0.77        93\n",
      "          15       0.97      0.99      0.98        85\n",
      "          16       0.86      0.91      0.88        88\n",
      "          17       0.74      0.86      0.79        90\n",
      "          18       0.82      0.90      0.86        93\n",
      "          19       0.86      0.82      0.84       104\n",
      "          20       0.83      0.93      0.88       104\n",
      "          21       0.68      0.58      0.63       115\n",
      "          22       0.94      0.90      0.92       110\n",
      "          23       0.79      0.64      0.71       108\n",
      "          24       0.59      0.55      0.57       103\n",
      "          25       0.92      0.94      0.93       101\n",
      "          26       0.71      0.58      0.63       113\n",
      "          27       0.71      0.79      0.75        94\n",
      "          28       0.89      0.86      0.88       114\n",
      "          29       0.85      0.84      0.85        88\n",
      "          30       0.90      0.87      0.89        87\n",
      "          31       0.67      0.63      0.65        99\n",
      "          32       0.80      0.76      0.78       109\n",
      "          33       0.75      0.74      0.75        81\n",
      "          34       0.82      0.76      0.79       107\n",
      "          35       0.85      0.82      0.84       106\n",
      "          36       0.64      0.74      0.69        98\n",
      "          37       0.74      0.76      0.75       115\n",
      "          38       0.83      0.79      0.81       107\n",
      "          39       0.80      0.83      0.82       101\n",
      "          40       0.82      0.78      0.80        95\n",
      "          41       0.77      0.81      0.79       102\n",
      "          42       0.89      0.85      0.87        88\n",
      "          43       0.65      0.74      0.69        78\n",
      "          44       0.78      0.86      0.82       110\n",
      "          45       0.92      0.94      0.93        90\n",
      "          46       0.94      0.92      0.93        92\n",
      "          47       0.69      0.77      0.73        90\n",
      "          48       0.73      0.73      0.73       114\n",
      "          49       0.78      0.81      0.79       111\n",
      "          50       0.50      0.55      0.53        95\n",
      "          51       0.98      1.00      0.99        53\n",
      "          52       0.95      0.91      0.93        77\n",
      "          53       0.67      0.70      0.69        97\n",
      "\n",
      "    accuracy                           0.79      5296\n",
      "   macro avg       0.80      0.80      0.80      5296\n",
      "weighted avg       0.80      0.79      0.79      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 17,  accuracy score is 1.0\n",
      "at random state 17, confusion matrix is [[89  0  0 ...  0  0  0]\n",
      " [ 0 60  0 ...  0  0  4]\n",
      " [ 0  0 97 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 60  0  0]\n",
      " [ 0  0  0 ...  0 62  0]\n",
      " [ 0  3  3 ...  0  0 67]]\n",
      "at random state 17, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.79      0.77       112\n",
      "           1       0.62      0.62      0.62        96\n",
      "           2       0.89      0.86      0.87       113\n",
      "           3       0.69      0.81      0.74        86\n",
      "           4       0.81      0.92      0.86        95\n",
      "           5       0.72      0.64      0.68       104\n",
      "           6       0.97      0.97      0.97        92\n",
      "           7       0.81      0.87      0.84        99\n",
      "           8       0.90      0.90      0.90       105\n",
      "           9       0.75      0.86      0.80        91\n",
      "          10       0.77      0.67      0.72       107\n",
      "          11       0.87      0.90      0.88       107\n",
      "          12       0.86      0.85      0.86       102\n",
      "          13       0.69      0.71      0.70        98\n",
      "          14       0.75      0.79      0.77       116\n",
      "          15       0.93      0.98      0.96        88\n",
      "          16       0.84      0.84      0.84       109\n",
      "          17       0.76      0.76      0.76        90\n",
      "          18       0.83      0.81      0.82        90\n",
      "          19       0.80      0.76      0.78       110\n",
      "          20       0.77      0.82      0.79       104\n",
      "          21       0.66      0.52      0.58        96\n",
      "          22       0.89      0.94      0.92        99\n",
      "          23       0.87      0.72      0.79       105\n",
      "          24       0.58      0.63      0.60        90\n",
      "          25       0.96      0.91      0.93        93\n",
      "          26       0.61      0.70      0.65        97\n",
      "          27       0.86      0.86      0.86       105\n",
      "          28       0.86      0.86      0.86        97\n",
      "          29       0.97      0.94      0.95        88\n",
      "          30       0.95      0.85      0.90       107\n",
      "          31       0.60      0.60      0.60       101\n",
      "          32       0.75      0.70      0.73        94\n",
      "          33       0.66      0.70      0.68        91\n",
      "          34       0.85      0.70      0.77       124\n",
      "          35       0.82      0.87      0.84       105\n",
      "          36       0.67      0.69      0.68       100\n",
      "          37       0.79      0.74      0.76        96\n",
      "          38       0.76      0.80      0.78       109\n",
      "          39       0.86      0.81      0.83       108\n",
      "          40       0.83      0.82      0.83        97\n",
      "          41       0.80      0.74      0.77       105\n",
      "          42       0.86      0.85      0.86        89\n",
      "          43       0.77      0.74      0.75        97\n",
      "          44       0.70      0.78      0.74        95\n",
      "          45       0.91      0.95      0.93        83\n",
      "          46       0.94      0.82      0.88        91\n",
      "          47       0.64      0.73      0.68        88\n",
      "          48       0.74      0.68      0.71       105\n",
      "          49       0.78      0.79      0.78       107\n",
      "          50       0.46      0.51      0.49        90\n",
      "          51       0.98      1.00      0.99        60\n",
      "          52       0.91      0.97      0.94        64\n",
      "          53       0.67      0.63      0.65       106\n",
      "\n",
      "    accuracy                           0.79      5296\n",
      "   macro avg       0.79      0.79      0.79      5296\n",
      "weighted avg       0.79      0.79      0.79      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 18,  accuracy score is 1.0\n",
      "at random state 18, confusion matrix is [[74  0  0 ...  0  0  0]\n",
      " [ 0 76  0 ...  0  0  0]\n",
      " [ 0  0 96 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 51  0  0]\n",
      " [ 0  0  0 ...  0 56  0]\n",
      " [ 0  0  4 ...  0  0 64]]\n",
      "at random state 18, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.75      0.80        99\n",
      "           1       0.70      0.73      0.71       104\n",
      "           2       0.84      0.90      0.87       107\n",
      "           3       0.81      0.82      0.81        88\n",
      "           4       0.93      0.83      0.88        90\n",
      "           5       0.73      0.66      0.69        97\n",
      "           6       0.97      0.92      0.94        99\n",
      "           7       0.81      0.84      0.82       104\n",
      "           8       0.93      0.92      0.92        86\n",
      "           9       0.84      0.78      0.81       103\n",
      "          10       0.74      0.71      0.73        98\n",
      "          11       0.92      0.83      0.87       106\n",
      "          12       0.87      0.79      0.83       109\n",
      "          13       0.79      0.69      0.74       101\n",
      "          14       0.84      0.88      0.86        93\n",
      "          15       0.89      0.90      0.90       104\n",
      "          16       0.91      0.91      0.91       102\n",
      "          17       0.86      0.87      0.86       102\n",
      "          18       0.81      0.87      0.84        89\n",
      "          19       0.82      0.79      0.81       116\n",
      "          20       0.89      0.90      0.90       100\n",
      "          21       0.56      0.62      0.59        93\n",
      "          22       0.82      0.92      0.87        88\n",
      "          23       0.80      0.81      0.80       109\n",
      "          24       0.72      0.63      0.67       109\n",
      "          25       0.91      0.86      0.88       111\n",
      "          26       0.62      0.78      0.69        98\n",
      "          27       0.84      0.74      0.79       117\n",
      "          28       0.91      0.78      0.84       106\n",
      "          29       0.88      0.93      0.90        86\n",
      "          30       0.88      0.91      0.89        98\n",
      "          31       0.74      0.64      0.69       100\n",
      "          32       0.74      0.83      0.79        90\n",
      "          33       0.70      0.84      0.77        95\n",
      "          34       0.79      0.79      0.79       102\n",
      "          35       0.81      0.90      0.85       100\n",
      "          36       0.65      0.67      0.66       110\n",
      "          37       0.71      0.78      0.74        96\n",
      "          38       0.79      0.83      0.81        99\n",
      "          39       0.78      0.78      0.78        97\n",
      "          40       0.90      0.84      0.87       111\n",
      "          41       0.76      0.75      0.76       105\n",
      "          42       0.85      0.86      0.86        96\n",
      "          43       0.70      0.69      0.69       105\n",
      "          44       0.80      0.81      0.81       100\n",
      "          45       0.95      0.92      0.93        85\n",
      "          46       0.86      0.94      0.90        85\n",
      "          47       0.80      0.75      0.78       104\n",
      "          48       0.74      0.63      0.68        99\n",
      "          49       0.69      0.85      0.76        94\n",
      "          50       0.52      0.65      0.58        93\n",
      "          51       0.98      0.94      0.96        54\n",
      "          52       0.89      0.86      0.88        65\n",
      "          53       0.77      0.65      0.70        99\n",
      "\n",
      "    accuracy                           0.80      5296\n",
      "   macro avg       0.81      0.81      0.80      5296\n",
      "weighted avg       0.81      0.80      0.80      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 19,  accuracy score is 1.0\n",
      "at random state 19, confusion matrix is [[ 81   0   0 ...   0   0   0]\n",
      " [  0  73   0 ...   0   0   1]\n",
      " [  0   0 102 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  47   0   0]\n",
      " [  0   0   0 ...   1  75   0]\n",
      " [  0   0   3 ...   0   0  66]]\n",
      "at random state 19, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.74      0.74       109\n",
      "           1       0.73      0.68      0.71       107\n",
      "           2       0.86      0.94      0.90       109\n",
      "           3       0.83      0.81      0.82       124\n",
      "           4       0.83      0.88      0.85        97\n",
      "           5       0.69      0.73      0.71       100\n",
      "           6       0.95      0.95      0.95        86\n",
      "           7       0.81      0.75      0.78        95\n",
      "           8       0.91      0.88      0.89       101\n",
      "           9       0.78      0.86      0.81        97\n",
      "          10       0.63      0.67      0.65        86\n",
      "          11       0.87      0.87      0.87       105\n",
      "          12       0.89      0.79      0.84       113\n",
      "          13       0.76      0.74      0.75       102\n",
      "          14       0.81      0.77      0.79        93\n",
      "          15       0.93      0.90      0.92        78\n",
      "          16       0.86      0.91      0.89       104\n",
      "          17       0.84      0.75      0.79       109\n",
      "          18       0.88      0.88      0.88       111\n",
      "          19       0.81      0.81      0.81        98\n",
      "          20       0.81      0.80      0.81        96\n",
      "          21       0.57      0.61      0.59       103\n",
      "          22       0.86      0.90      0.88        91\n",
      "          23       0.70      0.65      0.67        88\n",
      "          24       0.70      0.56      0.62       122\n",
      "          25       0.91      0.95      0.93        87\n",
      "          26       0.77      0.54      0.63       109\n",
      "          27       0.79      0.87      0.83       105\n",
      "          28       0.85      0.89      0.87        92\n",
      "          29       0.89      0.93      0.91        94\n",
      "          30       0.85      0.84      0.85       103\n",
      "          31       0.54      0.69      0.61        98\n",
      "          32       0.78      0.75      0.76       121\n",
      "          33       0.71      0.84      0.77        92\n",
      "          34       0.78      0.87      0.82        83\n",
      "          35       0.92      0.86      0.89        98\n",
      "          36       0.78      0.55      0.64       107\n",
      "          37       0.73      0.80      0.76       103\n",
      "          38       0.78      0.82      0.80        94\n",
      "          39       0.81      0.87      0.84        92\n",
      "          40       0.75      0.84      0.79        99\n",
      "          41       0.77      0.74      0.76       100\n",
      "          42       0.81      0.88      0.84        88\n",
      "          43       0.66      0.80      0.73        91\n",
      "          44       0.73      0.75      0.74       101\n",
      "          45       0.91      0.93      0.92        92\n",
      "          46       0.88      0.90      0.89        89\n",
      "          47       0.75      0.66      0.70        93\n",
      "          48       0.65      0.67      0.66        89\n",
      "          49       0.70      0.72      0.71        92\n",
      "          50       0.52      0.46      0.49       125\n",
      "          51       0.94      1.00      0.97        47\n",
      "          52       0.97      0.87      0.92        86\n",
      "          53       0.69      0.65      0.67       102\n",
      "\n",
      "    accuracy                           0.78      5296\n",
      "   macro avg       0.79      0.79      0.79      5296\n",
      "weighted avg       0.79      0.78      0.78      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 20,  accuracy score is 1.0\n",
      "at random state 20, confusion matrix is [[ 70   0   0 ...   0   0   0]\n",
      " [  0  60   0 ...   0   0   2]\n",
      " [  0   0 101 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  53   0   0]\n",
      " [  0   0   0 ...   0  64   0]\n",
      " [  0   0   2 ...   0   0  66]]\n",
      "at random state 20, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.84      0.80        83\n",
      "           1       0.75      0.65      0.70        92\n",
      "           2       0.94      0.94      0.94       108\n",
      "           3       0.84      0.80      0.82       112\n",
      "           4       0.90      0.90      0.90       102\n",
      "           5       0.69      0.61      0.65       114\n",
      "           6       0.96      0.92      0.94        87\n",
      "           7       0.82      0.82      0.82        95\n",
      "           8       0.89      0.95      0.92       119\n",
      "           9       0.73      0.76      0.75       100\n",
      "          10       0.68      0.71      0.69       116\n",
      "          11       0.82      0.85      0.84       111\n",
      "          12       0.82      0.85      0.83        85\n",
      "          13       0.77      0.69      0.73       106\n",
      "          14       0.83      0.80      0.82       106\n",
      "          15       0.99      0.90      0.94        94\n",
      "          16       0.86      0.90      0.88        93\n",
      "          17       0.78      0.70      0.74       118\n",
      "          18       0.79      0.88      0.83        97\n",
      "          19       0.76      0.81      0.79        91\n",
      "          20       0.86      0.86      0.86       108\n",
      "          21       0.63      0.58      0.60        97\n",
      "          22       0.93      0.93      0.93       107\n",
      "          23       0.71      0.78      0.74        87\n",
      "          24       0.67      0.72      0.69        92\n",
      "          25       0.88      0.96      0.92        91\n",
      "          26       0.52      0.66      0.58        96\n",
      "          27       0.84      0.80      0.82       102\n",
      "          28       0.78      0.82      0.80       105\n",
      "          29       0.93      0.80      0.86       111\n",
      "          30       0.85      0.88      0.86       101\n",
      "          31       0.66      0.67      0.67       100\n",
      "          32       0.79      0.63      0.70       107\n",
      "          33       0.86      0.83      0.84       107\n",
      "          34       0.78      0.76      0.77        82\n",
      "          35       0.80      0.78      0.79        86\n",
      "          36       0.71      0.67      0.69       106\n",
      "          37       0.78      0.72      0.75       101\n",
      "          38       0.76      0.86      0.81        86\n",
      "          39       0.76      0.84      0.79        97\n",
      "          40       0.83      0.82      0.82       106\n",
      "          41       0.70      0.68      0.69        95\n",
      "          42       0.90      0.87      0.89        87\n",
      "          43       0.68      0.68      0.68        99\n",
      "          44       0.79      0.79      0.79        99\n",
      "          45       0.87      0.97      0.91        93\n",
      "          46       0.89      0.84      0.86        91\n",
      "          47       0.75      0.74      0.74        96\n",
      "          48       0.71      0.73      0.72       106\n",
      "          49       0.79      0.83      0.81       109\n",
      "          50       0.56      0.51      0.53        98\n",
      "          51       0.98      1.00      0.99        53\n",
      "          52       0.90      0.91      0.91        70\n",
      "          53       0.65      0.69      0.67        96\n",
      "\n",
      "    accuracy                           0.79      5296\n",
      "   macro avg       0.79      0.79      0.79      5296\n",
      "weighted avg       0.79      0.79      0.79      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 21,  accuracy score is 1.0\n",
      "at random state 21, confusion matrix is [[69  0  0 ...  0  0  0]\n",
      " [ 0 68  0 ...  0  0  2]\n",
      " [ 0  0 83 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 53  0  0]\n",
      " [ 0  0  0 ...  1 71  0]\n",
      " [ 0  2  0 ...  0  0 75]]\n",
      "at random state 21, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.74      0.77        93\n",
      "           1       0.65      0.75      0.69        91\n",
      "           2       0.92      0.89      0.91        93\n",
      "           3       0.67      0.81      0.73        89\n",
      "           4       0.89      0.83      0.86        99\n",
      "           5       0.69      0.76      0.72       108\n",
      "           6       0.96      0.96      0.96        79\n",
      "           7       0.77      0.78      0.78       110\n",
      "           8       0.93      0.97      0.94       115\n",
      "           9       0.81      0.82      0.82       101\n",
      "          10       0.59      0.64      0.61        92\n",
      "          11       0.87      0.81      0.84       104\n",
      "          12       0.85      0.85      0.85        97\n",
      "          13       0.80      0.74      0.77       107\n",
      "          14       0.83      0.77      0.80       118\n",
      "          15       0.96      0.93      0.95        92\n",
      "          16       0.82      0.79      0.81        97\n",
      "          17       0.80      0.76      0.78        96\n",
      "          18       0.81      0.82      0.81        98\n",
      "          19       0.81      0.72      0.77       109\n",
      "          20       0.93      0.82      0.87       103\n",
      "          21       0.64      0.64      0.64        98\n",
      "          22       0.89      0.87      0.88        95\n",
      "          23       0.71      0.70      0.71       108\n",
      "          24       0.59      0.55      0.57       105\n",
      "          25       0.94      0.93      0.93       100\n",
      "          26       0.59      0.65      0.62        99\n",
      "          27       0.71      0.78      0.74        99\n",
      "          28       0.87      0.78      0.82       102\n",
      "          29       0.82      0.89      0.85        89\n",
      "          30       0.86      0.87      0.86        95\n",
      "          31       0.62      0.64      0.63       103\n",
      "          32       0.77      0.73      0.75        93\n",
      "          33       0.72      0.77      0.75        99\n",
      "          34       0.78      0.74      0.76       118\n",
      "          35       0.89      0.90      0.89       115\n",
      "          36       0.71      0.72      0.72        97\n",
      "          37       0.78      0.70      0.74       100\n",
      "          38       0.71      0.81      0.76        86\n",
      "          39       0.83      0.77      0.80       115\n",
      "          40       0.87      0.75      0.80       102\n",
      "          41       0.78      0.81      0.79       107\n",
      "          42       0.85      0.92      0.89        89\n",
      "          43       0.80      0.78      0.79        96\n",
      "          44       0.67      0.75      0.70        75\n",
      "          45       0.93      0.91      0.92        93\n",
      "          46       0.89      0.93      0.91       100\n",
      "          47       0.81      0.77      0.79       108\n",
      "          48       0.64      0.64      0.64        86\n",
      "          49       0.82      0.73      0.77       110\n",
      "          50       0.43      0.44      0.44        97\n",
      "          51       0.98      1.00      0.99        53\n",
      "          52       0.86      0.92      0.89        77\n",
      "          53       0.69      0.78      0.74        96\n",
      "\n",
      "    accuracy                           0.78      5296\n",
      "   macro avg       0.79      0.79      0.79      5296\n",
      "weighted avg       0.79      0.78      0.79      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 22,  accuracy score is 1.0\n",
      "at random state 22, confusion matrix is [[77  0  0 ...  0  0  0]\n",
      " [ 0 85  0 ...  0  0  1]\n",
      " [ 0  0 91 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 48  0  0]\n",
      " [ 0  0  0 ...  0 74  0]\n",
      " [ 0  3  1 ...  0  0 66]]\n",
      "at random state 22, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.84      0.83        92\n",
      "           1       0.77      0.75      0.76       114\n",
      "           2       0.87      0.94      0.90        97\n",
      "           3       0.79      0.84      0.81        94\n",
      "           4       0.87      0.87      0.87       101\n",
      "           5       0.73      0.69      0.71       102\n",
      "           6       0.98      0.98      0.98        84\n",
      "           7       0.77      0.76      0.77        97\n",
      "           8       0.94      0.87      0.90       107\n",
      "           9       0.79      0.82      0.80       103\n",
      "          10       0.63      0.66      0.64       103\n",
      "          11       0.84      0.86      0.85       109\n",
      "          12       0.88      0.75      0.81       100\n",
      "          13       0.73      0.83      0.78        96\n",
      "          14       0.71      0.76      0.73       110\n",
      "          15       0.97      0.94      0.95        78\n",
      "          16       0.84      0.88      0.86       105\n",
      "          17       0.81      0.79      0.80        98\n",
      "          18       0.83      0.86      0.84       100\n",
      "          19       0.84      0.75      0.79       102\n",
      "          20       0.82      0.85      0.83       107\n",
      "          21       0.64      0.54      0.59       100\n",
      "          22       0.91      0.97      0.94        88\n",
      "          23       0.81      0.77      0.79       102\n",
      "          24       0.60      0.62      0.61        93\n",
      "          25       0.94      0.96      0.95        94\n",
      "          26       0.58      0.63      0.60       108\n",
      "          27       0.85      0.88      0.86       112\n",
      "          28       0.86      0.76      0.80       103\n",
      "          29       0.86      0.91      0.88        92\n",
      "          30       0.91      0.87      0.89       114\n",
      "          31       0.68      0.64      0.66       101\n",
      "          32       0.74      0.74      0.74        96\n",
      "          33       0.75      0.69      0.72        94\n",
      "          34       0.82      0.81      0.81       100\n",
      "          35       0.88      0.89      0.88        97\n",
      "          36       0.66      0.68      0.67        90\n",
      "          37       0.73      0.75      0.74       111\n",
      "          38       0.74      0.77      0.75       111\n",
      "          39       0.83      0.79      0.81        96\n",
      "          40       0.79      0.81      0.80       104\n",
      "          41       0.76      0.73      0.74        97\n",
      "          42       0.90      0.91      0.91        93\n",
      "          43       0.76      0.74      0.75       101\n",
      "          44       0.78      0.78      0.78        88\n",
      "          45       0.93      0.92      0.92        85\n",
      "          46       0.94      0.94      0.94       103\n",
      "          47       0.70      0.66      0.68       105\n",
      "          48       0.70      0.68      0.69       107\n",
      "          49       0.79      0.76      0.78       101\n",
      "          50       0.41      0.48      0.44        85\n",
      "          51       1.00      1.00      1.00        48\n",
      "          52       0.91      0.93      0.92        80\n",
      "          53       0.66      0.67      0.67        98\n",
      "\n",
      "    accuracy                           0.79      5296\n",
      "   macro avg       0.80      0.80      0.80      5296\n",
      "weighted avg       0.79      0.79      0.79      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 23,  accuracy score is 1.0\n",
      "at random state 23, confusion matrix is [[86  0  0 ...  0  0  0]\n",
      " [ 0 74  0 ...  0  0  0]\n",
      " [ 0  0 86 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 48  0  0]\n",
      " [ 0  0  0 ...  0 81  0]\n",
      " [ 0  0  2 ...  0  0 76]]\n",
      "at random state 23, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.81      0.84       106\n",
      "           1       0.79      0.78      0.78        95\n",
      "           2       0.92      0.91      0.91        95\n",
      "           3       0.79      0.76      0.77       110\n",
      "           4       0.80      0.89      0.84        94\n",
      "           5       0.81      0.75      0.78       100\n",
      "           6       0.87      0.98      0.92        85\n",
      "           7       0.76      0.87      0.81        94\n",
      "           8       0.94      0.94      0.94       124\n",
      "           9       0.75      0.82      0.78        89\n",
      "          10       0.68      0.77      0.73       111\n",
      "          11       0.86      0.91      0.88       106\n",
      "          12       0.88      0.83      0.86       103\n",
      "          13       0.79      0.73      0.76       105\n",
      "          14       0.70      0.85      0.77        81\n",
      "          15       0.90      0.89      0.90        73\n",
      "          16       0.89      0.92      0.90       106\n",
      "          17       0.89      0.87      0.88        92\n",
      "          18       0.84      0.84      0.84       106\n",
      "          19       0.84      0.81      0.82       101\n",
      "          20       0.82      0.82      0.82       101\n",
      "          21       0.60      0.70      0.65        88\n",
      "          22       0.91      0.93      0.92       106\n",
      "          23       0.78      0.75      0.77       102\n",
      "          24       0.65      0.61      0.63       108\n",
      "          25       0.87      0.90      0.89       104\n",
      "          26       0.71      0.69      0.70       108\n",
      "          27       0.73      0.81      0.77        94\n",
      "          28       0.79      0.91      0.85        92\n",
      "          29       0.87      0.83      0.85        99\n",
      "          30       0.94      0.86      0.90       105\n",
      "          31       0.59      0.62      0.60       103\n",
      "          32       0.87      0.67      0.76       102\n",
      "          33       0.75      0.84      0.79       100\n",
      "          34       0.80      0.76      0.78       100\n",
      "          35       0.80      0.80      0.80        97\n",
      "          36       0.65      0.70      0.68       104\n",
      "          37       0.73      0.70      0.72        94\n",
      "          38       0.76      0.71      0.73       106\n",
      "          39       0.80      0.77      0.79       101\n",
      "          40       0.87      0.83      0.85        93\n",
      "          41       0.71      0.68      0.70        96\n",
      "          42       0.90      0.84      0.87        95\n",
      "          43       0.66      0.69      0.67        81\n",
      "          44       0.82      0.85      0.84        89\n",
      "          45       0.93      0.87      0.90        93\n",
      "          46       0.97      0.88      0.92       105\n",
      "          47       0.78      0.73      0.76       100\n",
      "          48       0.76      0.76      0.76        83\n",
      "          49       0.85      0.79      0.82       103\n",
      "          50       0.49      0.54      0.52       112\n",
      "          51       1.00      0.94      0.97        51\n",
      "          52       0.95      0.92      0.94        88\n",
      "          53       0.76      0.65      0.70       117\n",
      "\n",
      "    accuracy                           0.80      5296\n",
      "   macro avg       0.80      0.80      0.80      5296\n",
      "weighted avg       0.80      0.80      0.80      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 24,  accuracy score is 1.0\n",
      "at random state 24, confusion matrix is [[79  0  0 ...  0  0  0]\n",
      " [ 0 67  0 ...  0  0  2]\n",
      " [ 0  0 95 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 58  0  0]\n",
      " [ 0  0  0 ...  1 64  0]\n",
      " [ 0  0  2 ...  0  0 63]]\n",
      "at random state 24, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.72      0.74       110\n",
      "           1       0.64      0.74      0.69        90\n",
      "           2       0.82      0.93      0.87       102\n",
      "           3       0.84      0.75      0.79       103\n",
      "           4       0.85      0.84      0.85       112\n",
      "           5       0.74      0.72      0.73       111\n",
      "           6       0.98      0.91      0.94       100\n",
      "           7       0.85      0.88      0.87       107\n",
      "           8       0.84      0.94      0.89        98\n",
      "           9       0.82      0.86      0.84       116\n",
      "          10       0.77      0.73      0.75        99\n",
      "          11       0.90      0.75      0.82        83\n",
      "          12       0.94      0.81      0.87        97\n",
      "          13       0.74      0.73      0.74        96\n",
      "          14       0.78      0.73      0.76       100\n",
      "          15       0.89      0.98      0.93        64\n",
      "          16       0.80      0.87      0.83        99\n",
      "          17       0.86      0.80      0.83        95\n",
      "          18       0.87      0.87      0.87       106\n",
      "          19       0.78      0.74      0.76       108\n",
      "          20       0.85      0.83      0.84       101\n",
      "          21       0.71      0.68      0.69       102\n",
      "          22       0.92      0.96      0.94       103\n",
      "          23       0.71      0.76      0.73        96\n",
      "          24       0.60      0.62      0.61       103\n",
      "          25       0.93      0.96      0.94       103\n",
      "          26       0.63      0.65      0.64       100\n",
      "          27       0.83      0.83      0.83       110\n",
      "          28       0.92      0.79      0.85        92\n",
      "          29       0.92      0.93      0.92        86\n",
      "          30       0.84      0.85      0.84       102\n",
      "          31       0.60      0.57      0.58        91\n",
      "          32       0.77      0.72      0.75       100\n",
      "          33       0.72      0.80      0.76        89\n",
      "          34       0.76      0.81      0.79        96\n",
      "          35       0.83      0.87      0.85        94\n",
      "          36       0.73      0.71      0.72       101\n",
      "          37       0.76      0.75      0.76        91\n",
      "          38       0.79      0.81      0.80       118\n",
      "          39       0.85      0.85      0.85       122\n",
      "          40       0.79      0.81      0.80       101\n",
      "          41       0.73      0.76      0.74        91\n",
      "          42       0.83      0.84      0.84       101\n",
      "          43       0.72      0.70      0.71        86\n",
      "          44       0.79      0.72      0.75        99\n",
      "          45       0.94      0.98      0.96        90\n",
      "          46       0.91      0.95      0.93       101\n",
      "          47       0.75      0.75      0.75       102\n",
      "          48       0.74      0.73      0.73       104\n",
      "          49       0.72      0.76      0.74       103\n",
      "          50       0.46      0.42      0.44        99\n",
      "          51       0.98      1.00      0.99        58\n",
      "          52       0.86      0.91      0.89        70\n",
      "          53       0.66      0.66      0.66        95\n",
      "\n",
      "    accuracy                           0.79      5296\n",
      "   macro avg       0.80      0.80      0.80      5296\n",
      "weighted avg       0.80      0.79      0.79      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 25,  accuracy score is 1.0\n",
      "at random state 25, confusion matrix is [[ 84   0   0 ...   0   0   0]\n",
      " [  0  72   0 ...   0   0   1]\n",
      " [  0   0 101 ...   0   0   1]\n",
      " ...\n",
      " [  0   0   0 ...  45   0   0]\n",
      " [  0   0   0 ...   0  76   0]\n",
      " [  0   0   0 ...   0   0  61]]\n",
      "at random state 25, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.76      0.79       111\n",
      "           1       0.70      0.74      0.72        97\n",
      "           2       0.94      0.89      0.91       113\n",
      "           3       0.87      0.74      0.80       114\n",
      "           4       0.83      0.89      0.86        95\n",
      "           5       0.70      0.72      0.71        99\n",
      "           6       0.98      0.96      0.97        82\n",
      "           7       0.82      0.80      0.81        92\n",
      "           8       0.95      0.88      0.91        99\n",
      "           9       0.78      0.75      0.76       112\n",
      "          10       0.84      0.61      0.70       112\n",
      "          11       0.83      0.90      0.86        96\n",
      "          12       0.81      0.82      0.81       107\n",
      "          13       0.68      0.80      0.74       102\n",
      "          14       0.77      0.72      0.74       101\n",
      "          15       0.94      1.00      0.97        65\n",
      "          16       0.84      0.88      0.86       103\n",
      "          17       0.82      0.75      0.78        92\n",
      "          18       0.84      0.69      0.76       101\n",
      "          19       0.76      0.74      0.75       103\n",
      "          20       0.88      0.87      0.87        89\n",
      "          21       0.63      0.60      0.61        99\n",
      "          22       0.89      0.86      0.87        99\n",
      "          23       0.72      0.85      0.78       107\n",
      "          24       0.61      0.59      0.60        96\n",
      "          25       0.90      0.88      0.89        86\n",
      "          26       0.68      0.68      0.68       113\n",
      "          27       0.83      0.80      0.81        96\n",
      "          28       0.83      0.81      0.82       107\n",
      "          29       0.91      0.86      0.89       110\n",
      "          30       0.89      0.83      0.86        84\n",
      "          31       0.67      0.68      0.67       103\n",
      "          32       0.75      0.75      0.75       101\n",
      "          33       0.64      0.67      0.66       103\n",
      "          34       0.81      0.82      0.82        95\n",
      "          35       0.85      0.89      0.87       103\n",
      "          36       0.68      0.77      0.72       115\n",
      "          37       0.81      0.78      0.79       107\n",
      "          38       0.81      0.77      0.79       100\n",
      "          39       0.68      0.83      0.75        99\n",
      "          40       0.78      0.81      0.79        93\n",
      "          41       0.76      0.67      0.71        94\n",
      "          42       0.81      0.94      0.87        88\n",
      "          43       0.75      0.63      0.68       108\n",
      "          44       0.74      0.77      0.76       106\n",
      "          45       0.92      0.95      0.94        85\n",
      "          46       0.89      0.93      0.91        86\n",
      "          47       0.75      0.73      0.74       108\n",
      "          48       0.67      0.72      0.69       105\n",
      "          49       0.78      0.71      0.75       112\n",
      "          50       0.52      0.52      0.52        95\n",
      "          51       0.96      1.00      0.98        45\n",
      "          52       0.90      0.94      0.92        81\n",
      "          53       0.56      0.74      0.64        82\n",
      "\n",
      "    accuracy                           0.78      5296\n",
      "   macro avg       0.79      0.79      0.79      5296\n",
      "weighted avg       0.79      0.78      0.78      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 26,  accuracy score is 1.0\n",
      "at random state 26, confusion matrix is [[ 89   0   0 ...   0   0   0]\n",
      " [  0  68   0 ...   0   0   0]\n",
      " [  0   0 100 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  50   0   0]\n",
      " [  0   0   0 ...   0  64   0]\n",
      " [  0   4   0 ...   0   0  71]]\n",
      "at random state 26, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.79      0.78       112\n",
      "           1       0.65      0.65      0.65       104\n",
      "           2       0.93      0.92      0.92       109\n",
      "           3       0.87      0.83      0.85       108\n",
      "           4       0.85      0.94      0.89        96\n",
      "           5       0.72      0.67      0.69        94\n",
      "           6       0.94      0.91      0.92        87\n",
      "           7       0.81      0.81      0.81       102\n",
      "           8       0.92      0.96      0.94       101\n",
      "           9       0.80      0.82      0.81        97\n",
      "          10       0.69      0.62      0.65       116\n",
      "          11       0.82      0.85      0.84       115\n",
      "          12       0.91      0.81      0.86        88\n",
      "          13       0.77      0.68      0.72       113\n",
      "          14       0.77      0.80      0.78        94\n",
      "          15       0.94      0.95      0.95        85\n",
      "          16       0.88      0.85      0.87        87\n",
      "          17       0.90      0.78      0.83       112\n",
      "          18       0.78      0.74      0.76       101\n",
      "          19       0.83      0.75      0.79       115\n",
      "          20       0.90      0.88      0.89        83\n",
      "          21       0.60      0.57      0.59       105\n",
      "          22       0.93      1.00      0.96        88\n",
      "          23       0.76      0.86      0.81        86\n",
      "          24       0.67      0.61      0.64       108\n",
      "          25       0.93      0.94      0.94       106\n",
      "          26       0.65      0.73      0.69       105\n",
      "          27       0.76      0.72      0.74        92\n",
      "          28       0.88      0.86      0.87       115\n",
      "          29       0.84      0.87      0.85       101\n",
      "          30       0.89      0.87      0.88       107\n",
      "          31       0.61      0.54      0.57       106\n",
      "          32       0.79      0.73      0.76        94\n",
      "          33       0.71      0.80      0.75       105\n",
      "          34       0.82      0.82      0.82        82\n",
      "          35       0.91      0.83      0.87       102\n",
      "          36       0.68      0.73      0.70       106\n",
      "          37       0.74      0.76      0.75        94\n",
      "          38       0.83      0.84      0.84       112\n",
      "          39       0.87      0.86      0.86        99\n",
      "          40       0.76      0.86      0.81        99\n",
      "          41       0.74      0.73      0.74        94\n",
      "          42       0.91      0.86      0.89        86\n",
      "          43       0.70      0.81      0.75       104\n",
      "          44       0.79      0.78      0.78       109\n",
      "          45       0.93      0.92      0.92        85\n",
      "          46       0.93      0.87      0.90        87\n",
      "          47       0.80      0.80      0.80        98\n",
      "          48       0.76      0.72      0.74       105\n",
      "          49       0.77      0.76      0.76        94\n",
      "          50       0.41      0.53      0.46        89\n",
      "          51       0.98      0.98      0.98        51\n",
      "          52       0.88      0.94      0.91        68\n",
      "          53       0.68      0.75      0.71        95\n",
      "\n",
      "    accuracy                           0.80      5296\n",
      "   macro avg       0.80      0.80      0.80      5296\n",
      "weighted avg       0.80      0.80      0.80      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 27,  accuracy score is 1.0\n",
      "at random state 27, confusion matrix is [[79  0  0 ...  0  0  0]\n",
      " [ 0 70  0 ...  0  0  0]\n",
      " [ 0  0 96 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 57  0  0]\n",
      " [ 0  0  0 ...  0 87  0]\n",
      " [ 0  0  0 ...  0  0 73]]\n",
      "at random state 27, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.69      0.76       115\n",
      "           1       0.75      0.69      0.72       102\n",
      "           2       0.91      0.86      0.88       112\n",
      "           3       0.85      0.89      0.87        99\n",
      "           4       0.80      0.88      0.84       112\n",
      "           5       0.65      0.72      0.68        92\n",
      "           6       0.94      0.92      0.93        86\n",
      "           7       0.77      0.86      0.82        87\n",
      "           8       0.93      0.92      0.92       108\n",
      "           9       0.80      0.80      0.80        95\n",
      "          10       0.66      0.73      0.69       103\n",
      "          11       0.84      0.88      0.86       102\n",
      "          12       0.88      0.76      0.82       111\n",
      "          13       0.74      0.79      0.76       103\n",
      "          14       0.87      0.77      0.82       106\n",
      "          15       0.91      0.90      0.91        71\n",
      "          16       0.92      0.85      0.88       106\n",
      "          17       0.85      0.80      0.82       108\n",
      "          18       0.88      0.81      0.84       103\n",
      "          19       0.81      0.86      0.83       101\n",
      "          20       0.86      0.79      0.82        92\n",
      "          21       0.64      0.55      0.59       110\n",
      "          22       0.93      0.87      0.90        97\n",
      "          23       0.80      0.80      0.80       101\n",
      "          24       0.58      0.62      0.60        95\n",
      "          25       0.94      0.91      0.92       100\n",
      "          26       0.69      0.71      0.70        86\n",
      "          27       0.77      0.89      0.83        93\n",
      "          28       0.93      0.76      0.83        90\n",
      "          29       0.90      0.95      0.92        93\n",
      "          30       0.90      0.88      0.89        99\n",
      "          31       0.59      0.59      0.59        92\n",
      "          32       0.81      0.78      0.79       106\n",
      "          33       0.69      0.65      0.67        94\n",
      "          34       0.80      0.81      0.80       106\n",
      "          35       0.77      0.91      0.84        98\n",
      "          36       0.64      0.70      0.67        93\n",
      "          37       0.83      0.70      0.76       102\n",
      "          38       0.75      0.85      0.80        96\n",
      "          39       0.74      0.78      0.76        91\n",
      "          40       0.89      0.81      0.85       104\n",
      "          41       0.78      0.81      0.79       108\n",
      "          42       0.88      0.92      0.90        91\n",
      "          43       0.73      0.85      0.79       111\n",
      "          44       0.86      0.85      0.86       112\n",
      "          45       0.93      0.93      0.93        87\n",
      "          46       0.92      0.89      0.90        90\n",
      "          47       0.77      0.73      0.75        98\n",
      "          48       0.67      0.70      0.68        96\n",
      "          49       0.74      0.74      0.74        98\n",
      "          50       0.50      0.49      0.50        99\n",
      "          51       0.98      1.00      0.99        57\n",
      "          52       0.93      0.96      0.94        91\n",
      "          53       0.63      0.74      0.69        98\n",
      "\n",
      "    accuracy                           0.80      5296\n",
      "   macro avg       0.80      0.80      0.80      5296\n",
      "weighted avg       0.80      0.80      0.80      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 28,  accuracy score is 1.0\n",
      "at random state 28, confusion matrix is [[92  0  0 ...  0  0  0]\n",
      " [ 0 70  0 ...  0  0  0]\n",
      " [ 0  0 83 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 46  0  0]\n",
      " [ 0  0  0 ...  0 70  0]\n",
      " [ 0  0  1 ...  0  0 66]]\n",
      "at random state 28, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.79      0.82       117\n",
      "           1       0.67      0.74      0.70        94\n",
      "           2       0.82      0.89      0.86        93\n",
      "           3       0.77      0.84      0.80        97\n",
      "           4       0.85      0.87      0.86       105\n",
      "           5       0.66      0.69      0.67        88\n",
      "           6       0.92      0.98      0.95        96\n",
      "           7       0.82      0.88      0.85        90\n",
      "           8       0.89      0.88      0.88        96\n",
      "           9       0.81      0.87      0.84       105\n",
      "          10       0.63      0.69      0.66        93\n",
      "          11       0.88      0.81      0.84       105\n",
      "          12       0.89      0.82      0.85       107\n",
      "          13       0.73      0.72      0.73        96\n",
      "          14       0.85      0.76      0.80       100\n",
      "          15       0.96      0.89      0.92        80\n",
      "          16       0.78      0.85      0.81        95\n",
      "          17       0.75      0.78      0.76       103\n",
      "          18       0.84      0.87      0.85       111\n",
      "          19       0.76      0.66      0.71        97\n",
      "          20       0.74      0.88      0.80       100\n",
      "          21       0.71      0.61      0.66       111\n",
      "          22       0.92      0.91      0.92       107\n",
      "          23       0.74      0.77      0.76        88\n",
      "          24       0.70      0.64      0.67       113\n",
      "          25       0.94      0.95      0.95       100\n",
      "          26       0.61      0.58      0.60       108\n",
      "          27       0.69      0.77      0.73        93\n",
      "          28       0.90      0.70      0.79       113\n",
      "          29       0.86      0.91      0.88        97\n",
      "          30       0.83      0.89      0.86        96\n",
      "          31       0.68      0.61      0.64       104\n",
      "          32       0.86      0.72      0.79       105\n",
      "          33       0.72      0.72      0.72       105\n",
      "          34       0.85      0.76      0.80       108\n",
      "          35       0.80      0.86      0.83        90\n",
      "          36       0.74      0.67      0.70        94\n",
      "          37       0.74      0.80      0.77       101\n",
      "          38       0.78      0.72      0.75       110\n",
      "          39       0.82      0.77      0.79        98\n",
      "          40       0.85      0.84      0.85       103\n",
      "          41       0.75      0.81      0.78        95\n",
      "          42       0.87      0.89      0.88        92\n",
      "          43       0.72      0.71      0.71        95\n",
      "          44       0.79      0.81      0.80       106\n",
      "          45       0.94      0.99      0.96        89\n",
      "          46       0.90      0.90      0.90       100\n",
      "          47       0.66      0.70      0.68        97\n",
      "          48       0.81      0.68      0.74       106\n",
      "          49       0.75      0.76      0.76       110\n",
      "          50       0.50      0.60      0.55        85\n",
      "          51       1.00      1.00      1.00        46\n",
      "          52       0.92      0.90      0.91        78\n",
      "          53       0.62      0.78      0.69        85\n",
      "\n",
      "    accuracy                           0.79      5296\n",
      "   macro avg       0.79      0.79      0.79      5296\n",
      "weighted avg       0.79      0.79      0.79      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 29,  accuracy score is 1.0\n",
      "at random state 29, confusion matrix is [[78  0  0 ...  0  0  0]\n",
      " [ 0 66  0 ...  0  0  1]\n",
      " [ 0  0 68 ...  0  0  4]\n",
      " ...\n",
      " [ 0  0  0 ... 55  0  0]\n",
      " [ 0  0  0 ...  0 76  0]\n",
      " [ 0  1  2 ...  0  0 67]]\n",
      "at random state 29, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.78      0.77       100\n",
      "           1       0.65      0.69      0.67        95\n",
      "           2       0.88      0.85      0.87        80\n",
      "           3       0.80      0.87      0.84        93\n",
      "           4       0.81      0.92      0.86        96\n",
      "           5       0.71      0.64      0.68       101\n",
      "           6       0.96      0.90      0.93       101\n",
      "           7       0.92      0.84      0.88        86\n",
      "           8       0.90      0.86      0.88        96\n",
      "           9       0.81      0.73      0.77       125\n",
      "          10       0.61      0.72      0.66       109\n",
      "          11       0.81      0.84      0.82        99\n",
      "          12       0.80      0.82      0.81        90\n",
      "          13       0.63      0.70      0.67       101\n",
      "          14       0.81      0.75      0.78        97\n",
      "          15       0.91      0.97      0.94        87\n",
      "          16       0.81      0.91      0.86        87\n",
      "          17       0.87      0.81      0.84       101\n",
      "          18       0.84      0.78      0.81       108\n",
      "          19       0.75      0.77      0.76       104\n",
      "          20       0.90      0.79      0.85       107\n",
      "          21       0.77      0.65      0.71       111\n",
      "          22       0.92      0.92      0.92       104\n",
      "          23       0.77      0.71      0.74        87\n",
      "          24       0.58      0.58      0.58        96\n",
      "          25       0.96      0.93      0.95       105\n",
      "          26       0.62      0.55      0.59       110\n",
      "          27       0.73      0.79      0.76        91\n",
      "          28       0.85      0.78      0.81       121\n",
      "          29       0.82      0.83      0.83       101\n",
      "          30       0.86      0.88      0.87       104\n",
      "          31       0.57      0.58      0.57        93\n",
      "          32       0.79      0.74      0.76       109\n",
      "          33       0.76      0.82      0.79        95\n",
      "          34       0.77      0.73      0.75        89\n",
      "          35       0.89      0.89      0.89       106\n",
      "          36       0.73      0.72      0.72       109\n",
      "          37       0.74      0.81      0.78        90\n",
      "          38       0.83      0.85      0.84       105\n",
      "          39       0.81      0.85      0.83       102\n",
      "          40       0.79      0.81      0.80       104\n",
      "          41       0.74      0.76      0.75        98\n",
      "          42       0.96      0.82      0.89       113\n",
      "          43       0.74      0.76      0.75        96\n",
      "          44       0.79      0.78      0.79        87\n",
      "          45       0.95      0.97      0.96        91\n",
      "          46       0.92      0.94      0.93        97\n",
      "          47       0.76      0.64      0.69        85\n",
      "          48       0.67      0.69      0.68       101\n",
      "          49       0.70      0.77      0.73        94\n",
      "          50       0.50      0.56      0.53       108\n",
      "          51       1.00      1.00      1.00        55\n",
      "          52       0.92      0.97      0.94        78\n",
      "          53       0.62      0.68      0.65        98\n",
      "\n",
      "    accuracy                           0.79      5296\n",
      "   macro avg       0.79      0.79      0.79      5296\n",
      "weighted avg       0.79      0.79      0.79      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 30,  accuracy score is 1.0\n",
      "at random state 30, confusion matrix is [[76  0  0 ...  0  0  0]\n",
      " [ 0 74  0 ...  0  0  3]\n",
      " [ 0  0 91 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 55  0  0]\n",
      " [ 0  0  0 ...  0 74  0]\n",
      " [ 0  1  3 ...  0  0 70]]\n",
      "at random state 30, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.75      0.75       102\n",
      "           1       0.73      0.72      0.73       103\n",
      "           2       0.88      0.90      0.89       101\n",
      "           3       0.82      0.84      0.83        96\n",
      "           4       0.89      0.89      0.89       119\n",
      "           5       0.66      0.69      0.67       109\n",
      "           6       0.95      0.96      0.96        85\n",
      "           7       0.78      0.74      0.76        90\n",
      "           8       0.94      0.94      0.94       109\n",
      "           9       0.83      0.78      0.80       108\n",
      "          10       0.72      0.75      0.73       110\n",
      "          11       0.84      0.88      0.86       103\n",
      "          12       0.87      0.79      0.83       105\n",
      "          13       0.68      0.81      0.74        94\n",
      "          14       0.88      0.84      0.86        99\n",
      "          15       0.99      0.92      0.95        85\n",
      "          16       0.95      0.90      0.92       116\n",
      "          17       0.87      0.81      0.84        96\n",
      "          18       0.82      0.85      0.83       100\n",
      "          19       0.75      0.83      0.79       101\n",
      "          20       0.84      0.86      0.85        99\n",
      "          21       0.57      0.71      0.63        93\n",
      "          22       0.93      0.94      0.93        99\n",
      "          23       0.73      0.82      0.78        84\n",
      "          24       0.61      0.71      0.65        99\n",
      "          25       0.90      0.97      0.93        96\n",
      "          26       0.74      0.67      0.70       101\n",
      "          27       0.79      0.81      0.80       101\n",
      "          28       0.86      0.81      0.83       103\n",
      "          29       0.89      0.94      0.91        94\n",
      "          30       0.88      0.88      0.88       108\n",
      "          31       0.59      0.65      0.62        94\n",
      "          32       0.75      0.76      0.76        97\n",
      "          33       0.76      0.71      0.73       107\n",
      "          34       0.87      0.81      0.84       103\n",
      "          35       0.91      0.82      0.87       114\n",
      "          36       0.72      0.68      0.70        92\n",
      "          37       0.70      0.70      0.70        91\n",
      "          38       0.71      0.78      0.74        94\n",
      "          39       0.80      0.82      0.81       107\n",
      "          40       0.80      0.85      0.82        85\n",
      "          41       0.79      0.73      0.76        99\n",
      "          42       0.89      0.89      0.89        87\n",
      "          43       0.80      0.72      0.76        96\n",
      "          44       0.82      0.80      0.81       119\n",
      "          45       0.93      0.91      0.92        86\n",
      "          46       0.80      0.91      0.85        90\n",
      "          47       0.77      0.76      0.77        84\n",
      "          48       0.74      0.68      0.71        99\n",
      "          49       0.77      0.66      0.71       109\n",
      "          50       0.52      0.49      0.51        89\n",
      "          51       1.00      0.98      0.99        56\n",
      "          52       0.90      0.93      0.91        80\n",
      "          53       0.75      0.64      0.69       110\n",
      "\n",
      "    accuracy                           0.80      5296\n",
      "   macro avg       0.80      0.80      0.80      5296\n",
      "weighted avg       0.80      0.80      0.80      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 31,  accuracy score is 1.0\n",
      "at random state 31, confusion matrix is [[85  0  0 ...  0  0  0]\n",
      " [ 0 73  0 ...  0  0  0]\n",
      " [ 0  0 87 ...  0  0  1]\n",
      " ...\n",
      " [ 0  0  0 ... 46  0  0]\n",
      " [ 0  0  0 ...  0 64  0]\n",
      " [ 0  0  0 ...  0  0 66]]\n",
      "at random state 31, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.82      0.86       104\n",
      "           1       0.68      0.73      0.70       100\n",
      "           2       0.92      0.94      0.93        93\n",
      "           3       0.70      0.75      0.73        83\n",
      "           4       0.86      0.88      0.87       101\n",
      "           5       0.65      0.71      0.68       108\n",
      "           6       0.95      0.92      0.93        99\n",
      "           7       0.78      0.83      0.80        96\n",
      "           8       0.89      0.95      0.92        94\n",
      "           9       0.84      0.82      0.83        98\n",
      "          10       0.62      0.73      0.67       102\n",
      "          11       0.82      0.84      0.83       107\n",
      "          12       0.92      0.82      0.86        98\n",
      "          13       0.88      0.72      0.79       112\n",
      "          14       0.83      0.83      0.83        98\n",
      "          15       0.93      0.95      0.94        83\n",
      "          16       0.86      0.93      0.89       110\n",
      "          17       0.82      0.78      0.80        99\n",
      "          18       0.84      0.80      0.82       122\n",
      "          19       0.78      0.80      0.79        95\n",
      "          20       0.82      0.83      0.83       108\n",
      "          21       0.55      0.70      0.61        94\n",
      "          22       0.93      0.95      0.94       105\n",
      "          23       0.85      0.67      0.75       118\n",
      "          24       0.67      0.61      0.63        99\n",
      "          25       0.94      0.94      0.94        94\n",
      "          26       0.56      0.62      0.58       104\n",
      "          27       0.78      0.77      0.77       115\n",
      "          28       0.79      0.71      0.75        90\n",
      "          29       0.90      0.86      0.88       101\n",
      "          30       0.89      0.85      0.87        94\n",
      "          31       0.68      0.70      0.69        96\n",
      "          32       0.78      0.77      0.78       106\n",
      "          33       0.76      0.76      0.76        97\n",
      "          34       0.72      0.79      0.75       104\n",
      "          35       0.84      0.87      0.86        99\n",
      "          36       0.64      0.68      0.66        94\n",
      "          37       0.76      0.75      0.76       104\n",
      "          38       0.72      0.85      0.78        92\n",
      "          39       0.72      0.84      0.78        93\n",
      "          40       0.89      0.72      0.80        97\n",
      "          41       0.79      0.72      0.76       105\n",
      "          42       0.90      0.82      0.86        95\n",
      "          43       0.72      0.72      0.72        97\n",
      "          44       0.85      0.73      0.78       113\n",
      "          45       0.93      0.97      0.95        90\n",
      "          46       0.88      0.84      0.86        89\n",
      "          47       0.76      0.70      0.73       102\n",
      "          48       0.61      0.52      0.56        93\n",
      "          49       0.76      0.80      0.78        91\n",
      "          50       0.53      0.56      0.54       105\n",
      "          51       1.00      0.98      0.99        47\n",
      "          52       0.90      0.90      0.90        71\n",
      "          53       0.67      0.72      0.69        92\n",
      "\n",
      "    accuracy                           0.79      5296\n",
      "   macro avg       0.80      0.79      0.79      5296\n",
      "weighted avg       0.79      0.79      0.79      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 32,  accuracy score is 1.0\n",
      "at random state 32, confusion matrix is [[87  0  0 ...  0  0  0]\n",
      " [ 0 62  0 ...  0  0  1]\n",
      " [ 0  0 83 ...  0  0  2]\n",
      " ...\n",
      " [ 0  0  0 ... 55  0  0]\n",
      " [ 0  0  0 ...  0 74  0]\n",
      " [ 0  1  0 ...  0  0 66]]\n",
      "at random state 32, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.84      0.84       104\n",
      "           1       0.63      0.70      0.67        88\n",
      "           2       0.87      0.87      0.87        95\n",
      "           3       0.81      0.75      0.78       101\n",
      "           4       0.88      0.83      0.86       101\n",
      "           5       0.70      0.66      0.68       103\n",
      "           6       0.93      0.89      0.91        70\n",
      "           7       0.84      0.80      0.82        97\n",
      "           8       0.88      0.90      0.89       102\n",
      "           9       0.80      0.80      0.80        92\n",
      "          10       0.67      0.69      0.68       107\n",
      "          11       0.84      0.85      0.85       107\n",
      "          12       0.79      0.84      0.82       101\n",
      "          13       0.72      0.69      0.71        88\n",
      "          14       0.81      0.77      0.79       114\n",
      "          15       0.97      0.97      0.97        77\n",
      "          16       0.88      0.94      0.91       104\n",
      "          17       0.81      0.74      0.77       102\n",
      "          18       0.77      0.84      0.80        83\n",
      "          19       0.77      0.75      0.76        95\n",
      "          20       0.90      0.79      0.84       108\n",
      "          21       0.70      0.61      0.65       100\n",
      "          22       0.92      0.95      0.93        94\n",
      "          23       0.69      0.81      0.74        93\n",
      "          24       0.59      0.58      0.59       100\n",
      "          25       0.98      0.96      0.97       103\n",
      "          26       0.63      0.70      0.66       105\n",
      "          27       0.74      0.82      0.78        95\n",
      "          28       0.85      0.83      0.84        92\n",
      "          29       0.90      0.91      0.90        96\n",
      "          30       0.83      0.90      0.86       106\n",
      "          31       0.67      0.55      0.60        99\n",
      "          32       0.73      0.75      0.74        99\n",
      "          33       0.79      0.67      0.72       112\n",
      "          34       0.83      0.81      0.82        96\n",
      "          35       0.85      0.94      0.89       103\n",
      "          36       0.68      0.64      0.66       110\n",
      "          37       0.70      0.79      0.74       110\n",
      "          38       0.80      0.85      0.82        96\n",
      "          39       0.83      0.86      0.85       110\n",
      "          40       0.82      0.81      0.81        95\n",
      "          41       0.78      0.67      0.72       103\n",
      "          42       0.85      0.84      0.85        82\n",
      "          43       0.74      0.74      0.74       105\n",
      "          44       0.78      0.80      0.79       100\n",
      "          45       0.94      0.96      0.95        99\n",
      "          46       0.89      0.89      0.89        89\n",
      "          47       0.80      0.75      0.78       102\n",
      "          48       0.74      0.61      0.67       118\n",
      "          49       0.83      0.77      0.80       112\n",
      "          50       0.48      0.65      0.55       104\n",
      "          51       1.00      0.98      0.99        56\n",
      "          52       0.91      0.90      0.91        82\n",
      "          53       0.65      0.73      0.68        91\n",
      "\n",
      "    accuracy                           0.79      5296\n",
      "   macro avg       0.80      0.80      0.80      5296\n",
      "weighted avg       0.79      0.79      0.79      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 33,  accuracy score is 1.0\n",
      "at random state 33, confusion matrix is [[ 77   0   0 ...   0   0   0]\n",
      " [  0  67   0 ...   0   0   3]\n",
      " [  0   0 104 ...   0   0   1]\n",
      " ...\n",
      " [  0   0   0 ...  58   0   0]\n",
      " [  0   0   0 ...   0  78   0]\n",
      " [  0   0   2 ...   0   0  64]]\n",
      "at random state 33, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.75      0.73       103\n",
      "           1       0.74      0.63      0.68       107\n",
      "           2       0.88      0.90      0.89       115\n",
      "           3       0.79      0.86      0.82        99\n",
      "           4       0.90      0.80      0.85       104\n",
      "           5       0.76      0.75      0.76       106\n",
      "           6       0.94      0.93      0.94        90\n",
      "           7       0.81      0.84      0.82       117\n",
      "           8       0.97      0.91      0.94        92\n",
      "           9       0.75      0.76      0.76        97\n",
      "          10       0.60      0.62      0.61        94\n",
      "          11       0.87      0.82      0.84       116\n",
      "          12       0.79      0.86      0.82       106\n",
      "          13       0.75      0.80      0.78        97\n",
      "          14       0.85      0.81      0.83       101\n",
      "          15       0.89      0.88      0.89        77\n",
      "          16       0.85      0.88      0.87        86\n",
      "          17       0.79      0.84      0.81        97\n",
      "          18       0.84      0.85      0.84        98\n",
      "          19       0.86      0.72      0.78       108\n",
      "          20       0.90      0.80      0.85       100\n",
      "          21       0.64      0.64      0.64        90\n",
      "          22       0.92      0.93      0.92       104\n",
      "          23       0.78      0.70      0.74       102\n",
      "          24       0.59      0.63      0.61       100\n",
      "          25       0.96      0.88      0.91       104\n",
      "          26       0.59      0.62      0.60       105\n",
      "          27       0.74      0.81      0.78        96\n",
      "          28       0.86      0.85      0.86       101\n",
      "          29       0.88      0.90      0.89        87\n",
      "          30       0.75      0.83      0.79        89\n",
      "          31       0.59      0.63      0.61        98\n",
      "          32       0.76      0.81      0.78       104\n",
      "          33       0.73      0.74      0.73        95\n",
      "          34       0.75      0.77      0.76        99\n",
      "          35       0.85      0.92      0.88       123\n",
      "          36       0.63      0.73      0.68       101\n",
      "          37       0.79      0.72      0.75        97\n",
      "          38       0.78      0.87      0.82        92\n",
      "          39       0.75      0.80      0.77        88\n",
      "          40       0.79      0.81      0.80        94\n",
      "          41       0.77      0.70      0.73       120\n",
      "          42       0.84      0.93      0.88        76\n",
      "          43       0.60      0.59      0.60        95\n",
      "          44       0.81      0.79      0.80       107\n",
      "          45       0.95      0.97      0.96        79\n",
      "          46       0.95      0.84      0.89        91\n",
      "          47       0.78      0.72      0.74       102\n",
      "          48       0.72      0.71      0.71        95\n",
      "          49       0.76      0.74      0.75       100\n",
      "          50       0.53      0.46      0.49       112\n",
      "          51       0.98      1.00      0.99        58\n",
      "          52       0.93      0.97      0.95        80\n",
      "          53       0.66      0.63      0.64       102\n",
      "\n",
      "    accuracy                           0.78      5296\n",
      "   macro avg       0.79      0.79      0.79      5296\n",
      "weighted avg       0.79      0.78      0.78      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 34,  accuracy score is 1.0\n",
      "at random state 34, confusion matrix is [[ 73   0   0 ...   0   0   0]\n",
      " [  0  76   0 ...   0   0   4]\n",
      " [  0   0 102 ...   0   0   3]\n",
      " ...\n",
      " [  0   0   0 ...  43   0   0]\n",
      " [  0   0   0 ...   0  72   0]\n",
      " [  0   2   3 ...   0   0  74]]\n",
      "at random state 34, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.77      0.80        95\n",
      "           1       0.68      0.77      0.72        99\n",
      "           2       0.88      0.93      0.90       110\n",
      "           3       0.82      0.79      0.81       101\n",
      "           4       0.87      0.80      0.84       112\n",
      "           5       0.71      0.64      0.67        95\n",
      "           6       0.88      0.96      0.92        83\n",
      "           7       0.79      0.85      0.82        87\n",
      "           8       0.97      0.92      0.94       101\n",
      "           9       0.87      0.85      0.86       106\n",
      "          10       0.60      0.58      0.59       100\n",
      "          11       0.88      0.80      0.83       108\n",
      "          12       0.86      0.84      0.85        99\n",
      "          13       0.83      0.67      0.74       106\n",
      "          14       0.78      0.79      0.78        98\n",
      "          15       0.98      0.93      0.95        85\n",
      "          16       0.88      0.88      0.88        98\n",
      "          17       0.81      0.79      0.80       109\n",
      "          18       0.85      0.88      0.86       100\n",
      "          19       0.77      0.80      0.79        97\n",
      "          20       0.78      0.83      0.80       111\n",
      "          21       0.61      0.60      0.60       105\n",
      "          22       0.93      0.94      0.94       104\n",
      "          23       0.74      0.77      0.76        97\n",
      "          24       0.48      0.54      0.51        85\n",
      "          25       0.92      0.96      0.94       101\n",
      "          26       0.59      0.67      0.63       101\n",
      "          27       0.69      0.79      0.74       101\n",
      "          28       0.92      0.85      0.89       122\n",
      "          29       0.96      0.94      0.95       102\n",
      "          30       0.83      0.87      0.85        98\n",
      "          31       0.64      0.66      0.65        97\n",
      "          32       0.79      0.80      0.79       119\n",
      "          33       0.76      0.74      0.75       106\n",
      "          34       0.85      0.81      0.83       115\n",
      "          35       0.87      0.89      0.88       100\n",
      "          36       0.63      0.67      0.65        97\n",
      "          37       0.65      0.67      0.66        86\n",
      "          38       0.76      0.81      0.78       113\n",
      "          39       0.85      0.90      0.87        97\n",
      "          40       0.74      0.80      0.77        80\n",
      "          41       0.74      0.72      0.73       108\n",
      "          42       0.97      0.86      0.91        96\n",
      "          43       0.65      0.68      0.66        94\n",
      "          44       0.82      0.82      0.82        99\n",
      "          45       0.93      0.89      0.91        93\n",
      "          46       0.88      0.92      0.90        91\n",
      "          47       0.84      0.78      0.81        98\n",
      "          48       0.69      0.63      0.66        89\n",
      "          49       0.76      0.72      0.74        78\n",
      "          50       0.45      0.44      0.44        93\n",
      "          51       1.00      1.00      1.00        43\n",
      "          52       0.96      0.94      0.95        77\n",
      "          53       0.69      0.67      0.68       111\n",
      "\n",
      "    accuracy                           0.79      5296\n",
      "   macro avg       0.79      0.79      0.79      5296\n",
      "weighted avg       0.79      0.79      0.79      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 35,  accuracy score is 1.0\n",
      "at random state 35, confusion matrix is [[ 77   0   0 ...   0   0   0]\n",
      " [  0  66   0 ...   0   0   2]\n",
      " [  0   0 106 ...   0   0   2]\n",
      " ...\n",
      " [  0   0   0 ...  43   0   0]\n",
      " [  0   0   0 ...   0  74   0]\n",
      " [  0   2   2 ...   0   0  64]]\n",
      "at random state 35, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.82      0.84        94\n",
      "           1       0.69      0.70      0.70        94\n",
      "           2       0.85      0.91      0.88       117\n",
      "           3       0.74      0.78      0.76        95\n",
      "           4       0.83      0.93      0.88        99\n",
      "           5       0.61      0.80      0.69        75\n",
      "           6       0.94      0.93      0.93       100\n",
      "           7       0.84      0.79      0.81        97\n",
      "           8       0.93      0.95      0.94        96\n",
      "           9       0.86      0.78      0.82       110\n",
      "          10       0.67      0.68      0.67       112\n",
      "          11       0.87      0.89      0.88       109\n",
      "          12       0.90      0.82      0.86       100\n",
      "          13       0.70      0.80      0.74       118\n",
      "          14       0.81      0.79      0.80        97\n",
      "          15       0.98      0.98      0.98        81\n",
      "          16       0.90      0.89      0.90       113\n",
      "          17       0.88      0.84      0.86       104\n",
      "          18       0.83      0.82      0.83       122\n",
      "          19       0.77      0.78      0.78        91\n",
      "          20       0.86      0.92      0.89        90\n",
      "          21       0.59      0.62      0.60       100\n",
      "          22       0.90      0.90      0.90        87\n",
      "          23       0.76      0.80      0.78       103\n",
      "          24       0.64      0.66      0.65       101\n",
      "          25       0.94      0.92      0.93       111\n",
      "          26       0.62      0.62      0.62       101\n",
      "          27       0.77      0.80      0.78       109\n",
      "          28       0.88      0.85      0.87       102\n",
      "          29       0.90      0.88      0.89       112\n",
      "          30       0.93      0.88      0.90       105\n",
      "          31       0.67      0.62      0.64       104\n",
      "          32       0.88      0.77      0.82       103\n",
      "          33       0.71      0.65      0.68        99\n",
      "          34       0.80      0.69      0.74        93\n",
      "          35       0.85      0.84      0.84       113\n",
      "          36       0.73      0.67      0.70        88\n",
      "          37       0.80      0.79      0.79       103\n",
      "          38       0.75      0.74      0.74        92\n",
      "          39       0.78      0.87      0.82       100\n",
      "          40       0.79      0.80      0.79        93\n",
      "          41       0.77      0.75      0.76       106\n",
      "          42       0.82      0.92      0.87        87\n",
      "          43       0.71      0.76      0.73        94\n",
      "          44       0.76      0.72      0.74        90\n",
      "          45       0.83      0.94      0.88        79\n",
      "          46       0.88      0.91      0.89        98\n",
      "          47       0.76      0.71      0.74        91\n",
      "          48       0.71      0.65      0.68       103\n",
      "          49       0.77      0.67      0.71        90\n",
      "          50       0.50      0.58      0.54       104\n",
      "          51       0.98      1.00      0.99        43\n",
      "          52       0.95      0.96      0.95        77\n",
      "          53       0.72      0.63      0.67       101\n",
      "\n",
      "    accuracy                           0.80      5296\n",
      "   macro avg       0.80      0.80      0.80      5296\n",
      "weighted avg       0.80      0.80      0.80      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 36,  accuracy score is 1.0\n",
      "at random state 36, confusion matrix is [[85  0  0 ...  0  0  0]\n",
      " [ 0 67  0 ...  0  0  2]\n",
      " [ 0  0 97 ...  0  0  2]\n",
      " ...\n",
      " [ 0  0  0 ... 54  1  0]\n",
      " [ 0  0  0 ...  0 65  0]\n",
      " [ 0  2  0 ...  0  0 73]]\n",
      "at random state 36, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.79      0.80       108\n",
      "           1       0.61      0.72      0.66        93\n",
      "           2       0.97      0.88      0.92       110\n",
      "           3       0.79      0.75      0.77        96\n",
      "           4       0.83      0.83      0.83        98\n",
      "           5       0.74      0.71      0.73       112\n",
      "           6       0.96      0.89      0.92        88\n",
      "           7       0.82      0.82      0.82       106\n",
      "           8       0.92      0.89      0.90        99\n",
      "           9       0.80      0.77      0.79       102\n",
      "          10       0.70      0.59      0.64       116\n",
      "          11       0.84      0.82      0.83       114\n",
      "          12       0.85      0.86      0.85       105\n",
      "          13       0.73      0.79      0.76       106\n",
      "          14       0.77      0.68      0.72       108\n",
      "          15       0.90      0.94      0.92        78\n",
      "          16       0.88      0.88      0.88       104\n",
      "          17       0.82      0.77      0.80        97\n",
      "          18       0.83      0.76      0.79       101\n",
      "          19       0.80      0.75      0.77        99\n",
      "          20       0.77      0.87      0.82        94\n",
      "          21       0.68      0.67      0.67        97\n",
      "          22       0.95      0.88      0.91        96\n",
      "          23       0.73      0.81      0.77       106\n",
      "          24       0.58      0.66      0.62        92\n",
      "          25       0.89      0.95      0.92       115\n",
      "          26       0.65      0.74      0.69        99\n",
      "          27       0.81      0.74      0.77       113\n",
      "          28       0.83      0.81      0.82       107\n",
      "          29       0.92      0.84      0.88        90\n",
      "          30       0.89      0.87      0.88       107\n",
      "          31       0.64      0.70      0.67       100\n",
      "          32       0.78      0.76      0.77       103\n",
      "          33       0.72      0.81      0.76        80\n",
      "          34       0.77      0.74      0.75        97\n",
      "          35       0.84      0.89      0.86       106\n",
      "          36       0.58      0.58      0.58        78\n",
      "          37       0.66      0.76      0.71        93\n",
      "          38       0.78      0.77      0.78       106\n",
      "          39       0.75      0.76      0.76        80\n",
      "          40       0.76      0.80      0.78       102\n",
      "          41       0.75      0.74      0.74       106\n",
      "          42       0.95      0.82      0.88        93\n",
      "          43       0.69      0.73      0.71       106\n",
      "          44       0.81      0.76      0.78       108\n",
      "          45       0.92      0.88      0.90        78\n",
      "          46       0.88      0.90      0.89        82\n",
      "          47       0.71      0.89      0.79        88\n",
      "          48       0.73      0.70      0.71       100\n",
      "          49       0.72      0.67      0.69        96\n",
      "          50       0.55      0.47      0.51       110\n",
      "          51       0.98      0.98      0.98        55\n",
      "          52       0.82      0.90      0.86        72\n",
      "          53       0.68      0.72      0.70       101\n",
      "\n",
      "    accuracy                           0.78      5296\n",
      "   macro avg       0.79      0.79      0.79      5296\n",
      "weighted avg       0.79      0.78      0.78      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 37,  accuracy score is 1.0\n",
      "at random state 37, confusion matrix is [[82  0  0 ...  0  0  0]\n",
      " [ 0 63  0 ...  0  0  0]\n",
      " [ 0  0 87 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 50  0  0]\n",
      " [ 0  0  0 ...  0 65  0]\n",
      " [ 0  2  1 ...  0  0 67]]\n",
      "at random state 37, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.73      0.77       112\n",
      "           1       0.69      0.68      0.68        93\n",
      "           2       0.87      0.90      0.88        97\n",
      "           3       0.77      0.77      0.77       103\n",
      "           4       0.83      0.83      0.83       101\n",
      "           5       0.72      0.77      0.75       102\n",
      "           6       0.90      0.92      0.91        78\n",
      "           7       0.83      0.86      0.84       105\n",
      "           8       0.90      0.94      0.92       101\n",
      "           9       0.75      0.83      0.79        94\n",
      "          10       0.67      0.69      0.68        85\n",
      "          11       0.85      0.87      0.86        97\n",
      "          12       0.79      0.84      0.82        90\n",
      "          13       0.68      0.80      0.73        89\n",
      "          14       0.78      0.78      0.78        93\n",
      "          15       0.99      0.96      0.97        77\n",
      "          16       0.89      0.85      0.87       115\n",
      "          17       0.84      0.84      0.84        80\n",
      "          18       0.85      0.74      0.79       111\n",
      "          19       0.77      0.81      0.79       106\n",
      "          20       0.85      0.76      0.80        91\n",
      "          21       0.72      0.70      0.71       102\n",
      "          22       0.94      0.96      0.95        99\n",
      "          23       0.72      0.69      0.70       113\n",
      "          24       0.61      0.68      0.64        99\n",
      "          25       0.95      0.95      0.95       111\n",
      "          26       0.68      0.62      0.65       101\n",
      "          27       0.76      0.75      0.75        95\n",
      "          28       0.75      0.81      0.78       103\n",
      "          29       0.88      0.86      0.87       102\n",
      "          30       0.90      0.91      0.90       109\n",
      "          31       0.65      0.71      0.68       101\n",
      "          32       0.86      0.78      0.82       103\n",
      "          33       0.69      0.77      0.73        96\n",
      "          34       0.79      0.80      0.79       101\n",
      "          35       0.82      0.82      0.82       102\n",
      "          36       0.75      0.72      0.73       111\n",
      "          37       0.75      0.76      0.76       101\n",
      "          38       0.75      0.76      0.76        93\n",
      "          39       0.79      0.80      0.80        95\n",
      "          40       0.82      0.82      0.82       108\n",
      "          41       0.79      0.73      0.76       116\n",
      "          42       0.88      0.88      0.88        98\n",
      "          43       0.75      0.63      0.68       102\n",
      "          44       0.76      0.84      0.80       100\n",
      "          45       0.90      0.91      0.90        86\n",
      "          46       0.88      0.94      0.91        97\n",
      "          47       0.79      0.74      0.76       116\n",
      "          48       0.81      0.70      0.75       105\n",
      "          49       0.78      0.74      0.76        86\n",
      "          50       0.54      0.52      0.53       102\n",
      "          51       0.98      1.00      0.99        50\n",
      "          52       0.89      0.86      0.87        76\n",
      "          53       0.68      0.69      0.69        97\n",
      "\n",
      "    accuracy                           0.79      5296\n",
      "   macro avg       0.80      0.80      0.80      5296\n",
      "weighted avg       0.79      0.79      0.79      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 38,  accuracy score is 1.0\n",
      "at random state 38, confusion matrix is [[79  0  0 ...  0  0  0]\n",
      " [ 0 68  0 ...  0  0  1]\n",
      " [ 0  0 94 ...  0  0  1]\n",
      " ...\n",
      " [ 0  0  0 ... 52  1  0]\n",
      " [ 0  0  0 ...  0 74  0]\n",
      " [ 0  1  1 ...  0  0 76]]\n",
      "at random state 38, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.85      0.81        93\n",
      "           1       0.77      0.72      0.75        94\n",
      "           2       0.92      0.92      0.92       102\n",
      "           3       0.70      0.82      0.76       106\n",
      "           4       0.87      0.84      0.85        94\n",
      "           5       0.77      0.70      0.74        97\n",
      "           6       0.96      0.87      0.91        78\n",
      "           7       0.84      0.78      0.81        96\n",
      "           8       0.95      0.88      0.91       107\n",
      "           9       0.74      0.82      0.78        82\n",
      "          10       0.60      0.54      0.57        90\n",
      "          11       0.84      0.86      0.85       111\n",
      "          12       0.86      0.89      0.88       107\n",
      "          13       0.82      0.72      0.77       104\n",
      "          14       0.89      0.84      0.86        85\n",
      "          15       0.92      0.96      0.94        76\n",
      "          16       0.83      0.88      0.86       108\n",
      "          17       0.80      0.79      0.79       100\n",
      "          18       0.84      0.84      0.84        98\n",
      "          19       0.74      0.80      0.77        97\n",
      "          20       0.88      0.77      0.82        91\n",
      "          21       0.71      0.66      0.69       110\n",
      "          22       0.86      0.96      0.91        95\n",
      "          23       0.82      0.72      0.77       111\n",
      "          24       0.60      0.69      0.64       107\n",
      "          25       0.94      0.93      0.93        96\n",
      "          26       0.62      0.67      0.64       102\n",
      "          27       0.76      0.74      0.75        88\n",
      "          28       0.83      0.95      0.88       101\n",
      "          29       0.85      0.81      0.83        96\n",
      "          30       0.87      0.83      0.85        96\n",
      "          31       0.68      0.59      0.64       106\n",
      "          32       0.75      0.71      0.72       112\n",
      "          33       0.81      0.75      0.78       110\n",
      "          34       0.78      0.81      0.80       102\n",
      "          35       0.90      0.83      0.86       100\n",
      "          36       0.63      0.72      0.67        93\n",
      "          37       0.62      0.78      0.70        83\n",
      "          38       0.76      0.78      0.77       102\n",
      "          39       0.81      0.89      0.85       117\n",
      "          40       0.80      0.83      0.81        94\n",
      "          41       0.74      0.80      0.77        98\n",
      "          42       0.84      0.91      0.87        96\n",
      "          43       0.85      0.74      0.79       114\n",
      "          44       0.87      0.78      0.82       120\n",
      "          45       0.86      0.96      0.91        77\n",
      "          46       0.93      0.87      0.90       103\n",
      "          47       0.78      0.70      0.74       113\n",
      "          48       0.73      0.61      0.66        94\n",
      "          49       0.75      0.83      0.79        95\n",
      "          50       0.51      0.51      0.51       109\n",
      "          51       0.98      0.98      0.98        53\n",
      "          52       0.90      0.84      0.87        88\n",
      "          53       0.66      0.77      0.71        99\n",
      "\n",
      "    accuracy                           0.79      5296\n",
      "   macro avg       0.80      0.80      0.80      5296\n",
      "weighted avg       0.80      0.79      0.79      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 39,  accuracy score is 1.0\n",
      "at random state 39, confusion matrix is [[77  0  0 ...  0  0  0]\n",
      " [ 0 64  0 ...  0  0  4]\n",
      " [ 0  0 94 ...  0  0  3]\n",
      " ...\n",
      " [ 0  0  0 ... 50  0  0]\n",
      " [ 0  0  0 ...  1 66  0]\n",
      " [ 0  0  3 ...  0  0 62]]\n",
      "at random state 39, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.83      0.83        93\n",
      "           1       0.67      0.65      0.66        98\n",
      "           2       0.88      0.90      0.89       105\n",
      "           3       0.71      0.77      0.74       100\n",
      "           4       0.82      0.94      0.87        96\n",
      "           5       0.67      0.67      0.67        99\n",
      "           6       0.94      0.89      0.92        85\n",
      "           7       0.78      0.77      0.77       108\n",
      "           8       0.91      0.93      0.92       101\n",
      "           9       0.82      0.73      0.77       105\n",
      "          10       0.60      0.66      0.63        89\n",
      "          11       0.82      0.88      0.85        89\n",
      "          12       0.82      0.81      0.81       109\n",
      "          13       0.67      0.74      0.70        92\n",
      "          14       0.78      0.81      0.80        94\n",
      "          15       0.94      0.93      0.94        88\n",
      "          16       0.89      0.86      0.88       118\n",
      "          17       0.87      0.73      0.80       109\n",
      "          18       0.84      0.84      0.84       111\n",
      "          19       0.86      0.77      0.82       115\n",
      "          20       0.89      0.85      0.87        97\n",
      "          21       0.64      0.62      0.63       103\n",
      "          22       0.89      0.92      0.91       106\n",
      "          23       0.73      0.77      0.75       107\n",
      "          24       0.56      0.60      0.58        88\n",
      "          25       0.87      0.97      0.92        95\n",
      "          26       0.60      0.63      0.61        98\n",
      "          27       0.74      0.75      0.75        96\n",
      "          28       0.88      0.86      0.87       100\n",
      "          29       0.89      0.83      0.86        84\n",
      "          30       0.92      0.84      0.88       113\n",
      "          31       0.58      0.64      0.61       111\n",
      "          32       0.77      0.71      0.74       106\n",
      "          33       0.81      0.80      0.81        86\n",
      "          34       0.77      0.80      0.78        99\n",
      "          35       0.91      0.85      0.88       110\n",
      "          36       0.84      0.68      0.75        91\n",
      "          37       0.67      0.75      0.71        87\n",
      "          38       0.73      0.81      0.77        91\n",
      "          39       0.80      0.71      0.75       113\n",
      "          40       0.78      0.87      0.82       107\n",
      "          41       0.77      0.69      0.73        97\n",
      "          42       0.85      0.89      0.87        93\n",
      "          43       0.70      0.76      0.73        90\n",
      "          44       0.73      0.79      0.76        95\n",
      "          45       0.90      0.89      0.90        92\n",
      "          46       0.90      0.87      0.89        93\n",
      "          47       0.75      0.73      0.74        86\n",
      "          48       0.68      0.68      0.68       111\n",
      "          49       0.84      0.74      0.78       110\n",
      "          50       0.52      0.48      0.50       120\n",
      "          51       0.98      1.00      0.99        50\n",
      "          52       0.90      0.88      0.89        75\n",
      "          53       0.63      0.67      0.65        92\n",
      "\n",
      "    accuracy                           0.78      5296\n",
      "   macro avg       0.79      0.79      0.79      5296\n",
      "weighted avg       0.79      0.78      0.78      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 40,  accuracy score is 1.0\n",
      "at random state 40, confusion matrix is [[76  0  0 ...  0  0  0]\n",
      " [ 0 78  0 ...  0  0  2]\n",
      " [ 0  0 89 ...  0  0  3]\n",
      " ...\n",
      " [ 0  0  0 ... 46  0  0]\n",
      " [ 0  0  0 ...  0 83  0]\n",
      " [ 0  0  1 ...  0  0 81]]\n",
      "at random state 40, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.76      0.77       100\n",
      "           1       0.70      0.78      0.74       100\n",
      "           2       0.92      0.90      0.91        99\n",
      "           3       0.80      0.85      0.82        91\n",
      "           4       0.85      0.79      0.82       111\n",
      "           5       0.74      0.68      0.71       105\n",
      "           6       0.89      0.91      0.90        79\n",
      "           7       0.72      0.85      0.78        79\n",
      "           8       0.88      0.90      0.89       105\n",
      "           9       0.85      0.80      0.82        91\n",
      "          10       0.65      0.69      0.67        97\n",
      "          11       0.83      0.83      0.83       102\n",
      "          12       0.82      0.90      0.86       104\n",
      "          13       0.69      0.73      0.71       103\n",
      "          14       0.75      0.86      0.80        83\n",
      "          15       0.96      0.95      0.95        91\n",
      "          16       0.91      0.91      0.91       100\n",
      "          17       0.83      0.77      0.80       100\n",
      "          18       0.82      0.86      0.84        98\n",
      "          19       0.81      0.78      0.79       103\n",
      "          20       0.83      0.89      0.86        89\n",
      "          21       0.63      0.61      0.62       102\n",
      "          22       0.92      0.95      0.94       108\n",
      "          23       0.75      0.68      0.71        90\n",
      "          24       0.68      0.63      0.66        95\n",
      "          25       0.90      0.98      0.94        90\n",
      "          26       0.70      0.63      0.67       109\n",
      "          27       0.79      0.80      0.79        98\n",
      "          28       0.84      0.81      0.82       100\n",
      "          29       0.86      0.84      0.85        86\n",
      "          30       0.79      0.93      0.86        96\n",
      "          31       0.66      0.66      0.66        98\n",
      "          32       0.82      0.77      0.80        96\n",
      "          33       0.77      0.72      0.74       109\n",
      "          34       0.81      0.77      0.79       113\n",
      "          35       0.87      0.90      0.88       108\n",
      "          36       0.75      0.69      0.72       105\n",
      "          37       0.75      0.71      0.73       109\n",
      "          38       0.82      0.76      0.79       123\n",
      "          39       0.81      0.75      0.78       108\n",
      "          40       0.89      0.83      0.86       112\n",
      "          41       0.83      0.76      0.79       113\n",
      "          42       0.87      0.79      0.83        96\n",
      "          43       0.68      0.70      0.69        87\n",
      "          44       0.78      0.79      0.79       107\n",
      "          45       0.96      0.91      0.94        89\n",
      "          46       0.95      0.88      0.91       107\n",
      "          47       0.75      0.83      0.79        87\n",
      "          48       0.64      0.72      0.67        78\n",
      "          49       0.72      0.64      0.68       111\n",
      "          50       0.52      0.69      0.59        84\n",
      "          51       0.96      1.00      0.98        46\n",
      "          52       0.86      0.86      0.86        96\n",
      "          53       0.69      0.74      0.71       110\n",
      "\n",
      "    accuracy                           0.79      5296\n",
      "   macro avg       0.80      0.80      0.80      5296\n",
      "weighted avg       0.80      0.79      0.79      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 41,  accuracy score is 1.0\n",
      "at random state 41, confusion matrix is [[78  0  0 ...  0  0  0]\n",
      " [ 0 75  0 ...  0  0  8]\n",
      " [ 0  0 78 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 51  0  0]\n",
      " [ 0  0  0 ...  0 74  0]\n",
      " [ 0  2  4 ...  0  0 63]]\n",
      "at random state 41, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.74      0.72       106\n",
      "           1       0.65      0.68      0.66       111\n",
      "           2       0.86      0.84      0.85        93\n",
      "           3       0.80      0.84      0.82       101\n",
      "           4       0.84      0.85      0.85       100\n",
      "           5       0.65      0.71      0.68       105\n",
      "           6       0.99      0.90      0.94        89\n",
      "           7       0.83      0.81      0.82        99\n",
      "           8       0.98      0.89      0.93       114\n",
      "           9       0.76      0.82      0.79        89\n",
      "          10       0.71      0.66      0.68        90\n",
      "          11       0.86      0.83      0.84       121\n",
      "          12       0.86      0.74      0.80       103\n",
      "          13       0.77      0.77      0.77        93\n",
      "          14       0.79      0.84      0.81        93\n",
      "          15       0.94      0.96      0.95        77\n",
      "          16       0.87      0.84      0.86       113\n",
      "          17       0.84      0.77      0.80        99\n",
      "          18       0.80      0.87      0.83       104\n",
      "          19       0.78      0.72      0.75        96\n",
      "          20       0.83      0.87      0.85        89\n",
      "          21       0.67      0.68      0.67       102\n",
      "          22       0.91      0.95      0.93       101\n",
      "          23       0.84      0.76      0.79       123\n",
      "          24       0.57      0.59      0.58        98\n",
      "          25       0.89      0.97      0.92        88\n",
      "          26       0.62      0.70      0.66        97\n",
      "          27       0.89      0.74      0.81       104\n",
      "          28       0.83      0.91      0.87        96\n",
      "          29       0.89      0.87      0.88        94\n",
      "          30       0.86      0.87      0.86        95\n",
      "          31       0.59      0.63      0.61       105\n",
      "          32       0.76      0.69      0.72       114\n",
      "          33       0.71      0.75      0.73       104\n",
      "          34       0.81      0.78      0.80       101\n",
      "          35       0.88      0.84      0.86       102\n",
      "          36       0.64      0.70      0.67        83\n",
      "          37       0.69      0.76      0.72        89\n",
      "          38       0.73      0.66      0.70        95\n",
      "          39       0.84      0.82      0.83       100\n",
      "          40       0.81      0.80      0.81       116\n",
      "          41       0.72      0.74      0.73        93\n",
      "          42       0.89      0.85      0.87       100\n",
      "          43       0.76      0.72      0.74        92\n",
      "          44       0.76      0.81      0.78       101\n",
      "          45       0.92      0.93      0.92        84\n",
      "          46       0.88      0.89      0.89        91\n",
      "          47       0.68      0.72      0.70       102\n",
      "          48       0.67      0.63      0.65        92\n",
      "          49       0.76      0.71      0.73       111\n",
      "          50       0.53      0.55      0.54       110\n",
      "          51       1.00      1.00      1.00        51\n",
      "          52       0.93      0.93      0.93        80\n",
      "          53       0.61      0.65      0.63        97\n",
      "\n",
      "    accuracy                           0.78      5296\n",
      "   macro avg       0.79      0.79      0.79      5296\n",
      "weighted avg       0.79      0.78      0.78      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 42,  accuracy score is 1.0\n",
      "at random state 42, confusion matrix is [[80  0  0 ...  0  0  0]\n",
      " [ 0 71  0 ...  0  0  5]\n",
      " [ 0  0 84 ...  0  0  1]\n",
      " ...\n",
      " [ 0  0  0 ... 50  0  0]\n",
      " [ 0  0  0 ...  1 59  0]\n",
      " [ 0  0  0 ...  0  0 57]]\n",
      "at random state 42, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.78      0.78       103\n",
      "           1       0.70      0.72      0.71        99\n",
      "           2       0.90      0.84      0.87       100\n",
      "           3       0.82      0.77      0.79       100\n",
      "           4       0.88      0.87      0.88       103\n",
      "           5       0.69      0.71      0.70       105\n",
      "           6       0.96      0.96      0.96        89\n",
      "           7       0.90      0.84      0.87       113\n",
      "           8       0.91      0.91      0.91        94\n",
      "           9       0.69      0.66      0.68       113\n",
      "          10       0.69      0.61      0.65        92\n",
      "          11       0.85      0.88      0.86       113\n",
      "          12       0.91      0.86      0.88       104\n",
      "          13       0.70      0.76      0.73        92\n",
      "          14       0.86      0.77      0.82       111\n",
      "          15       0.92      0.95      0.94        84\n",
      "          16       0.92      0.89      0.90       106\n",
      "          17       0.89      0.82      0.85        99\n",
      "          18       0.84      0.86      0.85       114\n",
      "          19       0.81      0.81      0.81        94\n",
      "          20       0.85      0.81      0.83       107\n",
      "          21       0.61      0.63      0.62        98\n",
      "          22       0.96      0.90      0.93       104\n",
      "          23       0.71      0.72      0.71        93\n",
      "          24       0.62      0.55      0.58        95\n",
      "          25       0.95      0.92      0.93       114\n",
      "          26       0.64      0.69      0.66        97\n",
      "          27       0.75      0.75      0.75       104\n",
      "          28       0.83      0.78      0.81        88\n",
      "          29       0.86      0.93      0.89        94\n",
      "          30       0.89      0.90      0.89       107\n",
      "          31       0.69      0.62      0.65       103\n",
      "          32       0.74      0.81      0.77        89\n",
      "          33       0.68      0.81      0.74       110\n",
      "          34       0.77      0.75      0.76        93\n",
      "          35       0.88      0.90      0.89        92\n",
      "          36       0.67      0.74      0.70        77\n",
      "          37       0.72      0.82      0.77        87\n",
      "          38       0.77      0.79      0.78       108\n",
      "          39       0.72      0.86      0.79        94\n",
      "          40       0.77      0.79      0.78       100\n",
      "          41       0.75      0.78      0.77       113\n",
      "          42       0.91      0.85      0.88       100\n",
      "          43       0.83      0.78      0.80        98\n",
      "          44       0.80      0.75      0.77       114\n",
      "          45       0.93      0.89      0.91        92\n",
      "          46       0.87      0.87      0.87        86\n",
      "          47       0.74      0.71      0.73       118\n",
      "          48       0.68      0.70      0.69        90\n",
      "          49       0.80      0.81      0.80       101\n",
      "          50       0.55      0.67      0.61       100\n",
      "          51       0.98      1.00      0.99        50\n",
      "          52       0.95      0.98      0.97        60\n",
      "          53       0.70      0.62      0.66        92\n",
      "\n",
      "    accuracy                           0.80      5296\n",
      "   macro avg       0.80      0.80      0.80      5296\n",
      "weighted avg       0.80      0.80      0.80      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 43,  accuracy score is 1.0\n",
      "at random state 43, confusion matrix is [[ 79   0   0 ...   0   0   0]\n",
      " [  0  63   0 ...   0   0   0]\n",
      " [  0   0 100 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  51   0   0]\n",
      " [  0   0   0 ...   0  72   0]\n",
      " [  0   2   1 ...   0   0  77]]\n",
      "at random state 43, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.76      0.80       104\n",
      "           1       0.72      0.66      0.69        95\n",
      "           2       0.88      0.95      0.91       105\n",
      "           3       0.79      0.67      0.72        90\n",
      "           4       0.78      0.87      0.82       100\n",
      "           5       0.62      0.79      0.69        97\n",
      "           6       0.89      0.98      0.93        82\n",
      "           7       0.78      0.82      0.80       101\n",
      "           8       0.89      0.90      0.90        93\n",
      "           9       0.80      0.73      0.76       114\n",
      "          10       0.66      0.76      0.71        80\n",
      "          11       0.86      0.92      0.89       109\n",
      "          12       0.89      0.87      0.88       100\n",
      "          13       0.68      0.74      0.71        96\n",
      "          14       0.72      0.85      0.78        85\n",
      "          15       0.95      0.91      0.93        78\n",
      "          16       0.84      0.91      0.87       101\n",
      "          17       0.82      0.82      0.82       102\n",
      "          18       0.85      0.84      0.85       101\n",
      "          19       0.78      0.76      0.77       109\n",
      "          20       0.80      0.92      0.85        96\n",
      "          21       0.66      0.69      0.67       118\n",
      "          22       0.93      0.92      0.93        93\n",
      "          23       0.83      0.83      0.83       110\n",
      "          24       0.50      0.58      0.54        97\n",
      "          25       0.93      0.91      0.92        89\n",
      "          26       0.61      0.68      0.64        87\n",
      "          27       0.82      0.80      0.81       105\n",
      "          28       0.80      0.79      0.80        99\n",
      "          29       0.83      0.79      0.81       100\n",
      "          30       0.92      0.72      0.81       109\n",
      "          31       0.71      0.67      0.69       112\n",
      "          32       0.84      0.71      0.77       107\n",
      "          33       0.74      0.74      0.74       107\n",
      "          34       0.83      0.75      0.79       110\n",
      "          35       0.83      0.88      0.85       100\n",
      "          36       0.59      0.63      0.61        97\n",
      "          37       0.71      0.80      0.75        99\n",
      "          38       0.72      0.74      0.73        99\n",
      "          39       0.87      0.83      0.85       109\n",
      "          40       0.72      0.77      0.74        98\n",
      "          41       0.78      0.73      0.75       109\n",
      "          42       0.91      0.89      0.90        93\n",
      "          43       0.69      0.68      0.69        90\n",
      "          44       0.90      0.80      0.84       108\n",
      "          45       0.95      0.95      0.95        81\n",
      "          46       0.95      0.89      0.92        88\n",
      "          47       0.75      0.60      0.67        90\n",
      "          48       0.68      0.55      0.61        94\n",
      "          49       0.80      0.66      0.73       110\n",
      "          50       0.47      0.53      0.50       106\n",
      "          51       0.96      1.00      0.98        51\n",
      "          52       0.89      0.92      0.91        78\n",
      "          53       0.67      0.67      0.67       115\n",
      "\n",
      "    accuracy                           0.78      5296\n",
      "   macro avg       0.79      0.79      0.79      5296\n",
      "weighted avg       0.79      0.78      0.78      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 44,  accuracy score is 1.0\n",
      "at random state 44, confusion matrix is [[82  0  0 ...  0  0  0]\n",
      " [ 0 65  0 ...  0  0  4]\n",
      " [ 0  0 93 ...  0  0  3]\n",
      " ...\n",
      " [ 0  0  0 ... 58  0  0]\n",
      " [ 0  0  0 ...  0 53  0]\n",
      " [ 0  1  1 ...  0  0 59]]\n",
      "at random state 44, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.78      0.82       105\n",
      "           1       0.69      0.70      0.70        93\n",
      "           2       0.97      0.92      0.94       101\n",
      "           3       0.78      0.86      0.82        96\n",
      "           4       0.90      0.92      0.91        99\n",
      "           5       0.76      0.77      0.76       108\n",
      "           6       0.95      0.95      0.95        86\n",
      "           7       0.79      0.87      0.83       106\n",
      "           8       0.96      0.94      0.95       110\n",
      "           9       0.89      0.78      0.83       110\n",
      "          10       0.63      0.67      0.65       101\n",
      "          11       0.80      0.87      0.84        87\n",
      "          12       0.88      0.80      0.84       117\n",
      "          13       0.65      0.69      0.67        94\n",
      "          14       0.80      0.75      0.77        95\n",
      "          15       0.89      0.96      0.92        76\n",
      "          16       0.89      0.91      0.90       100\n",
      "          17       0.81      0.68      0.74        99\n",
      "          18       0.85      0.86      0.86       101\n",
      "          19       0.73      0.80      0.76        94\n",
      "          20       0.88      0.84      0.86       105\n",
      "          21       0.62      0.59      0.60       115\n",
      "          22       0.97      0.92      0.94        97\n",
      "          23       0.86      0.68      0.76        97\n",
      "          24       0.67      0.64      0.65       113\n",
      "          25       0.95      0.94      0.95       102\n",
      "          26       0.60      0.62      0.61       110\n",
      "          27       0.81      0.82      0.81       107\n",
      "          28       0.83      0.82      0.83       114\n",
      "          29       0.88      0.85      0.86        92\n",
      "          30       0.89      0.89      0.89        99\n",
      "          31       0.62      0.63      0.63       109\n",
      "          32       0.73      0.78      0.76       111\n",
      "          33       0.75      0.76      0.76       102\n",
      "          34       0.85      0.87      0.86       104\n",
      "          35       0.87      0.92      0.89        83\n",
      "          36       0.63      0.68      0.65        90\n",
      "          37       0.70      0.78      0.73        80\n",
      "          38       0.83      0.85      0.84        95\n",
      "          39       0.88      0.88      0.88        90\n",
      "          40       0.80      0.84      0.82        93\n",
      "          41       0.72      0.73      0.72        97\n",
      "          42       0.84      0.84      0.84        85\n",
      "          43       0.78      0.72      0.75       123\n",
      "          44       0.84      0.78      0.81       100\n",
      "          45       0.94      0.97      0.96        88\n",
      "          46       0.88      0.93      0.90        95\n",
      "          47       0.77      0.71      0.74       100\n",
      "          48       0.62      0.64      0.63        92\n",
      "          49       0.76      0.75      0.75       110\n",
      "          50       0.46      0.53      0.49       106\n",
      "          51       0.98      1.00      0.99        58\n",
      "          52       0.95      0.88      0.91        60\n",
      "          53       0.61      0.61      0.61        96\n",
      "\n",
      "    accuracy                           0.79      5296\n",
      "   macro avg       0.80      0.80      0.80      5296\n",
      "weighted avg       0.80      0.79      0.79      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 45,  accuracy score is 1.0\n",
      "at random state 45, confusion matrix is [[81  0  0 ...  0  0  0]\n",
      " [ 0 76  0 ...  0  0  1]\n",
      " [ 0  0 81 ...  0  0  4]\n",
      " ...\n",
      " [ 0  0  0 ... 58  0  0]\n",
      " [ 0  0  0 ...  1 74  0]\n",
      " [ 0  0  0 ...  0  0 70]]\n",
      "at random state 45, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.76      0.81       107\n",
      "           1       0.73      0.77      0.75        99\n",
      "           2       0.90      0.84      0.87        97\n",
      "           3       0.84      0.75      0.79       106\n",
      "           4       0.75      0.90      0.82        84\n",
      "           5       0.64      0.78      0.70       103\n",
      "           6       0.97      0.88      0.92        95\n",
      "           7       0.73      0.79      0.75        84\n",
      "           8       0.86      0.90      0.88        93\n",
      "           9       0.72      0.77      0.75       102\n",
      "          10       0.69      0.72      0.70        85\n",
      "          11       0.82      0.86      0.84       107\n",
      "          12       0.88      0.84      0.86        97\n",
      "          13       0.71      0.66      0.69        98\n",
      "          14       0.76      0.80      0.77        93\n",
      "          15       0.91      0.86      0.88        79\n",
      "          16       0.86      0.83      0.84        94\n",
      "          17       0.83      0.78      0.80       104\n",
      "          18       0.87      0.75      0.81       110\n",
      "          19       0.79      0.73      0.76       111\n",
      "          20       0.83      0.83      0.83       109\n",
      "          21       0.69      0.72      0.71        97\n",
      "          22       0.84      0.89      0.86        90\n",
      "          23       0.76      0.75      0.76        99\n",
      "          24       0.56      0.57      0.57        93\n",
      "          25       0.93      0.87      0.90       101\n",
      "          26       0.68      0.56      0.62       107\n",
      "          27       0.75      0.74      0.74       107\n",
      "          28       0.89      0.79      0.84       104\n",
      "          29       0.87      0.91      0.89       113\n",
      "          30       0.93      0.83      0.88       116\n",
      "          31       0.62      0.66      0.64        98\n",
      "          32       0.80      0.78      0.79       111\n",
      "          33       0.77      0.85      0.81       104\n",
      "          34       0.81      0.81      0.81       107\n",
      "          35       0.83      0.88      0.86        91\n",
      "          36       0.59      0.71      0.64        85\n",
      "          37       0.76      0.76      0.76       104\n",
      "          38       0.78      0.77      0.77       100\n",
      "          39       0.82      0.75      0.78       108\n",
      "          40       0.83      0.87      0.85       101\n",
      "          41       0.71      0.69      0.70       101\n",
      "          42       0.88      0.88      0.88        99\n",
      "          43       0.70      0.74      0.72        93\n",
      "          44       0.78      0.78      0.78        91\n",
      "          45       0.93      0.90      0.92        90\n",
      "          46       0.89      0.92      0.90        87\n",
      "          47       0.69      0.77      0.73        86\n",
      "          48       0.70      0.66      0.68       105\n",
      "          49       0.75      0.69      0.72       112\n",
      "          50       0.53      0.54      0.53       108\n",
      "          51       0.98      1.00      0.99        58\n",
      "          52       0.91      0.97      0.94        76\n",
      "          53       0.63      0.72      0.67        97\n",
      "\n",
      "    accuracy                           0.78      5296\n",
      "   macro avg       0.79      0.79      0.79      5296\n",
      "weighted avg       0.79      0.78      0.78      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 46,  accuracy score is 1.0\n",
      "at random state 46, confusion matrix is [[ 73   0   0 ...   0   0   0]\n",
      " [  0  68   0 ...   0   0   1]\n",
      " [  0   0 101 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  36   0   0]\n",
      " [  0   0   0 ...   0  71   0]\n",
      " [  0   0   4 ...   0   0  75]]\n",
      "at random state 46, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.75      0.76        97\n",
      "           1       0.79      0.72      0.76        94\n",
      "           2       0.88      0.95      0.91       106\n",
      "           3       0.80      0.83      0.82       103\n",
      "           4       0.78      0.87      0.83       111\n",
      "           5       0.79      0.68      0.73       103\n",
      "           6       0.89      0.93      0.91        84\n",
      "           7       0.85      0.80      0.83       102\n",
      "           8       0.94      0.97      0.95        95\n",
      "           9       0.76      0.83      0.79        86\n",
      "          10       0.70      0.71      0.71       108\n",
      "          11       0.88      0.86      0.87        87\n",
      "          12       0.89      0.82      0.85       114\n",
      "          13       0.66      0.77      0.71        94\n",
      "          14       0.83      0.75      0.79       107\n",
      "          15       0.91      0.95      0.93        82\n",
      "          16       0.89      0.83      0.86       120\n",
      "          17       0.81      0.83      0.82        94\n",
      "          18       0.86      0.85      0.85        99\n",
      "          19       0.74      0.79      0.77        87\n",
      "          20       0.77      0.82      0.79        93\n",
      "          21       0.57      0.72      0.64        93\n",
      "          22       0.88      0.92      0.90        92\n",
      "          23       0.77      0.73      0.75       103\n",
      "          24       0.58      0.58      0.58       103\n",
      "          25       0.97      0.92      0.95       113\n",
      "          26       0.71      0.66      0.68       108\n",
      "          27       0.73      0.77      0.75        88\n",
      "          28       0.85      0.88      0.86       105\n",
      "          29       0.92      0.90      0.91        81\n",
      "          30       0.88      0.93      0.90       111\n",
      "          31       0.59      0.59      0.59       103\n",
      "          32       0.73      0.64      0.68       105\n",
      "          33       0.66      0.84      0.74       104\n",
      "          34       0.78      0.78      0.78       101\n",
      "          35       0.87      0.88      0.87       102\n",
      "          36       0.69      0.72      0.70        96\n",
      "          37       0.72      0.76      0.74       100\n",
      "          38       0.83      0.76      0.79       103\n",
      "          39       0.86      0.78      0.82        94\n",
      "          40       0.89      0.85      0.87       101\n",
      "          41       0.78      0.67      0.72        93\n",
      "          42       0.91      0.85      0.88        96\n",
      "          43       0.61      0.74      0.67        93\n",
      "          44       0.84      0.71      0.77       111\n",
      "          45       0.94      0.92      0.93        90\n",
      "          46       0.93      0.88      0.91        93\n",
      "          47       0.80      0.74      0.77        93\n",
      "          48       0.76      0.78      0.77       108\n",
      "          49       0.72      0.76      0.74       115\n",
      "          50       0.49      0.48      0.49       103\n",
      "          51       1.00      1.00      1.00        36\n",
      "          52       0.95      0.92      0.93        77\n",
      "          53       0.71      0.65      0.68       116\n",
      "\n",
      "    accuracy                           0.79      5296\n",
      "   macro avg       0.80      0.80      0.80      5296\n",
      "weighted avg       0.80      0.79      0.79      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 47,  accuracy score is 1.0\n",
      "at random state 47, confusion matrix is [[69  0  0 ...  0  0  0]\n",
      " [ 0 69  0 ...  0  0  0]\n",
      " [ 0  0 79 ...  0  0  2]\n",
      " ...\n",
      " [ 0  0  0 ... 49  0  0]\n",
      " [ 0  0  0 ...  0 60  0]\n",
      " [ 0  3  0 ...  0  0 78]]\n",
      "at random state 47, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.71      0.75        97\n",
      "           1       0.71      0.66      0.69       104\n",
      "           2       0.82      0.88      0.85        90\n",
      "           3       0.74      0.78      0.76        90\n",
      "           4       0.84      0.90      0.87       104\n",
      "           5       0.69      0.69      0.69       125\n",
      "           6       0.99      0.93      0.96        97\n",
      "           7       0.81      0.82      0.81        98\n",
      "           8       0.90      0.92      0.91       106\n",
      "           9       0.75      0.80      0.77       105\n",
      "          10       0.74      0.61      0.67       104\n",
      "          11       0.84      0.88      0.86       111\n",
      "          12       0.89      0.85      0.87        99\n",
      "          13       0.72      0.75      0.74        97\n",
      "          14       0.82      0.82      0.82       114\n",
      "          15       0.97      0.91      0.94        80\n",
      "          16       0.82      0.89      0.85       110\n",
      "          17       0.86      0.81      0.83        95\n",
      "          18       0.82      0.82      0.82       101\n",
      "          19       0.73      0.73      0.73       102\n",
      "          20       0.86      0.80      0.83        96\n",
      "          21       0.58      0.62      0.60        92\n",
      "          22       0.92      0.94      0.93       106\n",
      "          23       0.70      0.74      0.72       100\n",
      "          24       0.75      0.54      0.63       112\n",
      "          25       0.96      0.93      0.94        99\n",
      "          26       0.66      0.63      0.64        97\n",
      "          27       0.74      0.83      0.78        94\n",
      "          28       0.85      0.86      0.86        87\n",
      "          29       0.85      0.96      0.90        96\n",
      "          30       0.90      0.89      0.90       101\n",
      "          31       0.55      0.66      0.60        82\n",
      "          32       0.76      0.66      0.70        99\n",
      "          33       0.80      0.68      0.73       105\n",
      "          34       0.74      0.73      0.73        96\n",
      "          35       0.93      0.84      0.89        96\n",
      "          36       0.74      0.68      0.71       106\n",
      "          37       0.62      0.75      0.68        95\n",
      "          38       0.83      0.79      0.81       107\n",
      "          39       0.84      0.81      0.82       104\n",
      "          40       0.78      0.78      0.78       110\n",
      "          41       0.69      0.76      0.72        93\n",
      "          42       0.78      0.84      0.81        82\n",
      "          43       0.77      0.73      0.75       105\n",
      "          44       0.77      0.76      0.77       118\n",
      "          45       0.93      0.96      0.94        97\n",
      "          46       0.85      0.89      0.87        81\n",
      "          47       0.72      0.71      0.71        92\n",
      "          48       0.65      0.66      0.65       103\n",
      "          49       0.76      0.73      0.75       115\n",
      "          50       0.49      0.58      0.53        84\n",
      "          51       1.00      1.00      1.00        49\n",
      "          52       0.90      0.91      0.90        66\n",
      "          53       0.72      0.76      0.74       102\n",
      "\n",
      "    accuracy                           0.78      5296\n",
      "   macro avg       0.79      0.79      0.79      5296\n",
      "weighted avg       0.79      0.78      0.78      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 48,  accuracy score is 1.0\n",
      "at random state 48, confusion matrix is [[87  0  0 ...  0  0  0]\n",
      " [ 0 78  0 ...  0  0  2]\n",
      " [ 0  0 90 ...  0  0  1]\n",
      " ...\n",
      " [ 0  0  0 ... 55  0  0]\n",
      " [ 0  0  0 ...  0 71  0]\n",
      " [ 0  2  3 ...  0  0 58]]\n",
      "at random state 48, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.79      0.82       110\n",
      "           1       0.75      0.74      0.74       106\n",
      "           2       0.87      0.87      0.87       104\n",
      "           3       0.89      0.82      0.85        99\n",
      "           4       0.80      0.85      0.83        78\n",
      "           5       0.71      0.73      0.72       106\n",
      "           6       0.94      0.90      0.92        88\n",
      "           7       0.77      0.86      0.81        96\n",
      "           8       0.93      0.90      0.91        82\n",
      "           9       0.80      0.84      0.82        96\n",
      "          10       0.75      0.75      0.75       118\n",
      "          11       0.82      0.90      0.86        80\n",
      "          12       0.72      0.79      0.75        87\n",
      "          13       0.76      0.77      0.77       110\n",
      "          14       0.86      0.77      0.82       111\n",
      "          15       0.89      0.96      0.92        70\n",
      "          16       0.84      0.84      0.84       100\n",
      "          17       0.81      0.83      0.82        92\n",
      "          18       0.85      0.75      0.80       101\n",
      "          19       0.78      0.75      0.76       110\n",
      "          20       0.80      0.86      0.83        96\n",
      "          21       0.68      0.74      0.71        95\n",
      "          22       0.90      0.92      0.91       102\n",
      "          23       0.70      0.81      0.75        93\n",
      "          24       0.51      0.54      0.53        96\n",
      "          25       0.91      0.95      0.93        88\n",
      "          26       0.73      0.73      0.73       101\n",
      "          27       0.75      0.77      0.76       111\n",
      "          28       0.85      0.77      0.81       104\n",
      "          29       0.82      0.87      0.85       101\n",
      "          30       0.83      0.88      0.85        98\n",
      "          31       0.69      0.69      0.69       105\n",
      "          32       0.83      0.72      0.77       112\n",
      "          33       0.70      0.75      0.72       110\n",
      "          34       0.80      0.76      0.78       113\n",
      "          35       0.93      0.91      0.92       112\n",
      "          36       0.67      0.70      0.68       100\n",
      "          37       0.79      0.68      0.73       119\n",
      "          38       0.77      0.78      0.78        97\n",
      "          39       0.81      0.83      0.82        98\n",
      "          40       0.87      0.78      0.82       100\n",
      "          41       0.77      0.74      0.75       102\n",
      "          42       0.85      0.79      0.82        80\n",
      "          43       0.77      0.69      0.73       108\n",
      "          44       0.76      0.77      0.76       103\n",
      "          45       0.89      0.95      0.92        80\n",
      "          46       0.91      0.85      0.88        92\n",
      "          47       0.79      0.68      0.73       106\n",
      "          48       0.69      0.74      0.72        97\n",
      "          49       0.79      0.85      0.82       100\n",
      "          50       0.54      0.55      0.55       109\n",
      "          51       0.98      0.98      0.98        56\n",
      "          52       0.87      0.85      0.86        84\n",
      "          53       0.62      0.69      0.66        84\n",
      "\n",
      "    accuracy                           0.79      5296\n",
      "   macro avg       0.80      0.80      0.79      5296\n",
      "weighted avg       0.79      0.79      0.79      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 49,  accuracy score is 1.0\n",
      "at random state 49, confusion matrix is [[89  0  0 ...  0  0  0]\n",
      " [ 0 68  0 ...  0  0  1]\n",
      " [ 0  0 78 ...  0  0  1]\n",
      " ...\n",
      " [ 0  0  0 ... 57  0  0]\n",
      " [ 0  0  0 ...  0 76  0]\n",
      " [ 0  0  2 ...  0  0 70]]\n",
      "at random state 49, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.79      0.75       112\n",
      "           1       0.72      0.72      0.72        94\n",
      "           2       0.86      0.90      0.88        87\n",
      "           3       0.86      0.82      0.84        94\n",
      "           4       0.87      0.91      0.89        89\n",
      "           5       0.68      0.68      0.68       111\n",
      "           6       0.94      0.96      0.95        91\n",
      "           7       0.78      0.76      0.77        85\n",
      "           8       0.96      0.88      0.92       119\n",
      "           9       0.82      0.89      0.85        97\n",
      "          10       0.76      0.71      0.73       100\n",
      "          11       0.84      0.86      0.85       114\n",
      "          12       0.80      0.78      0.79       104\n",
      "          13       0.68      0.74      0.71        92\n",
      "          14       0.76      0.83      0.80        82\n",
      "          15       0.95      0.94      0.95        85\n",
      "          16       0.92      0.86      0.89       101\n",
      "          17       0.82      0.83      0.82        98\n",
      "          18       0.83      0.83      0.83        99\n",
      "          19       0.76      0.81      0.78        88\n",
      "          20       0.88      0.82      0.85       103\n",
      "          21       0.64      0.67      0.66       103\n",
      "          22       0.86      0.92      0.89        92\n",
      "          23       0.82      0.73      0.77        97\n",
      "          24       0.53      0.52      0.52       104\n",
      "          25       0.96      0.94      0.95       100\n",
      "          26       0.61      0.69      0.65        84\n",
      "          27       0.84      0.73      0.78       103\n",
      "          28       0.82      0.78      0.80       102\n",
      "          29       0.89      0.91      0.90       106\n",
      "          30       0.86      0.82      0.84       103\n",
      "          31       0.57      0.72      0.64        98\n",
      "          32       0.75      0.78      0.77        90\n",
      "          33       0.75      0.73      0.74       113\n",
      "          34       0.78      0.78      0.78       106\n",
      "          35       0.86      0.83      0.84       103\n",
      "          36       0.66      0.66      0.66       108\n",
      "          37       0.73      0.74      0.74       108\n",
      "          38       0.81      0.82      0.82       106\n",
      "          39       0.92      0.84      0.88       103\n",
      "          40       0.83      0.77      0.80       106\n",
      "          41       0.85      0.69      0.76       110\n",
      "          42       0.90      0.93      0.92        87\n",
      "          43       0.66      0.76      0.71        80\n",
      "          44       0.66      0.76      0.71        84\n",
      "          45       0.96      0.96      0.96        89\n",
      "          46       0.90      0.91      0.91        90\n",
      "          47       0.78      0.75      0.76        95\n",
      "          48       0.70      0.70      0.70       104\n",
      "          49       0.83      0.77      0.80       110\n",
      "          50       0.55      0.54      0.54       123\n",
      "          51       0.98      1.00      0.99        57\n",
      "          52       0.94      0.95      0.94        80\n",
      "          53       0.69      0.65      0.67       107\n",
      "\n",
      "    accuracy                           0.79      5296\n",
      "   macro avg       0.80      0.80      0.80      5296\n",
      "weighted avg       0.80      0.79      0.79      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 50,  accuracy score is 1.0\n",
      "at random state 50, confusion matrix is [[70  0  0 ...  0  0  0]\n",
      " [ 0 71  0 ...  0  0  1]\n",
      " [ 0  0 84 ...  0  0  1]\n",
      " ...\n",
      " [ 0  0  0 ... 60  1  0]\n",
      " [ 0  0  0 ...  0 64  0]\n",
      " [ 0  1  0 ...  0  0 59]]\n",
      "at random state 50, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.68      0.75       103\n",
      "           1       0.74      0.70      0.72       102\n",
      "           2       0.90      0.90      0.90        93\n",
      "           3       0.74      0.71      0.72        95\n",
      "           4       0.90      0.90      0.90       106\n",
      "           5       0.65      0.79      0.71        90\n",
      "           6       0.96      0.93      0.95        92\n",
      "           7       0.82      0.78      0.80       102\n",
      "           8       0.92      0.93      0.93       105\n",
      "           9       0.75      0.76      0.76        96\n",
      "          10       0.73      0.72      0.72       103\n",
      "          11       0.91      0.91      0.91       115\n",
      "          12       0.80      0.80      0.80       106\n",
      "          13       0.76      0.78      0.77       110\n",
      "          14       0.79      0.83      0.81        95\n",
      "          15       0.92      0.94      0.93        81\n",
      "          16       0.85      0.85      0.85       109\n",
      "          17       0.79      0.78      0.78        99\n",
      "          18       0.85      0.91      0.88        91\n",
      "          19       0.77      0.73      0.75        98\n",
      "          20       0.83      0.90      0.86        87\n",
      "          21       0.69      0.69      0.69       108\n",
      "          22       0.91      0.92      0.91        97\n",
      "          23       0.71      0.74      0.73        97\n",
      "          24       0.65      0.59      0.62       115\n",
      "          25       0.93      0.96      0.94       105\n",
      "          26       0.61      0.63      0.62        91\n",
      "          27       0.74      0.84      0.79        96\n",
      "          28       0.91      0.79      0.84       100\n",
      "          29       0.85      0.88      0.87       104\n",
      "          30       0.87      0.85      0.86       106\n",
      "          31       0.63      0.62      0.63       103\n",
      "          32       0.93      0.80      0.86       107\n",
      "          33       0.80      0.74      0.77        98\n",
      "          34       0.81      0.78      0.79       101\n",
      "          35       0.89      0.91      0.90        99\n",
      "          36       0.65      0.67      0.66        99\n",
      "          37       0.71      0.83      0.76        99\n",
      "          38       0.75      0.75      0.75       114\n",
      "          39       0.86      0.81      0.83       113\n",
      "          40       0.79      0.82      0.80       100\n",
      "          41       0.75      0.71      0.73       105\n",
      "          42       0.88      0.93      0.91        88\n",
      "          43       0.71      0.64      0.67        97\n",
      "          44       0.87      0.78      0.82       103\n",
      "          45       0.90      0.89      0.90        92\n",
      "          46       0.88      0.93      0.90        83\n",
      "          47       0.76      0.81      0.78        90\n",
      "          48       0.75      0.72      0.73       107\n",
      "          49       0.76      0.77      0.77        97\n",
      "          50       0.47      0.56      0.51        86\n",
      "          51       0.97      0.98      0.98        61\n",
      "          52       0.91      0.91      0.91        70\n",
      "          53       0.67      0.68      0.67        87\n",
      "\n",
      "    accuracy                           0.80      5296\n",
      "   macro avg       0.80      0.80      0.80      5296\n",
      "weighted avg       0.80      0.80      0.80      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 51,  accuracy score is 1.0\n",
      "at random state 51, confusion matrix is [[89  0  0 ...  0  0  0]\n",
      " [ 0 74  0 ...  0  0  2]\n",
      " [ 0  0 82 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 40  0  0]\n",
      " [ 0  0  0 ...  0 71  0]\n",
      " [ 0  1  1 ...  0  0 71]]\n",
      "at random state 51, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.83      0.81       107\n",
      "           1       0.74      0.66      0.70       112\n",
      "           2       0.90      0.85      0.87        97\n",
      "           3       0.75      0.76      0.75        95\n",
      "           4       0.89      0.89      0.89       114\n",
      "           5       0.80      0.77      0.78       116\n",
      "           6       0.91      0.99      0.95        76\n",
      "           7       0.75      0.84      0.79        94\n",
      "           8       0.94      0.96      0.95       102\n",
      "           9       0.75      0.73      0.74       100\n",
      "          10       0.68      0.68      0.68       114\n",
      "          11       0.84      0.89      0.86        96\n",
      "          12       0.83      0.85      0.84       119\n",
      "          13       0.65      0.75      0.70        97\n",
      "          14       0.81      0.71      0.75       102\n",
      "          15       0.94      0.98      0.96        81\n",
      "          16       0.87      0.85      0.86        99\n",
      "          17       0.75      0.81      0.78        99\n",
      "          18       0.85      0.83      0.84       103\n",
      "          19       0.76      0.71      0.73        92\n",
      "          20       0.84      0.81      0.82       103\n",
      "          21       0.75      0.69      0.72       105\n",
      "          22       0.92      0.94      0.93        96\n",
      "          23       0.74      0.74      0.74        94\n",
      "          24       0.49      0.57      0.53        91\n",
      "          25       0.94      0.93      0.94        87\n",
      "          26       0.61      0.57      0.59        94\n",
      "          27       0.79      0.74      0.76       102\n",
      "          28       0.83      0.78      0.80        90\n",
      "          29       0.88      0.88      0.88        97\n",
      "          30       0.83      0.83      0.83        95\n",
      "          31       0.55      0.73      0.63        83\n",
      "          32       0.75      0.78      0.76       101\n",
      "          33       0.78      0.79      0.79       102\n",
      "          34       0.79      0.84      0.81        97\n",
      "          35       0.84      0.85      0.84       102\n",
      "          36       0.74      0.64      0.69        86\n",
      "          37       0.76      0.67      0.71       106\n",
      "          38       0.72      0.73      0.72       106\n",
      "          39       0.81      0.85      0.83       102\n",
      "          40       0.79      0.83      0.81       109\n",
      "          41       0.65      0.70      0.67        96\n",
      "          42       0.89      0.90      0.89       106\n",
      "          43       0.76      0.66      0.71       110\n",
      "          44       0.86      0.78      0.82       111\n",
      "          45       0.90      0.94      0.92        87\n",
      "          46       0.90      0.90      0.90        98\n",
      "          47       0.72      0.76      0.74        95\n",
      "          48       0.72      0.72      0.72        99\n",
      "          49       0.78      0.77      0.77        91\n",
      "          50       0.57      0.55      0.56       108\n",
      "          51       1.00      0.98      0.99        41\n",
      "          52       0.92      0.89      0.90        80\n",
      "          53       0.67      0.64      0.65       111\n",
      "\n",
      "    accuracy                           0.79      5296\n",
      "   macro avg       0.79      0.79      0.79      5296\n",
      "weighted avg       0.79      0.79      0.79      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 52,  accuracy score is 1.0\n",
      "at random state 52, confusion matrix is [[75  0  0 ...  0  0  0]\n",
      " [ 0 75  0 ...  0  0  1]\n",
      " [ 0  0 90 ...  0  0  3]\n",
      " ...\n",
      " [ 0  0  0 ... 57  0  0]\n",
      " [ 0  0  0 ...  0 73  0]\n",
      " [ 0  0  2 ...  0  0 74]]\n",
      "at random state 52, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.74      0.77       102\n",
      "           1       0.71      0.74      0.72       102\n",
      "           2       0.92      0.82      0.87       110\n",
      "           3       0.71      0.78      0.75        83\n",
      "           4       0.81      0.90      0.85       101\n",
      "           5       0.74      0.75      0.74       106\n",
      "           6       0.93      0.97      0.95        91\n",
      "           7       0.83      0.81      0.82        93\n",
      "           8       0.96      0.93      0.94        94\n",
      "           9       0.81      0.80      0.81       104\n",
      "          10       0.68      0.71      0.70        94\n",
      "          11       0.76      0.79      0.77       103\n",
      "          12       0.87      0.83      0.85       105\n",
      "          13       0.77      0.77      0.77        88\n",
      "          14       0.78      0.73      0.76       100\n",
      "          15       0.96      0.96      0.96        69\n",
      "          16       0.82      0.86      0.84        97\n",
      "          17       0.82      0.75      0.78        95\n",
      "          18       0.85      0.81      0.83       111\n",
      "          19       0.77      0.79      0.78       114\n",
      "          20       0.85      0.83      0.84       102\n",
      "          21       0.56      0.67      0.61        90\n",
      "          22       0.90      0.96      0.93       102\n",
      "          23       0.71      0.63      0.67        94\n",
      "          24       0.64      0.54      0.59        96\n",
      "          25       0.97      0.93      0.95       105\n",
      "          26       0.55      0.66      0.60        93\n",
      "          27       0.77      0.82      0.79       103\n",
      "          28       0.89      0.84      0.86       106\n",
      "          29       0.91      0.87      0.89        93\n",
      "          30       0.88      0.87      0.88        93\n",
      "          31       0.64      0.68      0.66       105\n",
      "          32       0.70      0.76      0.73        98\n",
      "          33       0.79      0.71      0.75       107\n",
      "          34       0.75      0.79      0.77       115\n",
      "          35       0.86      0.94      0.89       108\n",
      "          36       0.72      0.68      0.70       111\n",
      "          37       0.82      0.72      0.77       110\n",
      "          38       0.72      0.83      0.77        95\n",
      "          39       0.78      0.80      0.79        85\n",
      "          40       0.71      0.82      0.76        92\n",
      "          41       0.79      0.70      0.74       112\n",
      "          42       0.87      0.93      0.90        82\n",
      "          43       0.73      0.73      0.73       104\n",
      "          44       0.81      0.81      0.81       101\n",
      "          45       0.90      0.94      0.92        78\n",
      "          46       0.91      0.88      0.90        92\n",
      "          47       0.74      0.68      0.71       108\n",
      "          48       0.73      0.66      0.69       117\n",
      "          49       0.79      0.77      0.78       105\n",
      "          50       0.48      0.47      0.48        93\n",
      "          51       0.97      1.00      0.98        57\n",
      "          52       0.95      0.90      0.92        81\n",
      "          53       0.65      0.73      0.69       101\n",
      "\n",
      "    accuracy                           0.79      5296\n",
      "   macro avg       0.79      0.79      0.79      5296\n",
      "weighted avg       0.79      0.79      0.79      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 53,  accuracy score is 1.0\n",
      "at random state 53, confusion matrix is [[70  0  0 ...  0  0  0]\n",
      " [ 0 76  0 ...  0  0  1]\n",
      " [ 0  0 93 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 51  0  0]\n",
      " [ 0  0  0 ...  0 71  0]\n",
      " [ 0  1  1 ...  0  0 71]]\n",
      "at random state 53, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.69      0.74       101\n",
      "           1       0.82      0.69      0.75       110\n",
      "           2       0.92      0.87      0.89       107\n",
      "           3       0.80      0.88      0.84        89\n",
      "           4       0.83      0.92      0.87       109\n",
      "           5       0.73      0.65      0.69       110\n",
      "           6       0.95      0.94      0.94        95\n",
      "           7       0.79      0.87      0.82        89\n",
      "           8       0.93      0.91      0.92        93\n",
      "           9       0.83      0.83      0.83       103\n",
      "          10       0.75      0.69      0.72       110\n",
      "          11       0.85      0.83      0.84        88\n",
      "          12       0.86      0.79      0.82       107\n",
      "          13       0.67      0.76      0.71        95\n",
      "          14       0.82      0.85      0.84        94\n",
      "          15       0.91      0.93      0.92        84\n",
      "          16       0.89      0.93      0.91       112\n",
      "          17       0.84      0.81      0.83        91\n",
      "          18       0.82      0.87      0.84       100\n",
      "          19       0.79      0.84      0.81        92\n",
      "          20       0.82      0.88      0.85       112\n",
      "          21       0.55      0.75      0.64        89\n",
      "          22       0.87      0.92      0.89       107\n",
      "          23       0.74      0.77      0.76        97\n",
      "          24       0.59      0.69      0.63        89\n",
      "          25       0.91      0.87      0.89       106\n",
      "          26       0.62      0.68      0.65        88\n",
      "          27       0.77      0.77      0.77        98\n",
      "          28       0.85      0.86      0.85        92\n",
      "          29       0.94      0.88      0.91        86\n",
      "          30       0.90      0.88      0.89        90\n",
      "          31       0.62      0.65      0.63       101\n",
      "          32       0.76      0.74      0.75        96\n",
      "          33       0.71      0.75      0.73       102\n",
      "          34       0.79      0.76      0.78       102\n",
      "          35       0.84      0.93      0.88        99\n",
      "          36       0.66      0.64      0.65        94\n",
      "          37       0.84      0.77      0.80       114\n",
      "          38       0.74      0.74      0.74       106\n",
      "          39       0.81      0.77      0.79       107\n",
      "          40       0.80      0.84      0.82        98\n",
      "          41       0.69      0.76      0.72        99\n",
      "          42       0.87      0.91      0.89        99\n",
      "          43       0.75      0.70      0.73       104\n",
      "          44       0.80      0.75      0.78       113\n",
      "          45       0.96      0.84      0.89        80\n",
      "          46       0.96      0.93      0.94       100\n",
      "          47       0.76      0.66      0.70        90\n",
      "          48       0.69      0.69      0.69       101\n",
      "          49       0.78      0.74      0.76        99\n",
      "          50       0.59      0.50      0.54       114\n",
      "          51       1.00      1.00      1.00        51\n",
      "          52       0.89      0.87      0.88        82\n",
      "          53       0.67      0.63      0.65       112\n",
      "\n",
      "    accuracy                           0.79      5296\n",
      "   macro avg       0.80      0.80      0.80      5296\n",
      "weighted avg       0.80      0.79      0.79      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 54,  accuracy score is 1.0\n",
      "at random state 54, confusion matrix is [[81  0  0 ...  0  0  0]\n",
      " [ 0 62  0 ...  0  1  3]\n",
      " [ 0  0 89 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 52  0  0]\n",
      " [ 0  0  0 ...  0 65  0]\n",
      " [ 0  1  0 ...  0  0 65]]\n",
      "at random state 54, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.83      0.81        98\n",
      "           1       0.70      0.59      0.64       105\n",
      "           2       0.93      0.93      0.93        96\n",
      "           3       0.75      0.78      0.77        92\n",
      "           4       0.88      0.91      0.89       102\n",
      "           5       0.81      0.63      0.71       120\n",
      "           6       0.93      0.91      0.92        81\n",
      "           7       0.83      0.75      0.79        92\n",
      "           8       0.92      0.90      0.91        92\n",
      "           9       0.82      0.85      0.84       110\n",
      "          10       0.66      0.67      0.66        91\n",
      "          11       0.86      0.86      0.86        98\n",
      "          12       0.82      0.88      0.85       101\n",
      "          13       0.69      0.70      0.69       117\n",
      "          14       0.82      0.73      0.78        90\n",
      "          15       0.99      0.93      0.96        73\n",
      "          16       0.84      0.89      0.86       110\n",
      "          17       0.81      0.80      0.80       108\n",
      "          18       0.77      0.84      0.80        98\n",
      "          19       0.75      0.78      0.76        91\n",
      "          20       0.85      0.85      0.85        96\n",
      "          21       0.64      0.58      0.61       113\n",
      "          22       0.93      0.96      0.95        81\n",
      "          23       0.84      0.75      0.79       116\n",
      "          24       0.61      0.62      0.62       105\n",
      "          25       0.89      0.99      0.94        77\n",
      "          26       0.59      0.57      0.58       110\n",
      "          27       0.84      0.77      0.80        96\n",
      "          28       0.91      0.86      0.88       105\n",
      "          29       0.96      0.91      0.94       115\n",
      "          30       0.89      0.82      0.86        90\n",
      "          31       0.58      0.63      0.60        95\n",
      "          32       0.72      0.76      0.74       103\n",
      "          33       0.65      0.72      0.69        97\n",
      "          34       0.77      0.86      0.82        95\n",
      "          35       0.92      0.90      0.91       102\n",
      "          36       0.55      0.57      0.56        97\n",
      "          37       0.78      0.77      0.77       109\n",
      "          38       0.77      0.76      0.77       116\n",
      "          39       0.79      0.82      0.80       100\n",
      "          40       0.80      0.83      0.82       103\n",
      "          41       0.73      0.70      0.72       104\n",
      "          42       0.86      0.86      0.86       101\n",
      "          43       0.74      0.66      0.70        97\n",
      "          44       0.78      0.81      0.79        99\n",
      "          45       0.93      0.93      0.93        86\n",
      "          46       0.86      0.95      0.90        94\n",
      "          47       0.77      0.83      0.80        87\n",
      "          48       0.66      0.73      0.69       104\n",
      "          49       0.72      0.76      0.74       112\n",
      "          50       0.47      0.47      0.47       104\n",
      "          51       1.00      1.00      1.00        52\n",
      "          52       0.97      0.87      0.92        75\n",
      "          53       0.65      0.68      0.67        95\n",
      "\n",
      "    accuracy                           0.78      5296\n",
      "   macro avg       0.79      0.79      0.79      5296\n",
      "weighted avg       0.79      0.78      0.78      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 55,  accuracy score is 1.0\n",
      "at random state 55, confusion matrix is [[82  0  0 ...  0  0  0]\n",
      " [ 0 69  0 ...  0  0  3]\n",
      " [ 0  0 92 ...  0  0  1]\n",
      " ...\n",
      " [ 0  0  0 ... 55  1  0]\n",
      " [ 0  0  0 ...  0 71  0]\n",
      " [ 0  3  1 ...  0  0 67]]\n",
      "at random state 55, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.80      0.80       103\n",
      "           1       0.67      0.68      0.68       101\n",
      "           2       0.84      0.93      0.88        99\n",
      "           3       0.63      0.73      0.68        83\n",
      "           4       0.77      0.86      0.81        91\n",
      "           5       0.74      0.69      0.71       105\n",
      "           6       0.91      0.95      0.93        84\n",
      "           7       0.83      0.80      0.81       114\n",
      "           8       0.96      0.97      0.97       109\n",
      "           9       0.79      0.82      0.80        95\n",
      "          10       0.79      0.78      0.78       104\n",
      "          11       0.92      0.80      0.86       102\n",
      "          12       0.87      0.81      0.84        93\n",
      "          13       0.80      0.72      0.76        97\n",
      "          14       0.70      0.79      0.75        96\n",
      "          15       0.96      0.91      0.94        80\n",
      "          16       0.88      0.78      0.83       100\n",
      "          17       0.82      0.82      0.82       104\n",
      "          18       0.72      0.83      0.77        86\n",
      "          19       0.82      0.80      0.81        86\n",
      "          20       0.87      0.84      0.85        98\n",
      "          21       0.67      0.60      0.63       117\n",
      "          22       0.94      0.91      0.92       108\n",
      "          23       0.68      0.69      0.68        99\n",
      "          24       0.67      0.61      0.64       102\n",
      "          25       0.93      0.96      0.94        98\n",
      "          26       0.63      0.59      0.61       109\n",
      "          27       0.81      0.76      0.78       109\n",
      "          28       0.94      0.78      0.85       104\n",
      "          29       0.85      0.97      0.90        87\n",
      "          30       0.89      0.86      0.88       115\n",
      "          31       0.62      0.64      0.63       106\n",
      "          32       0.78      0.75      0.77       102\n",
      "          33       0.65      0.64      0.64        97\n",
      "          34       0.76      0.73      0.74        96\n",
      "          35       0.89      0.83      0.86       104\n",
      "          36       0.67      0.71      0.69        95\n",
      "          37       0.72      0.77      0.74        96\n",
      "          38       0.82      0.80      0.81       105\n",
      "          39       0.69      0.81      0.74       102\n",
      "          40       0.78      0.75      0.76       101\n",
      "          41       0.74      0.72      0.73       104\n",
      "          42       0.86      0.93      0.90        88\n",
      "          43       0.77      0.72      0.75       105\n",
      "          44       0.82      0.84      0.83       102\n",
      "          45       0.90      0.92      0.91        88\n",
      "          46       0.92      0.92      0.92        98\n",
      "          47       0.74      0.68      0.71       114\n",
      "          48       0.67      0.62      0.65       109\n",
      "          49       0.81      0.83      0.82        87\n",
      "          50       0.47      0.57      0.52        94\n",
      "          51       1.00      0.98      0.99        56\n",
      "          52       0.93      0.90      0.92        79\n",
      "          53       0.60      0.74      0.66        90\n",
      "\n",
      "    accuracy                           0.78      5296\n",
      "   macro avg       0.79      0.79      0.79      5296\n",
      "weighted avg       0.79      0.78      0.79      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 56,  accuracy score is 1.0\n",
      "at random state 56, confusion matrix is [[ 79   0   0 ...   0   0   0]\n",
      " [  0  81   0 ...   0   0   1]\n",
      " [  0   0 102 ...   0   0   2]\n",
      " ...\n",
      " [  0   0   0 ...  52   0   0]\n",
      " [  0   0   0 ...   0  71   0]\n",
      " [  0   0   0 ...   0   0  80]]\n",
      "at random state 56, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.76      0.73       104\n",
      "           1       0.80      0.72      0.76       112\n",
      "           2       0.91      0.88      0.89       116\n",
      "           3       0.89      0.87      0.88       112\n",
      "           4       0.83      0.86      0.85       108\n",
      "           5       0.72      0.70      0.71       109\n",
      "           6       0.92      0.95      0.93        94\n",
      "           7       0.80      0.82      0.81        80\n",
      "           8       0.94      0.96      0.95       106\n",
      "           9       0.78      0.87      0.82        95\n",
      "          10       0.70      0.65      0.67       110\n",
      "          11       0.90      0.86      0.88        97\n",
      "          12       0.91      0.82      0.86        89\n",
      "          13       0.67      0.74      0.71        89\n",
      "          14       0.83      0.75      0.79        93\n",
      "          15       0.95      0.96      0.96        84\n",
      "          16       0.84      0.84      0.84       103\n",
      "          17       0.84      0.78      0.81        90\n",
      "          18       0.74      0.85      0.79        92\n",
      "          19       0.80      0.81      0.81       100\n",
      "          20       0.88      0.86      0.87       107\n",
      "          21       0.71      0.64      0.67       102\n",
      "          22       0.93      0.88      0.90        97\n",
      "          23       0.77      0.73      0.75       104\n",
      "          24       0.56      0.62      0.59        95\n",
      "          25       0.92      0.90      0.91        93\n",
      "          26       0.66      0.69      0.67        97\n",
      "          27       0.77      0.81      0.79        99\n",
      "          28       0.76      0.82      0.79        92\n",
      "          29       0.87      0.86      0.86       100\n",
      "          30       0.83      0.82      0.82        94\n",
      "          31       0.57      0.57      0.57        89\n",
      "          32       0.81      0.85      0.83       110\n",
      "          33       0.72      0.78      0.75       101\n",
      "          34       0.84      0.77      0.80       116\n",
      "          35       0.85      0.87      0.86        98\n",
      "          36       0.69      0.72      0.70       104\n",
      "          37       0.75      0.80      0.78       100\n",
      "          38       0.87      0.83      0.85       112\n",
      "          39       0.81      0.78      0.79        96\n",
      "          40       0.84      0.85      0.85       100\n",
      "          41       0.75      0.62      0.68       122\n",
      "          42       0.89      0.95      0.92        77\n",
      "          43       0.74      0.68      0.71        99\n",
      "          44       0.77      0.82      0.80       108\n",
      "          45       0.88      0.97      0.92        86\n",
      "          46       0.92      0.88      0.90        83\n",
      "          47       0.70      0.75      0.72        95\n",
      "          48       0.62      0.65      0.63       102\n",
      "          49       0.76      0.82      0.79        94\n",
      "          50       0.48      0.47      0.47       103\n",
      "          51       1.00      0.95      0.97        55\n",
      "          52       0.97      0.95      0.96        75\n",
      "          53       0.75      0.74      0.74       108\n",
      "\n",
      "    accuracy                           0.79      5296\n",
      "   macro avg       0.80      0.80      0.80      5296\n",
      "weighted avg       0.80      0.79      0.79      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 57,  accuracy score is 1.0\n",
      "at random state 57, confusion matrix is [[80  0  0 ...  0  0  0]\n",
      " [ 0 78  0 ...  0  0  0]\n",
      " [ 0  0 77 ...  0  0  2]\n",
      " ...\n",
      " [ 0  0  0 ... 41  0  0]\n",
      " [ 0  0  0 ...  0 70  0]\n",
      " [ 0  3  2 ...  0  0 72]]\n",
      "at random state 57, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.80      0.77       100\n",
      "           1       0.70      0.72      0.71       108\n",
      "           2       0.94      0.90      0.92        86\n",
      "           3       0.84      0.84      0.84       122\n",
      "           4       0.78      0.90      0.84        97\n",
      "           5       0.69      0.72      0.70       109\n",
      "           6       0.95      0.89      0.92        90\n",
      "           7       0.75      0.80      0.77        84\n",
      "           8       0.86      0.93      0.90        92\n",
      "           9       0.89      0.78      0.83       103\n",
      "          10       0.71      0.74      0.72        91\n",
      "          11       0.81      0.89      0.85        95\n",
      "          12       0.91      0.85      0.88        94\n",
      "          13       0.78      0.65      0.71       113\n",
      "          14       0.84      0.76      0.80       100\n",
      "          15       0.90      0.97      0.93        79\n",
      "          16       0.86      0.91      0.89        91\n",
      "          17       0.80      0.80      0.80       115\n",
      "          18       0.86      0.75      0.80       103\n",
      "          19       0.80      0.82      0.81       102\n",
      "          20       0.86      0.90      0.88        99\n",
      "          21       0.65      0.58      0.61       103\n",
      "          22       0.93      0.90      0.91        97\n",
      "          23       0.72      0.68      0.70        99\n",
      "          24       0.62      0.62      0.62        96\n",
      "          25       0.93      0.92      0.93       103\n",
      "          26       0.58      0.65      0.61        92\n",
      "          27       0.86      0.74      0.79       102\n",
      "          28       0.85      0.83      0.84        96\n",
      "          29       0.85      0.94      0.89        97\n",
      "          30       0.92      0.78      0.84       103\n",
      "          31       0.62      0.65      0.64       113\n",
      "          32       0.88      0.82      0.85       114\n",
      "          33       0.68      0.77      0.72        97\n",
      "          34       0.73      0.84      0.78       106\n",
      "          35       0.87      0.80      0.83        89\n",
      "          36       0.67      0.61      0.64       125\n",
      "          37       0.71      0.72      0.72        93\n",
      "          38       0.75      0.82      0.78        93\n",
      "          39       0.82      0.85      0.83       110\n",
      "          40       0.76      0.72      0.74        99\n",
      "          41       0.61      0.84      0.71        82\n",
      "          42       0.94      0.88      0.91        89\n",
      "          43       0.66      0.75      0.70        92\n",
      "          44       0.78      0.81      0.79        99\n",
      "          45       0.90      0.88      0.89        89\n",
      "          46       0.90      0.91      0.90        78\n",
      "          47       0.78      0.70      0.74       100\n",
      "          48       0.72      0.75      0.73       101\n",
      "          49       0.77      0.72      0.75       109\n",
      "          50       0.54      0.53      0.54       115\n",
      "          51       1.00      0.98      0.99        42\n",
      "          52       0.84      0.89      0.86        79\n",
      "          53       0.72      0.60      0.65       121\n",
      "\n",
      "    accuracy                           0.78      5296\n",
      "   macro avg       0.79      0.79      0.79      5296\n",
      "weighted avg       0.79      0.78      0.78      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 58,  accuracy score is 1.0\n",
      "at random state 58, confusion matrix is [[94  0  0 ...  0  0  0]\n",
      " [ 0 61  0 ...  0  0  1]\n",
      " [ 0  0 95 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 45  0  0]\n",
      " [ 0  0  0 ...  0 70  1]\n",
      " [ 0  1  0 ...  0  0 66]]\n",
      "at random state 58, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.80      0.81       117\n",
      "           1       0.70      0.62      0.66        98\n",
      "           2       0.89      0.90      0.90       105\n",
      "           3       0.80      0.79      0.80       107\n",
      "           4       0.75      0.83      0.79        89\n",
      "           5       0.73      0.76      0.75       100\n",
      "           6       0.90      0.86      0.88        88\n",
      "           7       0.85      0.84      0.85       101\n",
      "           8       0.95      0.93      0.94       112\n",
      "           9       0.75      0.87      0.80        92\n",
      "          10       0.62      0.70      0.66        86\n",
      "          11       0.84      0.84      0.84       100\n",
      "          12       0.84      0.85      0.84       102\n",
      "          13       0.71      0.70      0.70        90\n",
      "          14       0.73      0.82      0.77       112\n",
      "          15       0.91      0.98      0.94        90\n",
      "          16       0.89      0.86      0.87        93\n",
      "          17       0.86      0.78      0.82       112\n",
      "          18       0.83      0.86      0.84       111\n",
      "          19       0.78      0.75      0.77        93\n",
      "          20       0.84      0.82      0.83       105\n",
      "          21       0.62      0.56      0.59       100\n",
      "          22       0.92      0.91      0.92        92\n",
      "          23       0.77      0.72      0.74       103\n",
      "          24       0.60      0.67      0.63       104\n",
      "          25       0.91      0.89      0.90       108\n",
      "          26       0.70      0.75      0.73       106\n",
      "          27       0.80      0.75      0.78       110\n",
      "          28       0.87      0.85      0.86       102\n",
      "          29       0.88      0.93      0.91        99\n",
      "          30       0.89      0.85      0.87        94\n",
      "          31       0.65      0.69      0.67        95\n",
      "          32       0.72      0.71      0.71       106\n",
      "          33       0.67      0.70      0.68       105\n",
      "          34       0.83      0.75      0.79       115\n",
      "          35       0.88      0.84      0.86       100\n",
      "          36       0.71      0.67      0.69       104\n",
      "          37       0.71      0.75      0.73        87\n",
      "          38       0.78      0.75      0.76        99\n",
      "          39       0.78      0.85      0.81        98\n",
      "          40       0.78      0.77      0.77        99\n",
      "          41       0.74      0.78      0.76        87\n",
      "          42       0.89      0.86      0.88        95\n",
      "          43       0.78      0.74      0.76        92\n",
      "          44       0.88      0.80      0.84       105\n",
      "          45       0.94      0.89      0.91        89\n",
      "          46       0.89      0.92      0.91        92\n",
      "          47       0.78      0.77      0.77        95\n",
      "          48       0.71      0.63      0.67       103\n",
      "          49       0.83      0.76      0.79        98\n",
      "          50       0.45      0.45      0.45        94\n",
      "          51       1.00      1.00      1.00        45\n",
      "          52       0.84      0.86      0.85        81\n",
      "          53       0.57      0.73      0.64        91\n",
      "\n",
      "    accuracy                           0.79      5296\n",
      "   macro avg       0.79      0.79      0.79      5296\n",
      "weighted avg       0.79      0.79      0.79      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 59,  accuracy score is 1.0\n",
      "at random state 59, confusion matrix is [[79  0  0 ...  0  0  0]\n",
      " [ 0 74  0 ...  0  0  5]\n",
      " [ 0  0 99 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 63  0  0]\n",
      " [ 0  0  0 ...  0 79  0]\n",
      " [ 0  3  2 ...  0  0 82]]\n",
      "at random state 59, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.80      0.78        99\n",
      "           1       0.76      0.71      0.73       104\n",
      "           2       0.88      0.93      0.90       107\n",
      "           3       0.72      0.80      0.75        83\n",
      "           4       0.93      0.85      0.89       129\n",
      "           5       0.72      0.70      0.71        97\n",
      "           6       0.93      0.94      0.94        86\n",
      "           7       0.79      0.82      0.81        97\n",
      "           8       0.91      0.94      0.92        95\n",
      "           9       0.80      0.77      0.79       105\n",
      "          10       0.69      0.66      0.68       109\n",
      "          11       0.84      0.86      0.85        99\n",
      "          12       0.80      0.80      0.80        98\n",
      "          13       0.76      0.84      0.80        98\n",
      "          14       0.81      0.80      0.80        90\n",
      "          15       0.95      0.94      0.95        88\n",
      "          16       0.84      0.88      0.86        91\n",
      "          17       0.91      0.79      0.85       106\n",
      "          18       0.81      0.81      0.81        99\n",
      "          19       0.81      0.86      0.83        97\n",
      "          20       0.87      0.83      0.85        96\n",
      "          21       0.66      0.57      0.61       104\n",
      "          22       0.92      0.88      0.90       100\n",
      "          23       0.69      0.75      0.72        97\n",
      "          24       0.65      0.59      0.62        94\n",
      "          25       0.89      0.94      0.92       104\n",
      "          26       0.62      0.67      0.65       106\n",
      "          27       0.78      0.84      0.81        95\n",
      "          28       0.90      0.88      0.89        92\n",
      "          29       0.87      0.87      0.87       102\n",
      "          30       0.86      0.96      0.91       104\n",
      "          31       0.59      0.63      0.61        90\n",
      "          32       0.76      0.80      0.78       101\n",
      "          33       0.69      0.76      0.72        99\n",
      "          34       0.85      0.75      0.80       112\n",
      "          35       0.85      0.82      0.84       113\n",
      "          36       0.71      0.70      0.71        91\n",
      "          37       0.77      0.70      0.73       103\n",
      "          38       0.74      0.76      0.75        84\n",
      "          39       0.86      0.86      0.86       103\n",
      "          40       0.78      0.77      0.77        96\n",
      "          41       0.73      0.77      0.75       108\n",
      "          42       0.93      0.90      0.91        89\n",
      "          43       0.79      0.73      0.76        90\n",
      "          44       0.71      0.82      0.76        96\n",
      "          45       0.94      0.92      0.93        87\n",
      "          46       0.85      0.92      0.88        89\n",
      "          47       0.77      0.70      0.73        88\n",
      "          48       0.75      0.68      0.72       122\n",
      "          49       0.81      0.74      0.77       102\n",
      "          50       0.55      0.60      0.58        93\n",
      "          51       1.00      1.00      1.00        63\n",
      "          52       0.90      0.96      0.93        82\n",
      "          53       0.73      0.66      0.69       124\n",
      "\n",
      "    accuracy                           0.80      5296\n",
      "   macro avg       0.80      0.80      0.80      5296\n",
      "weighted avg       0.80      0.80      0.80      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 60,  accuracy score is 1.0\n",
      "at random state 60, confusion matrix is [[80  0  0 ...  0  0  0]\n",
      " [ 0 76  0 ...  0  0  1]\n",
      " [ 0  0 92 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 41  1  0]\n",
      " [ 0  0  0 ...  0 73  0]\n",
      " [ 0  3  1 ...  0  0 77]]\n",
      "at random state 60, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.79      0.75       101\n",
      "           1       0.66      0.79      0.72        96\n",
      "           2       0.89      0.92      0.91       100\n",
      "           3       0.91      0.77      0.83       104\n",
      "           4       0.84      0.80      0.82       101\n",
      "           5       0.81      0.79      0.80       106\n",
      "           6       0.95      0.87      0.91        86\n",
      "           7       0.85      0.77      0.81       100\n",
      "           8       0.96      0.92      0.94       102\n",
      "           9       0.82      0.80      0.81       104\n",
      "          10       0.69      0.75      0.72       114\n",
      "          11       0.91      0.89      0.90        97\n",
      "          12       0.88      0.85      0.86        97\n",
      "          13       0.76      0.71      0.73       101\n",
      "          14       0.82      0.73      0.78        90\n",
      "          15       0.96      0.95      0.95        91\n",
      "          16       0.81      0.94      0.87       104\n",
      "          17       0.79      0.80      0.80       105\n",
      "          18       0.86      0.81      0.83        89\n",
      "          19       0.82      0.84      0.83       104\n",
      "          20       0.87      0.74      0.80        96\n",
      "          21       0.65      0.64      0.64        94\n",
      "          22       0.90      0.97      0.93       103\n",
      "          23       0.71      0.82      0.76       106\n",
      "          24       0.59      0.61      0.60       104\n",
      "          25       0.95      0.94      0.94        93\n",
      "          26       0.59      0.58      0.58       111\n",
      "          27       0.69      0.72      0.70        88\n",
      "          28       0.75      0.82      0.78        93\n",
      "          29       0.96      0.83      0.89        94\n",
      "          30       0.82      0.79      0.80       109\n",
      "          31       0.69      0.55      0.61       111\n",
      "          32       0.79      0.74      0.77       115\n",
      "          33       0.77      0.76      0.77        97\n",
      "          34       0.75      0.81      0.78       101\n",
      "          35       0.81      0.93      0.87       123\n",
      "          36       0.67      0.66      0.67        95\n",
      "          37       0.71      0.72      0.72        82\n",
      "          38       0.84      0.84      0.84        90\n",
      "          39       0.84      0.86      0.85       101\n",
      "          40       0.83      0.82      0.83       107\n",
      "          41       0.80      0.80      0.80       106\n",
      "          42       0.82      0.85      0.83        84\n",
      "          43       0.74      0.72      0.73       109\n",
      "          44       0.82      0.84      0.83        95\n",
      "          45       0.95      0.96      0.95        94\n",
      "          46       0.89      0.90      0.89        94\n",
      "          47       0.85      0.70      0.77        91\n",
      "          48       0.67      0.67      0.67       104\n",
      "          49       0.81      0.78      0.79        96\n",
      "          50       0.39      0.49      0.43        88\n",
      "          51       0.98      0.98      0.98        42\n",
      "          52       0.92      0.91      0.92        80\n",
      "          53       0.69      0.71      0.70       108\n",
      "\n",
      "    accuracy                           0.79      5296\n",
      "   macro avg       0.80      0.80      0.80      5296\n",
      "weighted avg       0.80      0.79      0.79      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 61,  accuracy score is 1.0\n",
      "at random state 61, confusion matrix is [[81  0  0 ...  0  0  0]\n",
      " [ 0 73  0 ...  0  0  2]\n",
      " [ 0  0 92 ...  0  0  1]\n",
      " ...\n",
      " [ 0  0  0 ... 50  0  0]\n",
      " [ 0  0  0 ...  0 68  0]\n",
      " [ 0  1  5 ...  0  0 59]]\n",
      "at random state 61, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.76      0.77       106\n",
      "           1       0.72      0.65      0.68       113\n",
      "           2       0.78      0.88      0.83       104\n",
      "           3       0.75      0.80      0.77        89\n",
      "           4       0.84      0.86      0.85        98\n",
      "           5       0.74      0.79      0.76       112\n",
      "           6       0.91      0.92      0.91        84\n",
      "           7       0.76      0.78      0.77        96\n",
      "           8       0.93      0.97      0.95       102\n",
      "           9       0.81      0.81      0.81       113\n",
      "          10       0.73      0.60      0.66       118\n",
      "          11       0.86      0.89      0.87        96\n",
      "          12       0.83      0.89      0.86       108\n",
      "          13       0.61      0.65      0.63        81\n",
      "          14       0.91      0.80      0.85       106\n",
      "          15       0.90      0.92      0.91        78\n",
      "          16       0.86      0.84      0.85       111\n",
      "          17       0.86      0.85      0.86       105\n",
      "          18       0.86      0.87      0.87       110\n",
      "          19       0.89      0.81      0.85       108\n",
      "          20       0.85      0.78      0.81        98\n",
      "          21       0.64      0.63      0.64        93\n",
      "          22       0.91      0.89      0.90        93\n",
      "          23       0.80      0.76      0.78        94\n",
      "          24       0.66      0.73      0.69        93\n",
      "          25       0.89      0.94      0.92       104\n",
      "          26       0.63      0.68      0.65       103\n",
      "          27       0.81      0.79      0.80       104\n",
      "          28       0.82      0.77      0.80       101\n",
      "          29       0.77      0.87      0.82        86\n",
      "          30       0.86      0.88      0.87       104\n",
      "          31       0.75      0.75      0.75       110\n",
      "          32       0.80      0.79      0.80       102\n",
      "          33       0.73      0.73      0.73        95\n",
      "          34       0.72      0.78      0.75        99\n",
      "          35       0.87      0.87      0.87        98\n",
      "          36       0.59      0.69      0.64        93\n",
      "          37       0.75      0.77      0.76        99\n",
      "          38       0.74      0.75      0.75       104\n",
      "          39       0.77      0.83      0.80        99\n",
      "          40       0.84      0.79      0.82       106\n",
      "          41       0.73      0.73      0.73       106\n",
      "          42       0.95      0.85      0.89        85\n",
      "          43       0.72      0.65      0.68        98\n",
      "          44       0.79      0.81      0.80        95\n",
      "          45       0.94      0.93      0.94        89\n",
      "          46       0.95      0.85      0.90        94\n",
      "          47       0.76      0.69      0.72        94\n",
      "          48       0.74      0.77      0.75        90\n",
      "          49       0.81      0.75      0.78       110\n",
      "          50       0.55      0.65      0.59        89\n",
      "          51       1.00      0.98      0.99        51\n",
      "          52       0.88      0.86      0.87        79\n",
      "          53       0.65      0.59      0.62       100\n",
      "\n",
      "    accuracy                           0.79      5296\n",
      "   macro avg       0.80      0.79      0.79      5296\n",
      "weighted avg       0.79      0.79      0.79      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 62,  accuracy score is 1.0\n",
      "at random state 62, confusion matrix is [[82  0  0 ...  0  0  0]\n",
      " [ 0 73  0 ...  0  0  2]\n",
      " [ 0  0 88 ...  0  0  5]\n",
      " ...\n",
      " [ 0  0  0 ... 43  0  0]\n",
      " [ 0  0  0 ...  0 69  0]\n",
      " [ 0  1  2 ...  0  0 64]]\n",
      "at random state 62, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.72      0.75       114\n",
      "           1       0.72      0.72      0.72       101\n",
      "           2       0.84      0.88      0.86       100\n",
      "           3       0.79      0.76      0.78       101\n",
      "           4       0.83      0.85      0.84        84\n",
      "           5       0.66      0.76      0.71        91\n",
      "           6       0.98      0.95      0.97        88\n",
      "           7       0.82      0.79      0.81       101\n",
      "           8       0.95      0.90      0.92       115\n",
      "           9       0.80      0.86      0.83       104\n",
      "          10       0.76      0.73      0.74        96\n",
      "          11       0.79      0.88      0.83       104\n",
      "          12       0.76      0.84      0.80        91\n",
      "          13       0.65      0.70      0.68        91\n",
      "          14       0.82      0.82      0.82       104\n",
      "          15       0.94      0.94      0.94        86\n",
      "          16       0.91      0.89      0.90       108\n",
      "          17       0.83      0.88      0.85        72\n",
      "          18       0.87      0.81      0.84        96\n",
      "          19       0.86      0.74      0.80       105\n",
      "          20       0.89      0.88      0.88        98\n",
      "          21       0.63      0.68      0.65        99\n",
      "          22       0.90      0.89      0.90       104\n",
      "          23       0.71      0.83      0.76       109\n",
      "          24       0.59      0.61      0.60        94\n",
      "          25       0.89      0.86      0.87       105\n",
      "          26       0.73      0.70      0.72       105\n",
      "          27       0.81      0.79      0.80       100\n",
      "          28       0.88      0.83      0.86       118\n",
      "          29       0.87      0.92      0.89        98\n",
      "          30       0.90      0.94      0.92       106\n",
      "          31       0.59      0.75      0.66        88\n",
      "          32       0.76      0.72      0.74       109\n",
      "          33       0.82      0.74      0.78        96\n",
      "          34       0.84      0.80      0.82       101\n",
      "          35       0.91      0.85      0.88       105\n",
      "          36       0.75      0.68      0.71       103\n",
      "          37       0.67      0.78      0.72        83\n",
      "          38       0.82      0.82      0.82       103\n",
      "          39       0.86      0.86      0.86       115\n",
      "          40       0.84      0.81      0.83       105\n",
      "          41       0.77      0.73      0.75       111\n",
      "          42       0.89      0.86      0.88        99\n",
      "          43       0.75      0.79      0.77        91\n",
      "          44       0.80      0.74      0.77        99\n",
      "          45       0.91      0.97      0.94        91\n",
      "          46       0.96      0.91      0.94        89\n",
      "          47       0.81      0.69      0.75       101\n",
      "          48       0.74      0.68      0.71       104\n",
      "          49       0.83      0.76      0.79       106\n",
      "          50       0.55      0.53      0.54        96\n",
      "          51       0.98      1.00      0.99        43\n",
      "          52       0.86      0.92      0.89        75\n",
      "          53       0.56      0.67      0.61        95\n",
      "\n",
      "    accuracy                           0.80      5296\n",
      "   macro avg       0.80      0.80      0.80      5296\n",
      "weighted avg       0.80      0.80      0.80      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 63,  accuracy score is 1.0\n",
      "at random state 63, confusion matrix is [[75  0  0 ...  0  0  0]\n",
      " [ 0 80  0 ...  0  0  1]\n",
      " [ 0  0 99 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 42  0  0]\n",
      " [ 0  0  0 ...  0 68  1]\n",
      " [ 0  1  1 ...  0  0 64]]\n",
      "at random state 63, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.80      0.77        94\n",
      "           1       0.75      0.76      0.76       105\n",
      "           2       0.93      0.89      0.91       111\n",
      "           3       0.84      0.79      0.82       101\n",
      "           4       0.88      0.91      0.89        95\n",
      "           5       0.71      0.71      0.71        97\n",
      "           6       0.94      0.96      0.95        83\n",
      "           7       0.88      0.78      0.83       126\n",
      "           8       0.93      0.90      0.92       102\n",
      "           9       0.85      0.81      0.83       109\n",
      "          10       0.70      0.77      0.74       101\n",
      "          11       0.82      0.90      0.86       106\n",
      "          12       0.86      0.85      0.86        89\n",
      "          13       0.73      0.76      0.75       114\n",
      "          14       0.82      0.82      0.82       100\n",
      "          15       0.94      0.95      0.95        66\n",
      "          16       0.81      0.90      0.85       107\n",
      "          17       0.79      0.84      0.82        90\n",
      "          18       0.86      0.82      0.84       115\n",
      "          19       0.81      0.78      0.79       104\n",
      "          20       0.84      0.80      0.82       101\n",
      "          21       0.71      0.68      0.70       101\n",
      "          22       0.85      0.85      0.85        93\n",
      "          23       0.81      0.78      0.80        93\n",
      "          24       0.58      0.56      0.57       104\n",
      "          25       0.90      0.93      0.91        85\n",
      "          26       0.68      0.63      0.65       103\n",
      "          27       0.75      0.81      0.78       103\n",
      "          28       0.80      0.81      0.81       110\n",
      "          29       0.89      0.93      0.91       100\n",
      "          30       0.87      0.81      0.84        95\n",
      "          31       0.58      0.62      0.60        98\n",
      "          32       0.78      0.74      0.76       103\n",
      "          33       0.74      0.74      0.74       107\n",
      "          34       0.86      0.75      0.80        95\n",
      "          35       0.84      0.80      0.82       107\n",
      "          36       0.66      0.66      0.66        95\n",
      "          37       0.68      0.83      0.75        92\n",
      "          38       0.77      0.79      0.78        95\n",
      "          39       0.77      0.77      0.77       104\n",
      "          40       0.81      0.86      0.83        96\n",
      "          41       0.72      0.76      0.74        90\n",
      "          42       0.87      0.92      0.89        97\n",
      "          43       0.80      0.75      0.78       113\n",
      "          44       0.74      0.85      0.79        89\n",
      "          45       0.94      0.92      0.93        72\n",
      "          46       0.91      0.85      0.88        96\n",
      "          47       0.75      0.79      0.77       112\n",
      "          48       0.78      0.71      0.74       103\n",
      "          49       0.87      0.79      0.82       107\n",
      "          50       0.51      0.51      0.51       110\n",
      "          51       1.00      1.00      1.00        42\n",
      "          52       0.94      0.96      0.95        71\n",
      "          53       0.71      0.65      0.68        99\n",
      "\n",
      "    accuracy                           0.80      5296\n",
      "   macro avg       0.80      0.80      0.80      5296\n",
      "weighted avg       0.80      0.80      0.80      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 64,  accuracy score is 1.0\n",
      "at random state 64, confusion matrix is [[79  0  0 ...  0  0  0]\n",
      " [ 0 75  0 ...  0  0  2]\n",
      " [ 0  0 98 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 41  0  0]\n",
      " [ 0  0  0 ...  0 67  0]\n",
      " [ 0  2  3 ...  0  0 59]]\n",
      "at random state 64, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.80      0.80        99\n",
      "           1       0.71      0.69      0.70       109\n",
      "           2       0.91      0.89      0.90       110\n",
      "           3       0.79      0.81      0.80        97\n",
      "           4       0.74      0.88      0.80        86\n",
      "           5       0.73      0.76      0.74        91\n",
      "           6       0.92      0.88      0.90        88\n",
      "           7       0.81      0.84      0.83       115\n",
      "           8       0.97      0.84      0.90       108\n",
      "           9       0.73      0.82      0.77        79\n",
      "          10       0.77      0.70      0.73       113\n",
      "          11       0.83      0.90      0.86        96\n",
      "          12       0.82      0.90      0.86        96\n",
      "          13       0.69      0.74      0.71       104\n",
      "          14       0.80      0.84      0.82       111\n",
      "          15       0.94      0.94      0.94        95\n",
      "          16       0.87      0.91      0.89       106\n",
      "          17       0.76      0.76      0.76        89\n",
      "          18       0.83      0.83      0.83        99\n",
      "          19       0.85      0.72      0.78       106\n",
      "          20       0.89      0.83      0.86        82\n",
      "          21       0.64      0.63      0.64       100\n",
      "          22       0.90      0.90      0.90       111\n",
      "          23       0.69      0.69      0.69        93\n",
      "          24       0.59      0.60      0.60        88\n",
      "          25       0.94      0.96      0.95       107\n",
      "          26       0.63      0.75      0.68       102\n",
      "          27       0.83      0.78      0.80        97\n",
      "          28       0.73      0.70      0.71        96\n",
      "          29       0.86      0.84      0.85        89\n",
      "          30       0.88      0.88      0.88       111\n",
      "          31       0.59      0.61      0.60        93\n",
      "          32       0.78      0.75      0.77       101\n",
      "          33       0.79      0.72      0.75       112\n",
      "          34       0.79      0.78      0.78        95\n",
      "          35       0.82      0.89      0.85       107\n",
      "          36       0.72      0.62      0.67       104\n",
      "          37       0.68      0.70      0.69       102\n",
      "          38       0.82      0.73      0.77       103\n",
      "          39       0.72      0.87      0.79        82\n",
      "          40       0.85      0.85      0.85       116\n",
      "          41       0.70      0.71      0.71        83\n",
      "          42       0.86      0.85      0.86        96\n",
      "          43       0.73      0.65      0.69       113\n",
      "          44       0.70      0.82      0.75        97\n",
      "          45       0.89      0.92      0.90        87\n",
      "          46       0.81      0.82      0.82        97\n",
      "          47       0.84      0.75      0.79       110\n",
      "          48       0.66      0.62      0.64       104\n",
      "          49       0.87      0.80      0.83       104\n",
      "          50       0.49      0.51      0.50       105\n",
      "          51       1.00      1.00      1.00        41\n",
      "          52       0.93      0.91      0.92        74\n",
      "          53       0.65      0.61      0.63        97\n",
      "\n",
      "    accuracy                           0.78      5296\n",
      "   macro avg       0.79      0.79      0.79      5296\n",
      "weighted avg       0.79      0.78      0.78      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 65,  accuracy score is 1.0\n",
      "at random state 65, confusion matrix is [[83  0  0 ...  0  0  0]\n",
      " [ 0 52  0 ...  0  0  0]\n",
      " [ 0  0 85 ...  0  0  1]\n",
      " ...\n",
      " [ 0  0  0 ... 52  0  0]\n",
      " [ 0  0  0 ...  0 77  0]\n",
      " [ 0  1  0 ...  0  0 64]]\n",
      "at random state 65, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.78      0.78       106\n",
      "           1       0.61      0.56      0.58        93\n",
      "           2       0.85      0.87      0.86        98\n",
      "           3       0.78      0.73      0.76        97\n",
      "           4       0.88      0.92      0.90       108\n",
      "           5       0.70      0.66      0.68       107\n",
      "           6       0.95      0.95      0.95        75\n",
      "           7       0.90      0.81      0.85       103\n",
      "           8       0.93      0.93      0.93       107\n",
      "           9       0.88      0.80      0.83       108\n",
      "          10       0.62      0.63      0.62        95\n",
      "          11       0.83      0.82      0.82       106\n",
      "          12       0.85      0.79      0.82        95\n",
      "          13       0.69      0.71      0.70       101\n",
      "          14       0.83      0.78      0.81       102\n",
      "          15       0.97      0.94      0.95        88\n",
      "          16       0.84      0.94      0.88       114\n",
      "          17       0.84      0.81      0.83        91\n",
      "          18       0.84      0.82      0.83       111\n",
      "          19       0.76      0.82      0.79        98\n",
      "          20       0.90      0.84      0.87       108\n",
      "          21       0.66      0.63      0.65        95\n",
      "          22       0.93      0.95      0.94        99\n",
      "          23       0.78      0.78      0.78        86\n",
      "          24       0.59      0.68      0.63       111\n",
      "          25       0.89      0.97      0.93        96\n",
      "          26       0.59      0.66      0.62        96\n",
      "          27       0.73      0.83      0.77       110\n",
      "          28       0.83      0.78      0.80       100\n",
      "          29       0.94      0.88      0.91       110\n",
      "          30       0.90      0.88      0.89        97\n",
      "          31       0.53      0.57      0.55        92\n",
      "          32       0.88      0.74      0.81       109\n",
      "          33       0.70      0.78      0.74       104\n",
      "          34       0.80      0.70      0.75        98\n",
      "          35       0.87      0.88      0.87       106\n",
      "          36       0.63      0.60      0.61       104\n",
      "          37       0.63      0.76      0.69        89\n",
      "          38       0.73      0.77      0.75        96\n",
      "          39       0.82      0.79      0.80        95\n",
      "          40       0.88      0.83      0.85       104\n",
      "          41       0.68      0.80      0.73        93\n",
      "          42       0.90      0.88      0.89        72\n",
      "          43       0.74      0.73      0.73       110\n",
      "          44       0.81      0.86      0.83        98\n",
      "          45       0.95      0.95      0.95        79\n",
      "          46       0.89      0.89      0.89       100\n",
      "          47       0.69      0.67      0.68       101\n",
      "          48       0.65      0.67      0.66        90\n",
      "          49       0.76      0.72      0.74       102\n",
      "          50       0.46      0.46      0.46       105\n",
      "          51       1.00      1.00      1.00        52\n",
      "          52       0.94      0.95      0.94        81\n",
      "          53       0.69      0.61      0.65       105\n",
      "\n",
      "    accuracy                           0.78      5296\n",
      "   macro avg       0.79      0.79      0.79      5296\n",
      "weighted avg       0.79      0.78      0.78      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 66,  accuracy score is 1.0\n",
      "at random state 66, confusion matrix is [[76  0  0 ...  0  0  0]\n",
      " [ 0 77  0 ...  0  0  2]\n",
      " [ 0  0 81 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 48  3  0]\n",
      " [ 0  0  0 ...  0 66  0]\n",
      " [ 0  0  0 ...  0  0 77]]\n",
      "at random state 66, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.82      0.81        93\n",
      "           1       0.71      0.76      0.74       101\n",
      "           2       0.89      0.84      0.86        97\n",
      "           3       0.72      0.72      0.72        88\n",
      "           4       0.87      0.90      0.88        89\n",
      "           5       0.63      0.79      0.70        84\n",
      "           6       0.93      0.95      0.94        86\n",
      "           7       0.74      0.82      0.78        91\n",
      "           8       0.92      0.96      0.94       102\n",
      "           9       0.72      0.83      0.77       101\n",
      "          10       0.68      0.62      0.65       118\n",
      "          11       0.88      0.86      0.87       127\n",
      "          12       0.90      0.86      0.88       111\n",
      "          13       0.80      0.71      0.75       106\n",
      "          14       0.79      0.74      0.76       100\n",
      "          15       0.95      0.96      0.96        81\n",
      "          16       0.83      0.94      0.88       101\n",
      "          17       0.76      0.81      0.79       100\n",
      "          18       0.85      0.84      0.84        86\n",
      "          19       0.78      0.73      0.76       108\n",
      "          20       0.85      0.84      0.85       111\n",
      "          21       0.66      0.61      0.63       110\n",
      "          22       0.93      0.94      0.93        98\n",
      "          23       0.79      0.68      0.73       120\n",
      "          24       0.73      0.58      0.64        99\n",
      "          25       0.91      0.92      0.91        98\n",
      "          26       0.65      0.62      0.64       114\n",
      "          27       0.79      0.76      0.78       109\n",
      "          28       0.90      0.81      0.85        96\n",
      "          29       0.91      0.90      0.91        92\n",
      "          30       0.86      0.89      0.88        93\n",
      "          31       0.66      0.67      0.66       108\n",
      "          32       0.78      0.75      0.76        95\n",
      "          33       0.76      0.76      0.76       102\n",
      "          34       0.79      0.84      0.81       101\n",
      "          35       0.83      0.84      0.83       107\n",
      "          36       0.65      0.75      0.70        91\n",
      "          37       0.61      0.77      0.68        82\n",
      "          38       0.81      0.69      0.74       108\n",
      "          39       0.76      0.78      0.77        96\n",
      "          40       0.83      0.81      0.82        98\n",
      "          41       0.76      0.83      0.79       103\n",
      "          42       0.90      0.88      0.89        78\n",
      "          43       0.71      0.79      0.75        91\n",
      "          44       0.77      0.80      0.78       106\n",
      "          45       0.94      0.90      0.92       105\n",
      "          46       0.92      0.87      0.89        87\n",
      "          47       0.75      0.73      0.74       100\n",
      "          48       0.77      0.68      0.72       112\n",
      "          49       0.76      0.72      0.74        92\n",
      "          50       0.48      0.49      0.48        97\n",
      "          51       1.00      0.94      0.97        51\n",
      "          52       0.88      0.87      0.87        76\n",
      "          53       0.70      0.77      0.73       100\n",
      "\n",
      "    accuracy                           0.79      5296\n",
      "   macro avg       0.80      0.79      0.79      5296\n",
      "weighted avg       0.79      0.79      0.79      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 67,  accuracy score is 1.0\n",
      "at random state 67, confusion matrix is [[70  0  0 ...  0  0  0]\n",
      " [ 0 67  0 ...  0  0  2]\n",
      " [ 0  0 93 ...  0  0  1]\n",
      " ...\n",
      " [ 0  0  0 ... 43  0  0]\n",
      " [ 0  0  0 ...  0 81  0]\n",
      " [ 0  0  3 ...  0  0 76]]\n",
      "at random state 67, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.70      0.75       100\n",
      "           1       0.67      0.62      0.64       108\n",
      "           2       0.89      0.89      0.89       104\n",
      "           3       0.84      0.84      0.84       116\n",
      "           4       0.87      0.90      0.89        99\n",
      "           5       0.65      0.75      0.69        95\n",
      "           6       0.91      0.91      0.91        87\n",
      "           7       0.81      0.72      0.76       102\n",
      "           8       0.90      0.94      0.92        97\n",
      "           9       0.80      0.79      0.80       106\n",
      "          10       0.63      0.70      0.66        87\n",
      "          11       0.89      0.80      0.84        98\n",
      "          12       0.84      0.88      0.86        95\n",
      "          13       0.69      0.77      0.73        92\n",
      "          14       0.80      0.82      0.81       102\n",
      "          15       0.92      0.93      0.93        73\n",
      "          16       0.93      0.86      0.89       104\n",
      "          17       0.82      0.78      0.80       100\n",
      "          18       0.82      0.87      0.85       103\n",
      "          19       0.81      0.80      0.80        99\n",
      "          20       0.84      0.87      0.85        91\n",
      "          21       0.55      0.66      0.60        88\n",
      "          22       0.92      0.89      0.91        92\n",
      "          23       0.75      0.83      0.79       106\n",
      "          24       0.59      0.58      0.58       109\n",
      "          25       0.91      0.94      0.92        95\n",
      "          26       0.76      0.72      0.74       119\n",
      "          27       0.80      0.74      0.77        98\n",
      "          28       0.88      0.89      0.89       103\n",
      "          29       0.91      0.81      0.85       108\n",
      "          30       0.85      0.84      0.85       103\n",
      "          31       0.60      0.62      0.61       104\n",
      "          32       0.75      0.86      0.80       104\n",
      "          33       0.74      0.79      0.76        92\n",
      "          34       0.74      0.81      0.77        89\n",
      "          35       0.86      0.88      0.87        98\n",
      "          36       0.73      0.72      0.72       109\n",
      "          37       0.74      0.74      0.74       102\n",
      "          38       0.74      0.76      0.75       105\n",
      "          39       0.89      0.90      0.89        88\n",
      "          40       0.82      0.78      0.80        92\n",
      "          41       0.82      0.75      0.78       110\n",
      "          42       0.96      0.88      0.92        93\n",
      "          43       0.77      0.79      0.78       101\n",
      "          44       0.74      0.82      0.78       101\n",
      "          45       0.93      0.87      0.90        85\n",
      "          46       0.88      0.91      0.90       100\n",
      "          47       0.75      0.59      0.66       106\n",
      "          48       0.68      0.70      0.69        98\n",
      "          49       0.75      0.62      0.68        95\n",
      "          50       0.47      0.55      0.51       102\n",
      "          51       0.98      0.98      0.98        44\n",
      "          52       0.95      0.94      0.95        86\n",
      "          53       0.75      0.67      0.71       113\n",
      "\n",
      "    accuracy                           0.79      5296\n",
      "   macro avg       0.80      0.80      0.80      5296\n",
      "weighted avg       0.79      0.79      0.79      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 68,  accuracy score is 1.0\n",
      "at random state 68, confusion matrix is [[80  0  0 ...  0  0  0]\n",
      " [ 0 65  0 ...  0  0  0]\n",
      " [ 0  0 93 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 55  0  0]\n",
      " [ 0  0  0 ...  0 71  0]\n",
      " [ 0  0  2 ...  0  0 69]]\n",
      "at random state 68, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.78      0.76       102\n",
      "           1       0.66      0.65      0.65       100\n",
      "           2       0.94      0.95      0.94        98\n",
      "           3       0.80      0.80      0.80       108\n",
      "           4       0.91      0.81      0.86       106\n",
      "           5       0.72      0.72      0.72       113\n",
      "           6       0.90      0.95      0.93        88\n",
      "           7       0.76      0.84      0.80        83\n",
      "           8       0.90      0.86      0.88        85\n",
      "           9       0.82      0.77      0.79       115\n",
      "          10       0.69      0.73      0.71        97\n",
      "          11       0.87      0.88      0.87       105\n",
      "          12       0.79      0.87      0.83       108\n",
      "          13       0.70      0.69      0.70       101\n",
      "          14       0.91      0.81      0.86       106\n",
      "          15       0.93      0.99      0.96        86\n",
      "          16       0.85      0.83      0.84        96\n",
      "          17       0.73      0.77      0.75        77\n",
      "          18       0.90      0.89      0.90       128\n",
      "          19       0.78      0.76      0.77       100\n",
      "          20       0.84      0.80      0.82        89\n",
      "          21       0.67      0.65      0.66        93\n",
      "          22       0.89      0.95      0.92        86\n",
      "          23       0.78      0.71      0.74        98\n",
      "          24       0.60      0.71      0.65        95\n",
      "          25       0.98      0.91      0.94       103\n",
      "          26       0.62      0.68      0.65        99\n",
      "          27       0.77      0.78      0.77       101\n",
      "          28       0.91      0.88      0.89        96\n",
      "          29       0.89      0.93      0.91        90\n",
      "          30       0.84      0.87      0.86        99\n",
      "          31       0.69      0.60      0.64       115\n",
      "          32       0.70      0.77      0.73        87\n",
      "          33       0.73      0.83      0.78       102\n",
      "          34       0.80      0.80      0.80       112\n",
      "          35       0.84      0.80      0.82        98\n",
      "          36       0.68      0.68      0.68        98\n",
      "          37       0.74      0.77      0.75       107\n",
      "          38       0.78      0.78      0.78        93\n",
      "          39       0.85      0.83      0.84       114\n",
      "          40       0.85      0.77      0.81       111\n",
      "          41       0.77      0.76      0.77        97\n",
      "          42       0.88      0.89      0.88        87\n",
      "          43       0.80      0.65      0.72       105\n",
      "          44       0.84      0.82      0.83        99\n",
      "          45       0.91      0.97      0.94        93\n",
      "          46       0.95      0.92      0.93        95\n",
      "          47       0.72      0.71      0.71       103\n",
      "          48       0.71      0.73      0.72        99\n",
      "          49       0.70      0.74      0.72        98\n",
      "          50       0.50      0.52      0.51        99\n",
      "          51       0.98      1.00      0.99        55\n",
      "          52       0.88      0.90      0.89        79\n",
      "          53       0.75      0.70      0.72        99\n",
      "\n",
      "    accuracy                           0.79      5296\n",
      "   macro avg       0.80      0.80      0.80      5296\n",
      "weighted avg       0.80      0.79      0.79      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 69,  accuracy score is 1.0\n",
      "at random state 69, confusion matrix is [[93  0  0 ...  0  0  0]\n",
      " [ 0 83  0 ...  0  0  2]\n",
      " [ 0  0 85 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 53  0  0]\n",
      " [ 0  0  0 ...  0 76  0]\n",
      " [ 0  2  0 ...  0  0 63]]\n",
      "at random state 69, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.80      0.83       116\n",
      "           1       0.77      0.70      0.73       119\n",
      "           2       0.86      0.88      0.87        97\n",
      "           3       0.84      0.79      0.81        97\n",
      "           4       0.88      0.88      0.88       100\n",
      "           5       0.67      0.73      0.70        88\n",
      "           6       0.95      0.89      0.92        87\n",
      "           7       0.75      0.90      0.82        91\n",
      "           8       0.94      0.94      0.94       115\n",
      "           9       0.84      0.85      0.85       103\n",
      "          10       0.67      0.72      0.69        88\n",
      "          11       0.87      0.87      0.87       108\n",
      "          12       0.82      0.74      0.78        99\n",
      "          13       0.73      0.76      0.74        94\n",
      "          14       0.80      0.77      0.79       106\n",
      "          15       0.92      0.93      0.92        85\n",
      "          16       0.90      0.89      0.89       116\n",
      "          17       0.89      0.73      0.81       113\n",
      "          18       0.79      0.90      0.84        98\n",
      "          19       0.74      0.83      0.78        86\n",
      "          20       0.87      0.86      0.86        97\n",
      "          21       0.60      0.65      0.62        98\n",
      "          22       0.94      0.94      0.94       113\n",
      "          23       0.82      0.78      0.80       102\n",
      "          24       0.63      0.62      0.63       101\n",
      "          25       0.92      0.93      0.92        85\n",
      "          26       0.66      0.65      0.65       102\n",
      "          27       0.82      0.75      0.78       106\n",
      "          28       0.88      0.85      0.87       102\n",
      "          29       0.92      0.90      0.91        86\n",
      "          30       0.87      0.92      0.89       109\n",
      "          31       0.61      0.68      0.64       105\n",
      "          32       0.81      0.86      0.84        94\n",
      "          33       0.77      0.74      0.76       101\n",
      "          34       0.78      0.77      0.77       115\n",
      "          35       0.85      0.87      0.86       100\n",
      "          36       0.61      0.65      0.63        99\n",
      "          37       0.76      0.67      0.71       100\n",
      "          38       0.72      0.71      0.72        84\n",
      "          39       0.89      0.83      0.86        96\n",
      "          40       0.73      0.88      0.80        88\n",
      "          41       0.79      0.69      0.74       110\n",
      "          42       0.87      0.90      0.89        91\n",
      "          43       0.70      0.69      0.69        99\n",
      "          44       0.78      0.83      0.80        86\n",
      "          45       0.92      0.89      0.90        89\n",
      "          46       0.88      0.86      0.87        93\n",
      "          47       0.73      0.73      0.73       101\n",
      "          48       0.69      0.68      0.68       102\n",
      "          49       0.80      0.77      0.78       103\n",
      "          50       0.43      0.53      0.48        90\n",
      "          51       1.00      1.00      1.00        53\n",
      "          52       0.87      0.93      0.90        82\n",
      "          53       0.71      0.58      0.64       108\n",
      "\n",
      "    accuracy                           0.79      5296\n",
      "   macro avg       0.80      0.80      0.80      5296\n",
      "weighted avg       0.80      0.79      0.79      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 70,  accuracy score is 1.0\n",
      "at random state 70, confusion matrix is [[80  0  0 ...  0  0  0]\n",
      " [ 0 69  0 ...  0  0  0]\n",
      " [ 0  0 85 ...  0  0  1]\n",
      " ...\n",
      " [ 0  0  0 ... 46  0  0]\n",
      " [ 0  0  0 ...  1 71  0]\n",
      " [ 0  5  1 ...  0  0 64]]\n",
      "at random state 70, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.77      0.78       104\n",
      "           1       0.69      0.68      0.69       101\n",
      "           2       0.88      0.89      0.88        96\n",
      "           3       0.84      0.92      0.88        96\n",
      "           4       0.80      0.87      0.84       102\n",
      "           5       0.73      0.65      0.69       111\n",
      "           6       0.93      0.94      0.93        82\n",
      "           7       0.79      0.86      0.82        92\n",
      "           8       0.88      0.85      0.87       109\n",
      "           9       0.79      0.78      0.79       104\n",
      "          10       0.73      0.63      0.68       119\n",
      "          11       0.81      0.86      0.84        96\n",
      "          12       0.88      0.79      0.83       110\n",
      "          13       0.71      0.76      0.73       111\n",
      "          14       0.75      0.77      0.76       111\n",
      "          15       0.96      0.91      0.94        79\n",
      "          16       0.88      0.85      0.86       112\n",
      "          17       0.78      0.80      0.79        99\n",
      "          18       0.79      0.83      0.81        90\n",
      "          19       0.83      0.80      0.82       102\n",
      "          20       0.88      0.85      0.87       106\n",
      "          21       0.62      0.59      0.60        98\n",
      "          22       0.87      0.90      0.89        99\n",
      "          23       0.72      0.76      0.74        93\n",
      "          24       0.59      0.57      0.58       104\n",
      "          25       0.92      0.92      0.92        86\n",
      "          26       0.63      0.66      0.64       104\n",
      "          27       0.79      0.79      0.79       107\n",
      "          28       0.82      0.86      0.84        92\n",
      "          29       0.87      0.79      0.83        97\n",
      "          30       0.88      0.83      0.85       104\n",
      "          31       0.56      0.65      0.60        93\n",
      "          32       0.72      0.76      0.74        94\n",
      "          33       0.79      0.73      0.76        94\n",
      "          34       0.83      0.73      0.78       101\n",
      "          35       0.88      0.81      0.84       107\n",
      "          36       0.63      0.70      0.66        99\n",
      "          37       0.76      0.73      0.74       101\n",
      "          38       0.76      0.70      0.73        94\n",
      "          39       0.79      0.90      0.85        94\n",
      "          40       0.78      0.91      0.84       102\n",
      "          41       0.68      0.61      0.64       118\n",
      "          42       0.88      0.90      0.89        79\n",
      "          43       0.70      0.65      0.68       101\n",
      "          44       0.80      0.82      0.81        96\n",
      "          45       0.88      0.95      0.91        83\n",
      "          46       0.97      0.93      0.95        90\n",
      "          47       0.78      0.76      0.77       106\n",
      "          48       0.71      0.79      0.75       100\n",
      "          49       0.73      0.74      0.73       102\n",
      "          50       0.52      0.49      0.50       111\n",
      "          51       0.98      1.00      0.99        46\n",
      "          52       0.91      0.93      0.92        76\n",
      "          53       0.74      0.69      0.71        93\n",
      "\n",
      "    accuracy                           0.78      5296\n",
      "   macro avg       0.79      0.79      0.79      5296\n",
      "weighted avg       0.78      0.78      0.78      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 71,  accuracy score is 1.0\n",
      "at random state 71, confusion matrix is [[92  0  0 ...  0  0  0]\n",
      " [ 0 72  0 ...  0  0  1]\n",
      " [ 0  0 93 ...  0  0  3]\n",
      " ...\n",
      " [ 0  0  0 ... 57  0  0]\n",
      " [ 0  0  0 ...  0 74  0]\n",
      " [ 0  2  4 ...  0  0 80]]\n",
      "at random state 71, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.77      0.78       120\n",
      "           1       0.64      0.73      0.68        99\n",
      "           2       0.89      0.90      0.90       103\n",
      "           3       0.81      0.80      0.81       102\n",
      "           4       0.85      0.90      0.87       107\n",
      "           5       0.71      0.72      0.71       103\n",
      "           6       0.91      0.99      0.95        87\n",
      "           7       0.78      0.82      0.80       102\n",
      "           8       0.98      0.91      0.95       104\n",
      "           9       0.85      0.84      0.85       118\n",
      "          10       0.74      0.83      0.78        88\n",
      "          11       0.85      0.87      0.86       112\n",
      "          12       0.83      0.80      0.82        91\n",
      "          13       0.77      0.75      0.76       103\n",
      "          14       0.80      0.72      0.75       109\n",
      "          15       0.92      0.97      0.94        70\n",
      "          16       0.85      0.89      0.87       103\n",
      "          17       0.83      0.77      0.80        97\n",
      "          18       0.84      0.85      0.85       108\n",
      "          19       0.77      0.85      0.81       102\n",
      "          20       0.91      0.83      0.86       104\n",
      "          21       0.64      0.53      0.58       105\n",
      "          22       0.90      0.92      0.91       101\n",
      "          23       0.74      0.80      0.77       101\n",
      "          24       0.63      0.62      0.62        97\n",
      "          25       0.94      0.92      0.93       108\n",
      "          26       0.69      0.74      0.72        92\n",
      "          27       0.81      0.81      0.81       103\n",
      "          28       0.82      0.85      0.84       110\n",
      "          29       0.86      0.86      0.86       101\n",
      "          30       0.86      0.90      0.88       100\n",
      "          31       0.66      0.69      0.67        99\n",
      "          32       0.79      0.80      0.79        89\n",
      "          33       0.77      0.72      0.75       104\n",
      "          34       0.81      0.73      0.77       101\n",
      "          35       0.82      0.85      0.84        96\n",
      "          36       0.69      0.68      0.68        90\n",
      "          37       0.71      0.67      0.69        90\n",
      "          38       0.74      0.70      0.72        88\n",
      "          39       0.83      0.80      0.81        79\n",
      "          40       0.85      0.85      0.85        99\n",
      "          41       0.69      0.76      0.72        95\n",
      "          42       0.90      0.91      0.90        95\n",
      "          43       0.84      0.77      0.80        99\n",
      "          44       0.69      0.75      0.72       100\n",
      "          45       0.91      0.92      0.92        89\n",
      "          46       0.98      0.91      0.94        93\n",
      "          47       0.82      0.73      0.77       118\n",
      "          48       0.76      0.67      0.71        92\n",
      "          49       0.84      0.81      0.83       102\n",
      "          50       0.41      0.48      0.44        86\n",
      "          51       1.00      1.00      1.00        57\n",
      "          52       0.90      0.97      0.94        76\n",
      "          53       0.75      0.73      0.74       109\n",
      "\n",
      "    accuracy                           0.80      5296\n",
      "   macro avg       0.80      0.80      0.80      5296\n",
      "weighted avg       0.80      0.80      0.80      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 72,  accuracy score is 1.0\n",
      "at random state 72, confusion matrix is [[71  0  0 ...  0  0  0]\n",
      " [ 0 74  0 ...  0  0  4]\n",
      " [ 0  0 82 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 51  0  0]\n",
      " [ 0  0  0 ...  0 74  0]\n",
      " [ 0  1  0 ...  0  0 72]]\n",
      "at random state 72, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.73      0.77        97\n",
      "           1       0.67      0.71      0.69       104\n",
      "           2       0.93      0.89      0.91        92\n",
      "           3       0.83      0.81      0.82       111\n",
      "           4       0.88      0.85      0.86       105\n",
      "           5       0.68      0.73      0.71       105\n",
      "           6       0.95      0.96      0.95        74\n",
      "           7       0.83      0.79      0.81       115\n",
      "           8       0.96      0.94      0.95        99\n",
      "           9       0.77      0.83      0.80        99\n",
      "          10       0.70      0.68      0.69       106\n",
      "          11       0.85      0.88      0.87       100\n",
      "          12       0.86      0.85      0.85       112\n",
      "          13       0.74      0.64      0.68       100\n",
      "          14       0.85      0.69      0.76       107\n",
      "          15       0.95      1.00      0.97        78\n",
      "          16       0.84      0.86      0.85       106\n",
      "          17       0.77      0.82      0.80        88\n",
      "          18       0.85      0.81      0.83       108\n",
      "          19       0.77      0.80      0.79       111\n",
      "          20       0.89      0.79      0.84       104\n",
      "          21       0.64      0.59      0.61       107\n",
      "          22       0.92      0.92      0.92        91\n",
      "          23       0.76      0.76      0.76       104\n",
      "          24       0.62      0.56      0.59       100\n",
      "          25       0.93      0.96      0.95        83\n",
      "          26       0.65      0.69      0.67       102\n",
      "          27       0.80      0.71      0.75        95\n",
      "          28       0.82      0.81      0.82        96\n",
      "          29       0.94      0.88      0.91       103\n",
      "          30       0.85      0.86      0.85        97\n",
      "          31       0.60      0.74      0.66        84\n",
      "          32       0.79      0.80      0.80       102\n",
      "          33       0.81      0.75      0.78       123\n",
      "          34       0.76      0.83      0.79       100\n",
      "          35       0.83      0.88      0.86       104\n",
      "          36       0.70      0.65      0.68        95\n",
      "          37       0.74      0.76      0.75        96\n",
      "          38       0.74      0.76      0.75        95\n",
      "          39       0.78      0.90      0.84        94\n",
      "          40       0.86      0.82      0.84        94\n",
      "          41       0.68      0.78      0.73        88\n",
      "          42       0.93      0.87      0.90        89\n",
      "          43       0.73      0.73      0.73       106\n",
      "          44       0.78      0.85      0.81       108\n",
      "          45       0.88      0.91      0.90        89\n",
      "          46       0.95      0.92      0.93       108\n",
      "          47       0.82      0.82      0.82       102\n",
      "          48       0.67      0.72      0.69        86\n",
      "          49       0.82      0.75      0.79       106\n",
      "          50       0.47      0.52      0.49        92\n",
      "          51       1.00      1.00      1.00        51\n",
      "          52       0.93      0.93      0.93        80\n",
      "          53       0.65      0.69      0.67       105\n",
      "\n",
      "    accuracy                           0.79      5296\n",
      "   macro avg       0.80      0.80      0.80      5296\n",
      "weighted avg       0.80      0.79      0.80      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 73,  accuracy score is 1.0\n",
      "at random state 73, confusion matrix is [[85  0  0 ...  0  0  0]\n",
      " [ 0 85  0 ...  0  0  1]\n",
      " [ 0  0 75 ...  0  0  2]\n",
      " ...\n",
      " [ 0  0  0 ... 45  0  0]\n",
      " [ 0  0  0 ...  0 63  0]\n",
      " [ 0  0  0 ...  0  0 68]]\n",
      "at random state 73, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.78      0.79       109\n",
      "           1       0.73      0.73      0.73       117\n",
      "           2       0.87      0.84      0.86        89\n",
      "           3       0.77      0.86      0.81        97\n",
      "           4       0.85      0.89      0.87       111\n",
      "           5       0.74      0.75      0.74        99\n",
      "           6       0.94      0.94      0.94        96\n",
      "           7       0.79      0.83      0.81       103\n",
      "           8       0.91      0.91      0.91       103\n",
      "           9       0.75      0.77      0.76        99\n",
      "          10       0.73      0.69      0.71       116\n",
      "          11       0.80      0.85      0.82        97\n",
      "          12       0.89      0.92      0.90       108\n",
      "          13       0.79      0.74      0.76       104\n",
      "          14       0.82      0.77      0.79        99\n",
      "          15       0.93      0.91      0.92        89\n",
      "          16       0.90      0.96      0.93        98\n",
      "          17       0.83      0.77      0.80       104\n",
      "          18       0.80      0.71      0.76       104\n",
      "          19       0.77      0.80      0.79        97\n",
      "          20       0.86      0.86      0.86        97\n",
      "          21       0.61      0.66      0.64       101\n",
      "          22       0.89      0.92      0.90        92\n",
      "          23       0.79      0.77      0.78       101\n",
      "          24       0.70      0.62      0.66       105\n",
      "          25       0.92      0.94      0.93        97\n",
      "          26       0.62      0.58      0.60       110\n",
      "          27       0.76      0.79      0.78        97\n",
      "          28       0.81      0.78      0.79        89\n",
      "          29       0.87      0.93      0.90       100\n",
      "          30       0.87      0.82      0.84       103\n",
      "          31       0.68      0.60      0.64       100\n",
      "          32       0.78      0.86      0.82       102\n",
      "          33       0.72      0.72      0.72       114\n",
      "          34       0.81      0.83      0.82       111\n",
      "          35       0.85      0.88      0.87        84\n",
      "          36       0.77      0.71      0.74       106\n",
      "          37       0.72      0.75      0.73        88\n",
      "          38       0.76      0.82      0.79       102\n",
      "          39       0.80      0.78      0.79        91\n",
      "          40       0.83      0.80      0.82       114\n",
      "          41       0.78      0.77      0.77       109\n",
      "          42       0.83      0.91      0.87        87\n",
      "          43       0.75      0.75      0.75       100\n",
      "          44       0.81      0.74      0.77        97\n",
      "          45       0.96      0.88      0.92        86\n",
      "          46       0.81      0.90      0.85        70\n",
      "          47       0.74      0.73      0.73        99\n",
      "          48       0.67      0.69      0.68        87\n",
      "          49       0.85      0.84      0.84       113\n",
      "          50       0.48      0.54      0.51        91\n",
      "          51       1.00      0.98      0.99        46\n",
      "          52       0.91      0.88      0.89        72\n",
      "          53       0.72      0.71      0.71        96\n",
      "\n",
      "    accuracy                           0.79      5296\n",
      "   macro avg       0.80      0.80      0.80      5296\n",
      "weighted avg       0.80      0.79      0.79      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 74,  accuracy score is 1.0\n",
      "at random state 74, confusion matrix is [[64  0  0 ...  0  0  0]\n",
      " [ 0 79  0 ...  0  0  4]\n",
      " [ 0  0 90 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 47  0  0]\n",
      " [ 0  0  0 ...  0 80  0]\n",
      " [ 0  0  1 ...  0  0 75]]\n",
      "at random state 74, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.80      0.78        80\n",
      "           1       0.77      0.64      0.70       123\n",
      "           2       0.89      0.91      0.90        99\n",
      "           3       0.84      0.77      0.81       105\n",
      "           4       0.90      0.83      0.86       105\n",
      "           5       0.73      0.69      0.71       107\n",
      "           6       0.92      0.95      0.93        81\n",
      "           7       0.84      0.80      0.82       104\n",
      "           8       0.92      0.88      0.90        98\n",
      "           9       0.80      0.81      0.81       101\n",
      "          10       0.70      0.75      0.72        89\n",
      "          11       0.87      0.86      0.86       106\n",
      "          12       0.88      0.79      0.83        94\n",
      "          13       0.70      0.78      0.74        97\n",
      "          14       0.74      0.79      0.77        91\n",
      "          15       0.90      0.91      0.91        79\n",
      "          16       0.84      0.89      0.87        91\n",
      "          17       0.80      0.87      0.83       121\n",
      "          18       0.86      0.84      0.85        97\n",
      "          19       0.83      0.85      0.84       106\n",
      "          20       0.86      0.84      0.85       120\n",
      "          21       0.62      0.70      0.66       104\n",
      "          22       0.95      0.91      0.93        89\n",
      "          23       0.73      0.81      0.77        93\n",
      "          24       0.62      0.64      0.63        99\n",
      "          25       0.91      0.95      0.93        99\n",
      "          26       0.68      0.63      0.65       110\n",
      "          27       0.83      0.74      0.78       100\n",
      "          28       0.85      0.89      0.87        96\n",
      "          29       0.87      0.87      0.87        94\n",
      "          30       0.88      0.84      0.86       117\n",
      "          31       0.63      0.63      0.63       101\n",
      "          32       0.79      0.72      0.75        96\n",
      "          33       0.78      0.81      0.79       107\n",
      "          34       0.84      0.83      0.83       109\n",
      "          35       0.86      0.79      0.82       107\n",
      "          36       0.66      0.69      0.68       107\n",
      "          37       0.66      0.75      0.70        91\n",
      "          38       0.73      0.72      0.73        94\n",
      "          39       0.84      0.80      0.82       103\n",
      "          40       0.80      0.80      0.80        95\n",
      "          41       0.75      0.66      0.70        82\n",
      "          42       0.87      0.91      0.89        93\n",
      "          43       0.78      0.69      0.73       120\n",
      "          44       0.74      0.74      0.74       102\n",
      "          45       0.96      0.97      0.97        80\n",
      "          46       0.87      0.94      0.90        93\n",
      "          47       0.86      0.74      0.80        98\n",
      "          48       0.63      0.73      0.68        93\n",
      "          49       0.72      0.68      0.70       108\n",
      "          50       0.47      0.55      0.51        89\n",
      "          51       1.00      1.00      1.00        47\n",
      "          52       0.95      0.95      0.95        84\n",
      "          53       0.65      0.74      0.69       102\n",
      "\n",
      "    accuracy                           0.79      5296\n",
      "   macro avg       0.80      0.80      0.80      5296\n",
      "weighted avg       0.80      0.79      0.79      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 75,  accuracy score is 1.0\n",
      "at random state 75, confusion matrix is [[ 80   0   0 ...   0   0   0]\n",
      " [  0  67   0 ...   0   0   1]\n",
      " [  0   0 105 ...   0   0   1]\n",
      " ...\n",
      " [  0   0   0 ...  48   0   0]\n",
      " [  0   0   0 ...   0  51   0]\n",
      " [  0   1   1 ...   0   0  67]]\n",
      "at random state 75, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.80      0.81       100\n",
      "           1       0.73      0.67      0.70       100\n",
      "           2       0.91      0.91      0.91       116\n",
      "           3       0.75      0.83      0.79        92\n",
      "           4       0.81      0.84      0.83       101\n",
      "           5       0.70      0.75      0.73       104\n",
      "           6       0.95      0.93      0.94        80\n",
      "           7       0.87      0.80      0.83       114\n",
      "           8       0.94      0.97      0.95        98\n",
      "           9       0.71      0.80      0.75       101\n",
      "          10       0.75      0.68      0.71       112\n",
      "          11       0.89      0.81      0.85       115\n",
      "          12       0.91      0.88      0.89       105\n",
      "          13       0.73      0.83      0.77       103\n",
      "          14       0.85      0.79      0.82       108\n",
      "          15       0.91      0.94      0.92        78\n",
      "          16       0.83      0.92      0.87       110\n",
      "          17       0.93      0.77      0.84       111\n",
      "          18       0.79      0.84      0.81        92\n",
      "          19       0.79      0.74      0.76       105\n",
      "          20       0.94      0.82      0.88        99\n",
      "          21       0.59      0.67      0.63        92\n",
      "          22       0.88      0.93      0.90        94\n",
      "          23       0.75      0.72      0.73       109\n",
      "          24       0.60      0.62      0.61        91\n",
      "          25       0.83      0.96      0.89        94\n",
      "          26       0.64      0.72      0.68        94\n",
      "          27       0.73      0.85      0.79        85\n",
      "          28       0.91      0.82      0.86        99\n",
      "          29       0.92      0.86      0.89       111\n",
      "          30       0.86      0.89      0.88       113\n",
      "          31       0.59      0.56      0.57        99\n",
      "          32       0.78      0.74      0.76       102\n",
      "          33       0.69      0.70      0.69        94\n",
      "          34       0.84      0.73      0.78        89\n",
      "          35       0.92      0.83      0.87       106\n",
      "          36       0.67      0.62      0.65        96\n",
      "          37       0.75      0.81      0.78       105\n",
      "          38       0.80      0.83      0.81       106\n",
      "          39       0.84      0.81      0.83       108\n",
      "          40       0.81      0.87      0.84       103\n",
      "          41       0.78      0.80      0.79        87\n",
      "          42       0.93      0.83      0.88        98\n",
      "          43       0.70      0.78      0.73        94\n",
      "          44       0.75      0.74      0.75        94\n",
      "          45       0.96      0.91      0.93        95\n",
      "          46       0.86      0.95      0.90       104\n",
      "          47       0.83      0.80      0.81        99\n",
      "          48       0.74      0.71      0.73        87\n",
      "          49       0.80      0.75      0.78       113\n",
      "          50       0.40      0.48      0.44        83\n",
      "          51       0.96      1.00      0.98        48\n",
      "          52       0.86      0.93      0.89        55\n",
      "          53       0.72      0.64      0.68       105\n",
      "\n",
      "    accuracy                           0.80      5296\n",
      "   macro avg       0.80      0.80      0.80      5296\n",
      "weighted avg       0.80      0.80      0.80      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 76,  accuracy score is 1.0\n",
      "at random state 76, confusion matrix is [[82  0  0 ...  0  0  0]\n",
      " [ 0 71  0 ...  0  0  0]\n",
      " [ 0  0 86 ...  0  0  1]\n",
      " ...\n",
      " [ 0  0  0 ... 52  0  0]\n",
      " [ 0  0  0 ...  1 70  0]\n",
      " [ 0  2  2 ...  0  0 62]]\n",
      "at random state 76, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.69      0.74       118\n",
      "           1       0.71      0.72      0.72        98\n",
      "           2       0.83      0.95      0.88        91\n",
      "           3       0.81      0.82      0.82       106\n",
      "           4       0.89      0.86      0.87       111\n",
      "           5       0.72      0.72      0.72        89\n",
      "           6       0.93      0.93      0.93        91\n",
      "           7       0.83      0.78      0.81       111\n",
      "           8       0.96      0.92      0.94        99\n",
      "           9       0.75      0.80      0.78        96\n",
      "          10       0.66      0.69      0.68        94\n",
      "          11       0.82      0.80      0.81       111\n",
      "          12       0.86      0.84      0.85       105\n",
      "          13       0.70      0.73      0.72        97\n",
      "          14       0.75      0.82      0.79        97\n",
      "          15       0.93      0.94      0.93        83\n",
      "          16       0.78      0.88      0.83        90\n",
      "          17       0.74      0.80      0.77        84\n",
      "          18       0.86      0.80      0.83       103\n",
      "          19       0.87      0.74      0.80       110\n",
      "          20       0.87      0.80      0.83       122\n",
      "          21       0.65      0.65      0.65        93\n",
      "          22       0.95      0.94      0.95       113\n",
      "          23       0.76      0.79      0.77       103\n",
      "          24       0.55      0.51      0.53       106\n",
      "          25       0.97      0.90      0.93       106\n",
      "          26       0.58      0.58      0.58       102\n",
      "          27       0.78      0.70      0.74       114\n",
      "          28       0.84      0.83      0.83       115\n",
      "          29       0.85      0.90      0.88        94\n",
      "          30       0.82      0.90      0.86        84\n",
      "          31       0.59      0.60      0.59        97\n",
      "          32       0.80      0.71      0.75       100\n",
      "          33       0.71      0.73      0.72       100\n",
      "          34       0.78      0.80      0.79       109\n",
      "          35       0.85      0.86      0.85        95\n",
      "          36       0.65      0.62      0.64        90\n",
      "          37       0.72      0.72      0.72        96\n",
      "          38       0.81      0.77      0.79        95\n",
      "          39       0.83      0.81      0.82       105\n",
      "          40       0.76      0.80      0.78       100\n",
      "          41       0.70      0.73      0.71       106\n",
      "          42       0.91      0.88      0.90        92\n",
      "          43       0.79      0.73      0.76       105\n",
      "          44       0.75      0.86      0.80        93\n",
      "          45       0.94      0.95      0.94        80\n",
      "          46       0.92      0.94      0.93        98\n",
      "          47       0.81      0.67      0.74        95\n",
      "          48       0.66      0.69      0.67        99\n",
      "          49       0.70      0.79      0.74        84\n",
      "          50       0.44      0.51      0.47        96\n",
      "          51       0.98      1.00      0.99        52\n",
      "          52       0.92      0.91      0.92        77\n",
      "          53       0.60      0.65      0.62        96\n",
      "\n",
      "    accuracy                           0.78      5296\n",
      "   macro avg       0.79      0.79      0.78      5296\n",
      "weighted avg       0.78      0.78      0.78      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 77,  accuracy score is 1.0\n",
      "at random state 77, confusion matrix is [[78  0  0 ...  0  0  0]\n",
      " [ 0 67  0 ...  0  0  1]\n",
      " [ 0  0 97 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 56  0  0]\n",
      " [ 0  0  0 ...  1 75  0]\n",
      " [ 0  0  3 ...  0  0 58]]\n",
      "at random state 77, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.76      0.80       103\n",
      "           1       0.64      0.71      0.67        94\n",
      "           2       0.84      0.92      0.88       105\n",
      "           3       0.82      0.76      0.79       111\n",
      "           4       0.87      0.84      0.85        93\n",
      "           5       0.65      0.69      0.67       104\n",
      "           6       0.97      0.97      0.97        89\n",
      "           7       0.78      0.74      0.76       100\n",
      "           8       0.94      0.96      0.95        96\n",
      "           9       0.77      0.82      0.79       119\n",
      "          10       0.65      0.74      0.69       110\n",
      "          11       0.87      0.84      0.85       108\n",
      "          12       0.88      0.82      0.84       103\n",
      "          13       0.84      0.73      0.78       101\n",
      "          14       0.77      0.79      0.78        90\n",
      "          15       0.90      0.95      0.92        93\n",
      "          16       0.95      0.85      0.89       104\n",
      "          17       0.83      0.75      0.79       102\n",
      "          18       0.81      0.89      0.85        91\n",
      "          19       0.89      0.79      0.84       108\n",
      "          20       0.87      0.88      0.87       110\n",
      "          21       0.61      0.64      0.63       104\n",
      "          22       0.91      0.94      0.93       101\n",
      "          23       0.71      0.79      0.75        99\n",
      "          24       0.62      0.42      0.50       103\n",
      "          25       0.96      0.89      0.92       101\n",
      "          26       0.68      0.60      0.64       105\n",
      "          27       0.76      0.83      0.79        92\n",
      "          28       0.76      0.73      0.75        83\n",
      "          29       0.88      0.89      0.88        98\n",
      "          30       0.81      0.94      0.87        82\n",
      "          31       0.64      0.63      0.64        98\n",
      "          32       0.82      0.80      0.81       102\n",
      "          33       0.76      0.75      0.76       109\n",
      "          34       0.78      0.80      0.79       103\n",
      "          35       0.90      0.90      0.90       104\n",
      "          36       0.72      0.72      0.72       110\n",
      "          37       0.68      0.78      0.73       100\n",
      "          38       0.82      0.80      0.81        87\n",
      "          39       0.80      0.89      0.84        99\n",
      "          40       0.81      0.79      0.80        97\n",
      "          41       0.69      0.73      0.71       106\n",
      "          42       0.85      0.83      0.84        84\n",
      "          43       0.71      0.75      0.73        93\n",
      "          44       0.73      0.74      0.74       100\n",
      "          45       0.94      0.82      0.88        80\n",
      "          46       0.84      0.94      0.89        86\n",
      "          47       0.76      0.72      0.74       129\n",
      "          48       0.68      0.68      0.68        95\n",
      "          49       0.73      0.70      0.72        97\n",
      "          50       0.43      0.51      0.47        94\n",
      "          51       0.97      0.98      0.97        57\n",
      "          52       0.93      0.95      0.94        79\n",
      "          53       0.82      0.68      0.74        85\n",
      "\n",
      "    accuracy                           0.79      5296\n",
      "   macro avg       0.79      0.79      0.79      5296\n",
      "weighted avg       0.79      0.79      0.79      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 78,  accuracy score is 1.0\n",
      "at random state 78, confusion matrix is [[84  0  0 ...  0  0  0]\n",
      " [ 0 74  0 ...  0  0  2]\n",
      " [ 0  0 84 ...  0  0  2]\n",
      " ...\n",
      " [ 0  0  0 ... 55  0  0]\n",
      " [ 0  0  0 ...  0 60  0]\n",
      " [ 0  0  3 ...  0  0 60]]\n",
      "at random state 78, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.77      0.77       109\n",
      "           1       0.65      0.73      0.69       102\n",
      "           2       0.86      0.83      0.84       101\n",
      "           3       0.79      0.79      0.79       106\n",
      "           4       0.85      0.88      0.87        85\n",
      "           5       0.74      0.73      0.74        90\n",
      "           6       0.98      0.91      0.94        95\n",
      "           7       0.80      0.86      0.83        95\n",
      "           8       0.94      0.92      0.93       122\n",
      "           9       0.85      0.80      0.82       104\n",
      "          10       0.72      0.71      0.72        96\n",
      "          11       0.89      0.87      0.88       107\n",
      "          12       0.91      0.83      0.87       109\n",
      "          13       0.76      0.79      0.77        99\n",
      "          14       0.82      0.69      0.75        84\n",
      "          15       0.96      0.90      0.93        82\n",
      "          16       0.91      0.86      0.89       110\n",
      "          17       0.70      0.81      0.75        90\n",
      "          18       0.83      0.89      0.86       108\n",
      "          19       0.85      0.88      0.86        89\n",
      "          20       0.85      0.85      0.85        97\n",
      "          21       0.63      0.62      0.62        94\n",
      "          22       0.91      0.94      0.93       101\n",
      "          23       0.81      0.80      0.81       109\n",
      "          24       0.61      0.54      0.57       118\n",
      "          25       0.91      0.93      0.92        98\n",
      "          26       0.64      0.72      0.68        99\n",
      "          27       0.81      0.77      0.79       112\n",
      "          28       0.85      0.88      0.86       105\n",
      "          29       0.85      0.93      0.89        82\n",
      "          30       0.93      0.83      0.88       106\n",
      "          31       0.62      0.61      0.62       105\n",
      "          32       0.78      0.72      0.75        96\n",
      "          33       0.61      0.66      0.64        94\n",
      "          34       0.82      0.83      0.82        99\n",
      "          35       0.88      0.87      0.87        89\n",
      "          36       0.72      0.70      0.71        82\n",
      "          37       0.82      0.71      0.76       113\n",
      "          38       0.79      0.80      0.79       109\n",
      "          39       0.86      0.83      0.85       108\n",
      "          40       0.83      0.79      0.81       104\n",
      "          41       0.79      0.76      0.78       101\n",
      "          42       0.84      0.88      0.86        96\n",
      "          43       0.73      0.69      0.71       105\n",
      "          44       0.78      0.89      0.83        91\n",
      "          45       0.94      0.93      0.93        98\n",
      "          46       0.85      0.92      0.88        96\n",
      "          47       0.80      0.77      0.79       117\n",
      "          48       0.71      0.68      0.69        98\n",
      "          49       0.76      0.84      0.80        77\n",
      "          50       0.46      0.58      0.51        99\n",
      "          51       1.00      1.00      1.00        55\n",
      "          52       0.85      0.92      0.88        65\n",
      "          53       0.62      0.63      0.63        95\n",
      "\n",
      "    accuracy                           0.80      5296\n",
      "   macro avg       0.80      0.80      0.80      5296\n",
      "weighted avg       0.80      0.80      0.80      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 79,  accuracy score is 1.0\n",
      "at random state 79, confusion matrix is [[ 90   0   0 ...   0   0   0]\n",
      " [  0  51   0 ...   0   0   1]\n",
      " [  0   0 104 ...   0   0   1]\n",
      " ...\n",
      " [  0   0   0 ...  52   0   0]\n",
      " [  0   0   0 ...   0  80   0]\n",
      " [  0   1   0 ...   0   0  50]]\n",
      "at random state 79, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.81      0.81       111\n",
      "           1       0.74      0.63      0.68        81\n",
      "           2       0.95      0.87      0.91       119\n",
      "           3       0.82      0.82      0.82        98\n",
      "           4       0.82      0.89      0.85       105\n",
      "           5       0.76      0.67      0.71       104\n",
      "           6       0.93      0.91      0.92        78\n",
      "           7       0.81      0.81      0.81        95\n",
      "           8       0.91      0.93      0.92        90\n",
      "           9       0.75      0.77      0.76       100\n",
      "          10       0.72      0.66      0.69       101\n",
      "          11       0.83      0.91      0.87       105\n",
      "          12       0.84      0.85      0.84       102\n",
      "          13       0.64      0.67      0.66        98\n",
      "          14       0.82      0.82      0.82       103\n",
      "          15       0.90      0.88      0.89        73\n",
      "          16       0.87      0.87      0.87        83\n",
      "          17       0.83      0.75      0.78       102\n",
      "          18       0.91      0.84      0.87       117\n",
      "          19       0.80      0.80      0.80        90\n",
      "          20       0.76      0.90      0.82        88\n",
      "          21       0.68      0.65      0.66        99\n",
      "          22       0.90      0.90      0.90       106\n",
      "          23       0.81      0.75      0.78       102\n",
      "          24       0.69      0.58      0.63       112\n",
      "          25       0.89      0.91      0.90        91\n",
      "          26       0.65      0.61      0.63       103\n",
      "          27       0.79      0.79      0.79       109\n",
      "          28       0.88      0.85      0.86        98\n",
      "          29       0.92      0.91      0.92       114\n",
      "          30       0.86      0.86      0.86        98\n",
      "          31       0.65      0.71      0.68        99\n",
      "          32       0.78      0.81      0.80        97\n",
      "          33       0.77      0.81      0.79        98\n",
      "          34       0.72      0.88      0.79        94\n",
      "          35       0.85      0.91      0.88        97\n",
      "          36       0.63      0.69      0.66        96\n",
      "          37       0.76      0.74      0.75       100\n",
      "          38       0.78      0.83      0.80       115\n",
      "          39       0.80      0.79      0.79        98\n",
      "          40       0.83      0.80      0.82       106\n",
      "          41       0.71      0.86      0.78       107\n",
      "          42       0.95      0.91      0.93        97\n",
      "          43       0.85      0.65      0.73        93\n",
      "          44       0.75      0.79      0.77       109\n",
      "          45       0.92      0.94      0.93        88\n",
      "          46       0.90      0.93      0.91       107\n",
      "          47       0.71      0.80      0.75        98\n",
      "          48       0.67      0.68      0.68        98\n",
      "          49       0.69      0.70      0.70        94\n",
      "          50       0.50      0.51      0.51        96\n",
      "          51       1.00      1.00      1.00        52\n",
      "          52       0.90      0.94      0.92        85\n",
      "          53       0.68      0.52      0.59        97\n",
      "\n",
      "    accuracy                           0.79      5296\n",
      "   macro avg       0.80      0.80      0.80      5296\n",
      "weighted avg       0.80      0.79      0.79      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 80,  accuracy score is 1.0\n",
      "at random state 80, confusion matrix is [[89  0  0 ...  0  0  0]\n",
      " [ 0 73  0 ...  0  0  2]\n",
      " [ 0  0 84 ...  0  0  1]\n",
      " ...\n",
      " [ 0  0  0 ... 53  0  0]\n",
      " [ 0  0  0 ...  1 71  0]\n",
      " [ 0  1  3 ...  0  0 65]]\n",
      "at random state 80, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.80      0.78       111\n",
      "           1       0.80      0.72      0.76       101\n",
      "           2       0.89      0.92      0.91        91\n",
      "           3       0.83      0.79      0.81        91\n",
      "           4       0.88      0.91      0.89        96\n",
      "           5       0.70      0.72      0.71       111\n",
      "           6       0.98      0.96      0.97        92\n",
      "           7       0.80      0.81      0.81       101\n",
      "           8       0.97      0.96      0.96        97\n",
      "           9       0.82      0.79      0.81       106\n",
      "          10       0.71      0.67      0.69       100\n",
      "          11       0.90      0.87      0.88       112\n",
      "          12       0.86      0.88      0.87       108\n",
      "          13       0.74      0.78      0.76        97\n",
      "          14       0.78      0.79      0.78        94\n",
      "          15       0.96      0.94      0.95        84\n",
      "          16       0.91      0.94      0.92       103\n",
      "          17       0.77      0.77      0.77        94\n",
      "          18       0.83      0.89      0.86        98\n",
      "          19       0.82      0.88      0.85        99\n",
      "          20       0.86      0.87      0.86       109\n",
      "          21       0.69      0.64      0.66        95\n",
      "          22       0.87      0.91      0.89        98\n",
      "          23       0.79      0.83      0.81        90\n",
      "          24       0.67      0.63      0.65       105\n",
      "          25       0.94      0.98      0.96        86\n",
      "          26       0.65      0.66      0.65       107\n",
      "          27       0.75      0.80      0.78       101\n",
      "          28       0.82      0.84      0.83        86\n",
      "          29       0.86      0.86      0.86       115\n",
      "          30       0.86      0.83      0.84        98\n",
      "          31       0.65      0.75      0.70       102\n",
      "          32       0.75      0.73      0.74       103\n",
      "          33       0.77      0.71      0.74        92\n",
      "          34       0.73      0.76      0.75       101\n",
      "          35       0.82      0.82      0.82        87\n",
      "          36       0.82      0.68      0.74       109\n",
      "          37       0.74      0.70      0.72       115\n",
      "          38       0.77      0.83      0.80       102\n",
      "          39       0.87      0.79      0.83       101\n",
      "          40       0.79      0.89      0.84        95\n",
      "          41       0.77      0.71      0.74       111\n",
      "          42       0.88      0.86      0.87        77\n",
      "          43       0.73      0.77      0.75       109\n",
      "          44       0.81      0.77      0.79       114\n",
      "          45       0.89      0.88      0.88        82\n",
      "          46       0.86      0.88      0.87       109\n",
      "          47       0.75      0.69      0.72        87\n",
      "          48       0.66      0.73      0.70       105\n",
      "          49       0.77      0.70      0.74       101\n",
      "          50       0.48      0.45      0.47        93\n",
      "          51       0.98      1.00      0.99        53\n",
      "          52       0.91      0.91      0.91        78\n",
      "          53       0.68      0.69      0.68        94\n",
      "\n",
      "    accuracy                           0.80      5296\n",
      "   macro avg       0.80      0.80      0.80      5296\n",
      "weighted avg       0.80      0.80      0.80      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 81,  accuracy score is 1.0\n",
      "at random state 81, confusion matrix is [[ 79   0   0 ...   0   0   0]\n",
      " [  0  73   0 ...   0   0   1]\n",
      " [  0   0 100 ...   0   0   3]\n",
      " ...\n",
      " [  0   0   0 ...  48   0   0]\n",
      " [  0   0   0 ...   1  66   0]\n",
      " [  0   1   0 ...   0   0  82]]\n",
      "at random state 81, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.79      0.79       100\n",
      "           1       0.76      0.68      0.72       108\n",
      "           2       0.94      0.90      0.92       111\n",
      "           3       0.78      0.83      0.81        96\n",
      "           4       0.88      0.87      0.87        97\n",
      "           5       0.76      0.71      0.74       104\n",
      "           6       0.89      0.91      0.90        82\n",
      "           7       0.81      0.85      0.83       100\n",
      "           8       0.92      0.81      0.86        99\n",
      "           9       0.84      0.83      0.83       104\n",
      "          10       0.69      0.76      0.72       104\n",
      "          11       0.78      0.87      0.82       109\n",
      "          12       0.82      0.89      0.85        95\n",
      "          13       0.79      0.75      0.77       112\n",
      "          14       0.81      0.81      0.81        90\n",
      "          15       0.91      0.95      0.93        82\n",
      "          16       0.81      0.88      0.84       105\n",
      "          17       0.77      0.77      0.77        93\n",
      "          18       0.87      0.78      0.83       115\n",
      "          19       0.79      0.82      0.80        95\n",
      "          20       0.85      0.82      0.83        88\n",
      "          21       0.73      0.74      0.74        92\n",
      "          22       0.90      0.95      0.92        97\n",
      "          23       0.81      0.70      0.75        91\n",
      "          24       0.60      0.66      0.63       101\n",
      "          25       0.95      0.95      0.95       100\n",
      "          26       0.65      0.65      0.65        99\n",
      "          27       0.82      0.78      0.80       110\n",
      "          28       0.88      0.83      0.86       100\n",
      "          29       0.87      0.92      0.90        93\n",
      "          30       0.87      0.85      0.86       104\n",
      "          31       0.60      0.60      0.60        96\n",
      "          32       0.77      0.81      0.79        94\n",
      "          33       0.70      0.67      0.69       116\n",
      "          34       0.81      0.76      0.79       101\n",
      "          35       0.76      0.86      0.81        85\n",
      "          36       0.74      0.69      0.71       110\n",
      "          37       0.82      0.71      0.76        96\n",
      "          38       0.79      0.80      0.79       103\n",
      "          39       0.85      0.84      0.84       106\n",
      "          40       0.80      0.81      0.81       113\n",
      "          41       0.72      0.75      0.73       120\n",
      "          42       0.82      0.82      0.82        93\n",
      "          43       0.66      0.71      0.68        97\n",
      "          44       0.78      0.85      0.81        95\n",
      "          45       0.94      0.89      0.92        94\n",
      "          46       0.94      0.90      0.92        72\n",
      "          47       0.82      0.71      0.76       101\n",
      "          48       0.69      0.72      0.71        94\n",
      "          49       0.78      0.80      0.79       108\n",
      "          50       0.46      0.43      0.45        99\n",
      "          51       0.98      1.00      0.99        48\n",
      "          52       0.99      0.93      0.96        71\n",
      "          53       0.66      0.76      0.71       108\n",
      "\n",
      "    accuracy                           0.79      5296\n",
      "   macro avg       0.80      0.80      0.80      5296\n",
      "weighted avg       0.80      0.79      0.79      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 82,  accuracy score is 1.0\n",
      "at random state 82, confusion matrix is [[79  0  0 ...  0  0  0]\n",
      " [ 0 73  0 ...  0  0  1]\n",
      " [ 0  0 81 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 45  0  0]\n",
      " [ 0  0  0 ...  0 74  0]\n",
      " [ 0  1  2 ...  0  0 69]]\n",
      "at random state 82, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.78      0.76       101\n",
      "           1       0.65      0.67      0.66       109\n",
      "           2       0.82      0.87      0.84        93\n",
      "           3       0.84      0.80      0.82        88\n",
      "           4       0.84      0.89      0.86       100\n",
      "           5       0.68      0.73      0.71        94\n",
      "           6       0.95      0.95      0.95       100\n",
      "           7       0.85      0.79      0.82       109\n",
      "           8       0.95      0.96      0.95        98\n",
      "           9       0.86      0.79      0.82       104\n",
      "          10       0.71      0.69      0.70       105\n",
      "          11       0.91      0.80      0.85       117\n",
      "          12       0.86      0.86      0.86        97\n",
      "          13       0.77      0.74      0.76       109\n",
      "          14       0.82      0.79      0.80       108\n",
      "          15       0.98      0.95      0.96        84\n",
      "          16       0.83      0.88      0.86       101\n",
      "          17       0.83      0.91      0.87        91\n",
      "          18       0.78      0.87      0.82        94\n",
      "          19       0.75      0.74      0.75        98\n",
      "          20       0.86      0.87      0.87       102\n",
      "          21       0.69      0.64      0.66       109\n",
      "          22       0.94      0.95      0.95        87\n",
      "          23       0.76      0.81      0.79        91\n",
      "          24       0.56      0.58      0.57        85\n",
      "          25       0.90      0.95      0.92        96\n",
      "          26       0.68      0.66      0.67       107\n",
      "          27       0.85      0.82      0.84        96\n",
      "          28       0.86      0.77      0.81       116\n",
      "          29       0.88      0.93      0.90        94\n",
      "          30       0.87      0.91      0.89       109\n",
      "          31       0.65      0.69      0.67        98\n",
      "          32       0.80      0.80      0.80       112\n",
      "          33       0.75      0.80      0.78       106\n",
      "          34       0.80      0.87      0.83        91\n",
      "          35       0.89      0.94      0.92        98\n",
      "          36       0.71      0.67      0.69       106\n",
      "          37       0.70      0.77      0.74       101\n",
      "          38       0.81      0.76      0.78        99\n",
      "          39       0.82      0.75      0.78        91\n",
      "          40       0.82      0.75      0.78       100\n",
      "          41       0.71      0.68      0.69       103\n",
      "          42       0.88      0.88      0.88        95\n",
      "          43       0.78      0.71      0.74       108\n",
      "          44       0.75      0.82      0.79       101\n",
      "          45       0.94      0.94      0.94        90\n",
      "          46       0.94      0.92      0.93        95\n",
      "          47       0.84      0.73      0.78        99\n",
      "          48       0.73      0.74      0.74       105\n",
      "          49       0.80      0.80      0.80        99\n",
      "          50       0.45      0.45      0.45        85\n",
      "          51       1.00      1.00      1.00        45\n",
      "          52       0.95      0.93      0.94        80\n",
      "          53       0.66      0.71      0.68        97\n",
      "\n",
      "    accuracy                           0.80      5296\n",
      "   macro avg       0.80      0.81      0.80      5296\n",
      "weighted avg       0.80      0.80      0.80      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 83,  accuracy score is 1.0\n",
      "at random state 83, confusion matrix is [[ 75   0   0 ...   0   0   0]\n",
      " [  0  61   0 ...   0   0   2]\n",
      " [  0   0 100 ...   0   0   1]\n",
      " ...\n",
      " [  0   0   0 ...  55   0   0]\n",
      " [  0   0   0 ...   0  60   0]\n",
      " [  0   1   4 ...   0   0  70]]\n",
      "at random state 83, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.75      0.74       100\n",
      "           1       0.62      0.71      0.66        86\n",
      "           2       0.82      0.94      0.88       106\n",
      "           3       0.75      0.77      0.76        95\n",
      "           4       0.81      0.84      0.83       103\n",
      "           5       0.73      0.69      0.71       110\n",
      "           6       0.95      0.96      0.95        96\n",
      "           7       0.84      0.88      0.86        95\n",
      "           8       0.93      0.92      0.93        91\n",
      "           9       0.80      0.81      0.80        96\n",
      "          10       0.70      0.73      0.71       106\n",
      "          11       0.91      0.85      0.88       102\n",
      "          12       0.90      0.80      0.84        99\n",
      "          13       0.68      0.68      0.68        99\n",
      "          14       0.82      0.84      0.83        98\n",
      "          15       0.90      0.95      0.92        84\n",
      "          16       0.92      0.87      0.89        90\n",
      "          17       0.79      0.90      0.85       103\n",
      "          18       0.85      0.85      0.85        96\n",
      "          19       0.77      0.80      0.78       103\n",
      "          20       0.88      0.89      0.88        99\n",
      "          21       0.62      0.65      0.63        99\n",
      "          22       0.93      0.92      0.92       109\n",
      "          23       0.74      0.77      0.76       102\n",
      "          24       0.56      0.53      0.55       101\n",
      "          25       0.96      0.91      0.94       103\n",
      "          26       0.64      0.64      0.64       104\n",
      "          27       0.84      0.69      0.76        97\n",
      "          28       0.83      0.75      0.79       113\n",
      "          29       0.90      0.91      0.90        87\n",
      "          30       0.87      0.89      0.88        92\n",
      "          31       0.63      0.61      0.62        97\n",
      "          32       0.84      0.66      0.74       102\n",
      "          33       0.70      0.76      0.73        90\n",
      "          34       0.89      0.81      0.85       112\n",
      "          35       0.81      0.88      0.85       100\n",
      "          36       0.66      0.65      0.66       104\n",
      "          37       0.74      0.81      0.78        96\n",
      "          38       0.87      0.72      0.79       106\n",
      "          39       0.82      0.81      0.82       102\n",
      "          40       0.86      0.83      0.85       100\n",
      "          41       0.80      0.80      0.80       102\n",
      "          42       0.88      0.83      0.85       109\n",
      "          43       0.74      0.72      0.73        94\n",
      "          44       0.79      0.76      0.77       101\n",
      "          45       0.92      0.89      0.91        95\n",
      "          46       0.91      0.89      0.90       103\n",
      "          47       0.69      0.67      0.68        98\n",
      "          48       0.74      0.75      0.74       111\n",
      "          49       0.72      0.76      0.74        93\n",
      "          50       0.47      0.56      0.51        99\n",
      "          51       0.98      0.98      0.98        56\n",
      "          52       0.86      0.97      0.91        62\n",
      "          53       0.67      0.70      0.69       100\n",
      "\n",
      "    accuracy                           0.79      5296\n",
      "   macro avg       0.80      0.80      0.79      5296\n",
      "weighted avg       0.79      0.79      0.79      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 84,  accuracy score is 1.0\n",
      "at random state 84, confusion matrix is [[65  0  0 ...  0  0  0]\n",
      " [ 0 65  0 ...  0  0  2]\n",
      " [ 0  0 84 ...  0  0  1]\n",
      " ...\n",
      " [ 0  0  0 ... 52  0  0]\n",
      " [ 0  0  0 ...  0 79  0]\n",
      " [ 0  2  1 ...  0  0 70]]\n",
      "at random state 84, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.81      0.74        80\n",
      "           1       0.66      0.72      0.69        90\n",
      "           2       0.89      0.89      0.89        94\n",
      "           3       0.83      0.77      0.80       108\n",
      "           4       0.86      0.86      0.86       101\n",
      "           5       0.73      0.73      0.73        91\n",
      "           6       0.93      0.86      0.90        96\n",
      "           7       0.82      0.82      0.82       101\n",
      "           8       0.93      0.90      0.91       101\n",
      "           9       0.81      0.77      0.79       109\n",
      "          10       0.70      0.67      0.68        93\n",
      "          11       0.84      0.87      0.85        93\n",
      "          12       0.90      0.81      0.85       100\n",
      "          13       0.75      0.76      0.75        95\n",
      "          14       0.79      0.77      0.78       101\n",
      "          15       0.98      0.93      0.95        85\n",
      "          16       0.91      0.90      0.91       112\n",
      "          17       0.76      0.79      0.78       103\n",
      "          18       0.84      0.79      0.81        96\n",
      "          19       0.80      0.79      0.80       111\n",
      "          20       0.91      0.84      0.88       102\n",
      "          21       0.66      0.71      0.68        89\n",
      "          22       0.90      0.95      0.92       111\n",
      "          23       0.75      0.76      0.76        96\n",
      "          24       0.56      0.61      0.58        98\n",
      "          25       0.92      0.96      0.94       102\n",
      "          26       0.67      0.67      0.67       100\n",
      "          27       0.77      0.74      0.75       103\n",
      "          28       0.89      0.81      0.85       103\n",
      "          29       0.90      0.92      0.91       103\n",
      "          30       0.82      0.79      0.81       112\n",
      "          31       0.58      0.58      0.58        81\n",
      "          32       0.86      0.78      0.82       100\n",
      "          33       0.78      0.79      0.78       108\n",
      "          34       0.77      0.77      0.77        98\n",
      "          35       0.85      0.88      0.86       101\n",
      "          36       0.68      0.73      0.71        89\n",
      "          37       0.77      0.81      0.79       100\n",
      "          38       0.81      0.82      0.82       100\n",
      "          39       0.81      0.84      0.83       114\n",
      "          40       0.83      0.79      0.81       102\n",
      "          41       0.77      0.77      0.77       100\n",
      "          42       0.89      0.90      0.89       106\n",
      "          43       0.74      0.68      0.71       101\n",
      "          44       0.83      0.83      0.83       115\n",
      "          45       0.92      0.93      0.93        88\n",
      "          46       0.83      0.94      0.88        79\n",
      "          47       0.74      0.68      0.71        94\n",
      "          48       0.70      0.73      0.72       101\n",
      "          49       0.77      0.78      0.78       106\n",
      "          50       0.51      0.52      0.52        96\n",
      "          51       1.00      1.00      1.00        52\n",
      "          52       0.89      0.89      0.89        89\n",
      "          53       0.74      0.72      0.73        97\n",
      "\n",
      "    accuracy                           0.80      5296\n",
      "   macro avg       0.80      0.80      0.80      5296\n",
      "weighted avg       0.80      0.80      0.80      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 85,  accuracy score is 1.0\n",
      "at random state 85, confusion matrix is [[94  0  0 ...  0  0  0]\n",
      " [ 0 72  0 ...  0  0  0]\n",
      " [ 0  0 95 ...  0  0  3]\n",
      " ...\n",
      " [ 0  0  0 ... 52  0  0]\n",
      " [ 0  0  0 ...  0 65  0]\n",
      " [ 0  2  4 ...  0  0 72]]\n",
      "at random state 85, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.82      0.80       114\n",
      "           1       0.71      0.76      0.73        95\n",
      "           2       0.84      0.91      0.88       104\n",
      "           3       0.78      0.83      0.80        96\n",
      "           4       0.80      0.82      0.81        99\n",
      "           5       0.77      0.62      0.68       104\n",
      "           6       0.95      0.98      0.96        90\n",
      "           7       0.82      0.82      0.82        97\n",
      "           8       0.97      0.93      0.95       108\n",
      "           9       0.86      0.81      0.83       104\n",
      "          10       0.69      0.63      0.66        95\n",
      "          11       0.84      0.86      0.85        99\n",
      "          12       0.90      0.86      0.88       109\n",
      "          13       0.75      0.75      0.75       102\n",
      "          14       0.86      0.79      0.82       107\n",
      "          15       0.88      0.95      0.91        76\n",
      "          16       0.86      0.88      0.87        93\n",
      "          17       0.81      0.79      0.80       103\n",
      "          18       0.85      0.86      0.86       110\n",
      "          19       0.77      0.70      0.73        88\n",
      "          20       0.86      0.83      0.85       108\n",
      "          21       0.64      0.68      0.66        91\n",
      "          22       0.91      0.90      0.90        88\n",
      "          23       0.72      0.67      0.69        98\n",
      "          24       0.65      0.73      0.69        91\n",
      "          25       0.95      0.89      0.92       106\n",
      "          26       0.58      0.72      0.64       103\n",
      "          27       0.74      0.79      0.77       102\n",
      "          28       0.85      0.88      0.87       103\n",
      "          29       0.91      0.90      0.91        94\n",
      "          30       0.86      0.88      0.87       104\n",
      "          31       0.59      0.59      0.59       101\n",
      "          32       0.73      0.79      0.76        95\n",
      "          33       0.73      0.79      0.76        87\n",
      "          34       0.77      0.76      0.77       102\n",
      "          35       0.85      0.79      0.82       118\n",
      "          36       0.71      0.64      0.68       107\n",
      "          37       0.82      0.77      0.79        98\n",
      "          38       0.80      0.82      0.81       109\n",
      "          39       0.78      0.84      0.80        91\n",
      "          40       0.82      0.84      0.83       100\n",
      "          41       0.79      0.75      0.77        99\n",
      "          42       0.91      0.82      0.86        95\n",
      "          43       0.67      0.73      0.70        94\n",
      "          44       0.74      0.78      0.76        93\n",
      "          45       0.93      0.94      0.94        86\n",
      "          46       0.86      0.92      0.89        90\n",
      "          47       0.75      0.73      0.74       113\n",
      "          48       0.71      0.59      0.65       101\n",
      "          49       0.76      0.83      0.79        86\n",
      "          50       0.50      0.49      0.49       113\n",
      "          51       1.00      1.00      1.00        52\n",
      "          52       0.90      0.90      0.90        72\n",
      "          53       0.70      0.64      0.67       113\n",
      "\n",
      "    accuracy                           0.79      5296\n",
      "   macro avg       0.80      0.80      0.80      5296\n",
      "weighted avg       0.79      0.79      0.79      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 86,  accuracy score is 1.0\n",
      "at random state 86, confusion matrix is [[86  0  0 ...  0  0  0]\n",
      " [ 0 70  0 ...  0  0  2]\n",
      " [ 0  0 80 ...  0  0  2]\n",
      " ...\n",
      " [ 0  0  0 ... 53  0  0]\n",
      " [ 0  0  0 ...  0 61  0]\n",
      " [ 0  1  1 ...  0  0 63]]\n",
      "at random state 86, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.80      0.83       107\n",
      "           1       0.72      0.71      0.71        99\n",
      "           2       0.88      0.87      0.87        92\n",
      "           3       0.87      0.81      0.84       104\n",
      "           4       0.91      0.85      0.88       105\n",
      "           5       0.65      0.72      0.68        99\n",
      "           6       0.95      0.96      0.95        77\n",
      "           7       0.84      0.84      0.84       109\n",
      "           8       0.91      0.95      0.93       101\n",
      "           9       0.74      0.80      0.77        90\n",
      "          10       0.67      0.68      0.68       111\n",
      "          11       0.82      0.87      0.84       100\n",
      "          12       0.84      0.83      0.83        93\n",
      "          13       0.79      0.76      0.78       101\n",
      "          14       0.77      0.81      0.79        91\n",
      "          15       0.95      0.94      0.94        79\n",
      "          16       0.92      0.89      0.91       103\n",
      "          17       0.84      0.84      0.84        93\n",
      "          18       0.88      0.81      0.84        99\n",
      "          19       0.80      0.72      0.76       103\n",
      "          20       0.81      0.90      0.85        97\n",
      "          21       0.69      0.64      0.67       103\n",
      "          22       0.91      0.95      0.93       111\n",
      "          23       0.75      0.72      0.74        98\n",
      "          24       0.55      0.55      0.55        96\n",
      "          25       0.92      0.93      0.93        91\n",
      "          26       0.72      0.64      0.68       123\n",
      "          27       0.77      0.70      0.73       107\n",
      "          28       0.84      0.76      0.80       116\n",
      "          29       0.87      0.82      0.84        87\n",
      "          30       0.90      0.88      0.89        98\n",
      "          31       0.59      0.58      0.58        95\n",
      "          32       0.75      0.82      0.79        97\n",
      "          33       0.75      0.76      0.76       104\n",
      "          34       0.78      0.76      0.77       108\n",
      "          35       0.76      0.84      0.80        96\n",
      "          36       0.71      0.71      0.71        91\n",
      "          37       0.73      0.67      0.70       106\n",
      "          38       0.74      0.82      0.78        97\n",
      "          39       0.82      0.84      0.83        95\n",
      "          40       0.82      0.82      0.82        89\n",
      "          41       0.81      0.83      0.82       120\n",
      "          42       0.88      0.84      0.86        93\n",
      "          43       0.67      0.70      0.68        92\n",
      "          44       0.79      0.81      0.80       109\n",
      "          45       0.93      0.90      0.91       113\n",
      "          46       0.90      0.91      0.90        99\n",
      "          47       0.76      0.80      0.78       104\n",
      "          48       0.70      0.70      0.70        99\n",
      "          49       0.74      0.86      0.80        94\n",
      "          50       0.40      0.38      0.39       103\n",
      "          51       1.00      0.98      0.99        54\n",
      "          52       0.85      0.92      0.88        66\n",
      "          53       0.68      0.71      0.70        89\n",
      "\n",
      "    accuracy                           0.79      5296\n",
      "   macro avg       0.79      0.79      0.79      5296\n",
      "weighted avg       0.79      0.79      0.79      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 87,  accuracy score is 1.0\n",
      "at random state 87, confusion matrix is [[76  0  0 ...  0  0  0]\n",
      " [ 0 83  0 ...  0  0  2]\n",
      " [ 0  0 99 ...  0  0  2]\n",
      " ...\n",
      " [ 0  0  0 ... 48  0  0]\n",
      " [ 0  0  0 ...  2 62  0]\n",
      " [ 0  1  1 ...  0  0 51]]\n",
      "at random state 87, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.77      0.79        99\n",
      "           1       0.78      0.79      0.79       105\n",
      "           2       0.89      0.89      0.89       111\n",
      "           3       0.79      0.75      0.77       102\n",
      "           4       0.90      0.86      0.88       111\n",
      "           5       0.73      0.67      0.70       117\n",
      "           6       0.97      0.96      0.96        91\n",
      "           7       0.80      0.73      0.77       105\n",
      "           8       0.89      0.89      0.89        88\n",
      "           9       0.83      0.75      0.79       113\n",
      "          10       0.66      0.69      0.68       107\n",
      "          11       0.77      0.78      0.77       107\n",
      "          12       0.82      0.87      0.84        93\n",
      "          13       0.71      0.74      0.73        98\n",
      "          14       0.80      0.71      0.75        99\n",
      "          15       0.99      0.93      0.96        83\n",
      "          16       0.85      0.89      0.87        91\n",
      "          17       0.80      0.86      0.83       105\n",
      "          18       0.76      0.76      0.76       101\n",
      "          19       0.76      0.79      0.78       102\n",
      "          20       0.88      0.91      0.89       102\n",
      "          21       0.66      0.60      0.63       103\n",
      "          22       0.91      0.95      0.93        83\n",
      "          23       0.75      0.74      0.74        92\n",
      "          24       0.54      0.61      0.57        92\n",
      "          25       0.92      0.94      0.93       101\n",
      "          26       0.71      0.74      0.72       102\n",
      "          27       0.72      0.79      0.75       101\n",
      "          28       0.81      0.85      0.83        97\n",
      "          29       0.88      0.88      0.88        92\n",
      "          30       0.82      0.88      0.85       101\n",
      "          31       0.65      0.64      0.65        95\n",
      "          32       0.79      0.72      0.76       112\n",
      "          33       0.73      0.69      0.71        99\n",
      "          34       0.81      0.72      0.76        99\n",
      "          35       0.81      0.84      0.82        99\n",
      "          36       0.75      0.75      0.75       102\n",
      "          37       0.65      0.72      0.69        83\n",
      "          38       0.76      0.80      0.78        92\n",
      "          39       0.87      0.84      0.86       106\n",
      "          40       0.85      0.84      0.84       110\n",
      "          41       0.79      0.71      0.75       108\n",
      "          42       0.89      0.86      0.88        88\n",
      "          43       0.69      0.71      0.70        92\n",
      "          44       0.81      0.79      0.80       102\n",
      "          45       0.95      0.92      0.93        95\n",
      "          46       0.89      0.92      0.90       101\n",
      "          47       0.68      0.74      0.71        98\n",
      "          48       0.73      0.74      0.74       101\n",
      "          49       0.75      0.76      0.75       116\n",
      "          50       0.55      0.51      0.53       111\n",
      "          51       0.92      1.00      0.96        48\n",
      "          52       0.94      0.91      0.93        68\n",
      "          53       0.60      0.66      0.63        77\n",
      "\n",
      "    accuracy                           0.79      5296\n",
      "   macro avg       0.79      0.79      0.79      5296\n",
      "weighted avg       0.79      0.79      0.79      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 88,  accuracy score is 1.0\n",
      "at random state 88, confusion matrix is [[74  0  0 ...  0  0  0]\n",
      " [ 0 81  0 ...  0  0  1]\n",
      " [ 0  0 77 ...  0  0  2]\n",
      " ...\n",
      " [ 0  0  0 ... 64  0  0]\n",
      " [ 0  0  0 ...  1 68  0]\n",
      " [ 0  3  4 ...  0  0 72]]\n",
      "at random state 88, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.76      0.77        98\n",
      "           1       0.75      0.69      0.72       118\n",
      "           2       0.83      0.92      0.87        84\n",
      "           3       0.73      0.87      0.79        91\n",
      "           4       0.86      0.81      0.83       110\n",
      "           5       0.70      0.73      0.71        91\n",
      "           6       0.93      0.95      0.94        73\n",
      "           7       0.79      0.78      0.78        85\n",
      "           8       0.93      0.85      0.89        99\n",
      "           9       0.80      0.87      0.83       113\n",
      "          10       0.73      0.67      0.70       105\n",
      "          11       0.91      0.83      0.87       103\n",
      "          12       0.84      0.89      0.87        98\n",
      "          13       0.70      0.76      0.73        90\n",
      "          14       0.85      0.80      0.82       103\n",
      "          15       0.88      0.95      0.92        87\n",
      "          16       0.86      0.82      0.84        85\n",
      "          17       0.80      0.82      0.81       107\n",
      "          18       0.84      0.87      0.85        91\n",
      "          19       0.87      0.76      0.81       106\n",
      "          20       0.85      0.87      0.86        89\n",
      "          21       0.71      0.65      0.68       124\n",
      "          22       0.94      0.88      0.91       116\n",
      "          23       0.74      0.72      0.73       110\n",
      "          24       0.66      0.62      0.64        96\n",
      "          25       0.88      0.89      0.88        80\n",
      "          26       0.61      0.68      0.64       101\n",
      "          27       0.76      0.77      0.76        99\n",
      "          28       0.80      0.79      0.79        99\n",
      "          29       0.94      0.90      0.92        97\n",
      "          30       0.86      0.86      0.86       106\n",
      "          31       0.63      0.70      0.66       102\n",
      "          32       0.72      0.81      0.76       108\n",
      "          33       0.67      0.84      0.74        87\n",
      "          34       0.82      0.74      0.78       103\n",
      "          35       0.84      0.91      0.87        97\n",
      "          36       0.61      0.76      0.68        86\n",
      "          37       0.67      0.76      0.71        87\n",
      "          38       0.74      0.75      0.74       106\n",
      "          39       0.74      0.84      0.79        97\n",
      "          40       0.82      0.86      0.84        91\n",
      "          41       0.82      0.70      0.76       108\n",
      "          42       0.87      0.91      0.89        91\n",
      "          43       0.79      0.62      0.70       117\n",
      "          44       0.79      0.73      0.76       124\n",
      "          45       0.96      0.95      0.96        82\n",
      "          46       0.90      0.97      0.93        93\n",
      "          47       0.76      0.73      0.74        99\n",
      "          48       0.72      0.65      0.68       105\n",
      "          49       0.85      0.81      0.83       116\n",
      "          50       0.54      0.57      0.55        97\n",
      "          51       0.98      1.00      0.99        64\n",
      "          52       0.93      0.89      0.91        76\n",
      "          53       0.77      0.68      0.72       106\n",
      "\n",
      "    accuracy                           0.79      5296\n",
      "   macro avg       0.80      0.80      0.80      5296\n",
      "weighted avg       0.80      0.79      0.79      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 89,  accuracy score is 1.0\n",
      "at random state 89, confusion matrix is [[78  0  0 ...  0  0  0]\n",
      " [ 0 74  0 ...  0  0  1]\n",
      " [ 0  0 96 ...  0  0  1]\n",
      " ...\n",
      " [ 0  0  0 ... 55  0  0]\n",
      " [ 0  0  0 ...  1 71  0]\n",
      " [ 0  2  1 ...  0  0 78]]\n",
      "at random state 89, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.70      0.75       112\n",
      "           1       0.70      0.70      0.70       105\n",
      "           2       0.86      0.85      0.86       113\n",
      "           3       0.77      0.82      0.79        99\n",
      "           4       0.86      0.76      0.80        95\n",
      "           5       0.76      0.74      0.75       105\n",
      "           6       0.92      0.99      0.95        93\n",
      "           7       0.81      0.81      0.81        94\n",
      "           8       0.92      0.92      0.92       108\n",
      "           9       0.81      0.82      0.81        83\n",
      "          10       0.74      0.68      0.71        94\n",
      "          11       0.92      0.87      0.89       104\n",
      "          12       0.80      0.86      0.83        95\n",
      "          13       0.75      0.80      0.77        99\n",
      "          14       0.77      0.73      0.75        94\n",
      "          15       0.94      0.88      0.91        75\n",
      "          16       0.82      0.89      0.85       108\n",
      "          17       0.77      0.75      0.76        97\n",
      "          18       0.85      0.89      0.87       106\n",
      "          19       0.84      0.76      0.80        92\n",
      "          20       0.86      0.83      0.85       102\n",
      "          21       0.62      0.71      0.66        85\n",
      "          22       0.89      0.94      0.92       105\n",
      "          23       0.76      0.75      0.75       111\n",
      "          24       0.62      0.58      0.60        97\n",
      "          25       0.97      0.88      0.92       100\n",
      "          26       0.61      0.60      0.61        93\n",
      "          27       0.69      0.60      0.64       100\n",
      "          28       0.91      0.88      0.89       111\n",
      "          29       0.80      0.90      0.85        91\n",
      "          30       0.87      0.88      0.88       108\n",
      "          31       0.62      0.63      0.62       102\n",
      "          32       0.75      0.79      0.77        98\n",
      "          33       0.72      0.77      0.75       102\n",
      "          34       0.75      0.78      0.77       103\n",
      "          35       0.87      0.86      0.86       111\n",
      "          36       0.74      0.61      0.67       111\n",
      "          37       0.73      0.78      0.76        91\n",
      "          38       0.80      0.81      0.80        99\n",
      "          39       0.83      0.79      0.81        94\n",
      "          40       0.88      0.81      0.84       121\n",
      "          41       0.68      0.75      0.71        92\n",
      "          42       0.83      0.79      0.81        82\n",
      "          43       0.71      0.69      0.70       107\n",
      "          44       0.80      0.82      0.81        95\n",
      "          45       0.86      0.95      0.91        86\n",
      "          46       0.90      0.86      0.88        88\n",
      "          47       0.69      0.73      0.71        84\n",
      "          48       0.65      0.63      0.64       115\n",
      "          49       0.72      0.78      0.75       104\n",
      "          50       0.44      0.52      0.48        92\n",
      "          51       0.95      1.00      0.97        55\n",
      "          52       0.92      0.85      0.88        84\n",
      "          53       0.67      0.74      0.70       106\n",
      "\n",
      "    accuracy                           0.78      5296\n",
      "   macro avg       0.79      0.79      0.79      5296\n",
      "weighted avg       0.79      0.78      0.78      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 90,  accuracy score is 1.0\n",
      "at random state 90, confusion matrix is [[87  0  0 ...  0  0  0]\n",
      " [ 0 73  0 ...  0  0  4]\n",
      " [ 0  0 82 ...  0  0  1]\n",
      " ...\n",
      " [ 0  0  0 ... 51  0  0]\n",
      " [ 0  0  0 ...  0 66  0]\n",
      " [ 0  0  1 ...  0  0 59]]\n",
      "at random state 90, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.84      0.82       104\n",
      "           1       0.70      0.68      0.69       108\n",
      "           2       0.89      0.92      0.91        89\n",
      "           3       0.80      0.78      0.79       115\n",
      "           4       0.91      0.82      0.87       102\n",
      "           5       0.69      0.60      0.64       109\n",
      "           6       0.90      0.96      0.93        89\n",
      "           7       0.80      0.85      0.82        93\n",
      "           8       0.94      0.94      0.94       109\n",
      "           9       0.79      0.80      0.80        92\n",
      "          10       0.67      0.69      0.68        95\n",
      "          11       0.86      0.88      0.87       101\n",
      "          12       0.89      0.85      0.87       110\n",
      "          13       0.75      0.72      0.73       106\n",
      "          14       0.84      0.82      0.83        93\n",
      "          15       0.94      0.97      0.95        90\n",
      "          16       0.78      0.92      0.85        87\n",
      "          17       0.79      0.73      0.76        81\n",
      "          18       0.87      0.82      0.84       105\n",
      "          19       0.85      0.85      0.85       108\n",
      "          20       0.82      0.82      0.82       101\n",
      "          21       0.70      0.64      0.67       108\n",
      "          22       0.93      0.94      0.93       110\n",
      "          23       0.74      0.81      0.77       111\n",
      "          24       0.50      0.53      0.52        96\n",
      "          25       0.96      0.93      0.95       104\n",
      "          26       0.59      0.56      0.57        88\n",
      "          27       0.77      0.83      0.80        86\n",
      "          28       0.88      0.91      0.90       100\n",
      "          29       0.96      0.86      0.91       108\n",
      "          30       0.86      0.92      0.89        98\n",
      "          31       0.57      0.62      0.60       108\n",
      "          32       0.81      0.85      0.83       100\n",
      "          33       0.74      0.70      0.72        93\n",
      "          34       0.83      0.80      0.81       103\n",
      "          35       0.89      0.89      0.89       119\n",
      "          36       0.64      0.59      0.62        95\n",
      "          37       0.73      0.70      0.72        91\n",
      "          38       0.77      0.82      0.80       102\n",
      "          39       0.78      0.83      0.81        96\n",
      "          40       0.82      0.76      0.79       117\n",
      "          41       0.73      0.79      0.76       104\n",
      "          42       0.89      0.89      0.89        87\n",
      "          43       0.72      0.75      0.73        96\n",
      "          44       0.86      0.84      0.85       102\n",
      "          45       0.93      0.92      0.93        75\n",
      "          46       0.83      0.88      0.85        86\n",
      "          47       0.85      0.70      0.77       104\n",
      "          48       0.73      0.66      0.70        98\n",
      "          49       0.70      0.67      0.69       106\n",
      "          50       0.47      0.46      0.46       116\n",
      "          51       0.98      1.00      0.99        51\n",
      "          52       0.89      0.90      0.90        73\n",
      "          53       0.59      0.76      0.66        78\n",
      "\n",
      "    accuracy                           0.79      5296\n",
      "   macro avg       0.79      0.80      0.79      5296\n",
      "weighted avg       0.79      0.79      0.79      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 91,  accuracy score is 1.0\n",
      "at random state 91, confusion matrix is [[81  0  0 ...  0  0  0]\n",
      " [ 0 67  0 ...  0  0  2]\n",
      " [ 0  0 92 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 55  0  0]\n",
      " [ 0  0  0 ...  0 71  0]\n",
      " [ 0  0  2 ...  0  0 64]]\n",
      "at random state 91, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.77      0.78       105\n",
      "           1       0.69      0.62      0.65       108\n",
      "           2       0.82      0.95      0.88        97\n",
      "           3       0.85      0.83      0.84       115\n",
      "           4       0.85      0.87      0.86       104\n",
      "           5       0.67      0.67      0.67       102\n",
      "           6       0.97      0.96      0.96        90\n",
      "           7       0.84      0.91      0.88        93\n",
      "           8       0.98      0.94      0.96        89\n",
      "           9       0.81      0.81      0.81        98\n",
      "          10       0.67      0.69      0.68        98\n",
      "          11       0.87      0.86      0.87       105\n",
      "          12       0.81      0.83      0.82       109\n",
      "          13       0.76      0.72      0.74       105\n",
      "          14       0.76      0.72      0.74        94\n",
      "          15       0.96      0.97      0.96        90\n",
      "          16       0.86      0.90      0.88        96\n",
      "          17       0.88      0.74      0.80       114\n",
      "          18       0.85      0.81      0.83        95\n",
      "          19       0.82      0.78      0.80       105\n",
      "          20       0.84      0.81      0.83        97\n",
      "          21       0.64      0.73      0.68        95\n",
      "          22       0.88      0.96      0.92        94\n",
      "          23       0.72      0.77      0.74        95\n",
      "          24       0.59      0.59      0.59       102\n",
      "          25       0.92      0.88      0.90       105\n",
      "          26       0.61      0.65      0.63        94\n",
      "          27       0.78      0.76      0.77        83\n",
      "          28       0.90      0.85      0.88       110\n",
      "          29       0.92      0.92      0.92       106\n",
      "          30       0.89      0.87      0.88       101\n",
      "          31       0.59      0.62      0.61        96\n",
      "          32       0.77      0.70      0.73        98\n",
      "          33       0.72      0.79      0.75       103\n",
      "          34       0.79      0.87      0.83       100\n",
      "          35       0.80      0.83      0.81        94\n",
      "          36       0.71      0.70      0.70       102\n",
      "          37       0.80      0.84      0.82        91\n",
      "          38       0.74      0.85      0.79        95\n",
      "          39       0.80      0.79      0.79       100\n",
      "          40       0.89      0.83      0.86       102\n",
      "          41       0.68      0.64      0.66        84\n",
      "          42       0.87      0.84      0.85       104\n",
      "          43       0.68      0.69      0.68       103\n",
      "          44       0.83      0.83      0.83       108\n",
      "          45       0.96      0.92      0.94        88\n",
      "          46       0.91      0.88      0.90        98\n",
      "          47       0.75      0.79      0.77        98\n",
      "          48       0.78      0.67      0.72        96\n",
      "          49       0.77      0.76      0.77       110\n",
      "          50       0.53      0.55      0.54       105\n",
      "          51       0.98      1.00      0.99        55\n",
      "          52       0.93      0.93      0.93        76\n",
      "          53       0.70      0.67      0.68        96\n",
      "\n",
      "    accuracy                           0.80      5296\n",
      "   macro avg       0.80      0.80      0.80      5296\n",
      "weighted avg       0.80      0.80      0.80      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 92,  accuracy score is 1.0\n",
      "at random state 92, confusion matrix is [[76  0  0 ...  0  0  0]\n",
      " [ 0 72  0 ...  0  0  2]\n",
      " [ 0  0 90 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 50  0  0]\n",
      " [ 0  0  0 ...  0 75  0]\n",
      " [ 0  1  2 ...  0  0 75]]\n",
      "at random state 92, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.78      0.76        98\n",
      "           1       0.69      0.77      0.73        93\n",
      "           2       0.91      0.91      0.91        99\n",
      "           3       0.83      0.84      0.83       107\n",
      "           4       0.90      0.87      0.89       103\n",
      "           5       0.74      0.68      0.71        98\n",
      "           6       0.97      0.95      0.96        79\n",
      "           7       0.76      0.84      0.80        88\n",
      "           8       0.92      0.91      0.91       109\n",
      "           9       0.72      0.76      0.74       104\n",
      "          10       0.71      0.73      0.72        97\n",
      "          11       0.89      0.90      0.89       107\n",
      "          12       0.93      0.87      0.90       108\n",
      "          13       0.75      0.79      0.77       100\n",
      "          14       0.87      0.79      0.83       103\n",
      "          15       0.94      0.94      0.94        86\n",
      "          16       0.87      0.87      0.87       101\n",
      "          17       0.91      0.80      0.85        99\n",
      "          18       0.85      0.86      0.86       101\n",
      "          19       0.87      0.85      0.86       105\n",
      "          20       0.86      0.83      0.84        95\n",
      "          21       0.66      0.57      0.61       103\n",
      "          22       0.95      0.94      0.95       124\n",
      "          23       0.81      0.81      0.81       108\n",
      "          24       0.55      0.47      0.50       103\n",
      "          25       0.93      0.92      0.93        93\n",
      "          26       0.69      0.60      0.64       100\n",
      "          27       0.80      0.77      0.78        99\n",
      "          28       0.82      0.82      0.82        85\n",
      "          29       0.92      0.91      0.92       114\n",
      "          30       0.85      0.87      0.86        83\n",
      "          31       0.67      0.62      0.64       104\n",
      "          32       0.70      0.80      0.74        91\n",
      "          33       0.74      0.77      0.75        98\n",
      "          34       0.75      0.85      0.80       107\n",
      "          35       0.84      0.85      0.84       102\n",
      "          36       0.77      0.71      0.74        95\n",
      "          37       0.74      0.69      0.72        98\n",
      "          38       0.79      0.81      0.80        96\n",
      "          39       0.74      0.85      0.79       102\n",
      "          40       0.87      0.81      0.84       104\n",
      "          41       0.72      0.76      0.74        89\n",
      "          42       0.89      0.83      0.86        93\n",
      "          43       0.69      0.80      0.74        95\n",
      "          44       0.81      0.81      0.81       101\n",
      "          45       0.90      0.98      0.94        97\n",
      "          46       0.91      0.92      0.91        95\n",
      "          47       0.83      0.75      0.78        95\n",
      "          48       0.68      0.63      0.66       100\n",
      "          49       0.80      0.86      0.83        98\n",
      "          50       0.43      0.56      0.48        99\n",
      "          51       1.00      1.00      1.00        50\n",
      "          52       0.97      0.91      0.94        82\n",
      "          53       0.73      0.66      0.69       113\n",
      "\n",
      "    accuracy                           0.80      5296\n",
      "   macro avg       0.81      0.80      0.80      5296\n",
      "weighted avg       0.80      0.80      0.80      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 93,  accuracy score is 1.0\n",
      "at random state 93, confusion matrix is [[68  0  0 ...  0  0  0]\n",
      " [ 0 73  0 ...  0  0  2]\n",
      " [ 0  0 85 ...  0  0  2]\n",
      " ...\n",
      " [ 0  0  0 ... 44  0  0]\n",
      " [ 0  0  0 ...  1 65  0]\n",
      " [ 0  2  3 ...  0  0 67]]\n",
      "at random state 93, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.74      0.76        92\n",
      "           1       0.73      0.70      0.71       105\n",
      "           2       0.83      0.89      0.85        96\n",
      "           3       0.84      0.79      0.81        99\n",
      "           4       0.89      0.92      0.91        99\n",
      "           5       0.68      0.72      0.70        97\n",
      "           6       0.93      0.93      0.93        89\n",
      "           7       0.82      0.81      0.81       109\n",
      "           8       0.91      0.88      0.89       101\n",
      "           9       0.78      0.82      0.80        87\n",
      "          10       0.64      0.68      0.66        93\n",
      "          11       0.82      0.82      0.82       114\n",
      "          12       0.83      0.79      0.81       106\n",
      "          13       0.74      0.73      0.73        96\n",
      "          14       0.90      0.75      0.82       106\n",
      "          15       0.96      0.98      0.97        88\n",
      "          16       0.90      0.86      0.88       105\n",
      "          17       0.84      0.88      0.86       104\n",
      "          18       0.74      0.79      0.76       102\n",
      "          19       0.78      0.77      0.78       104\n",
      "          20       0.87      0.82      0.84        95\n",
      "          21       0.67      0.67      0.67        96\n",
      "          22       0.92      0.92      0.92       103\n",
      "          23       0.76      0.81      0.78       101\n",
      "          24       0.62      0.56      0.59        93\n",
      "          25       0.95      0.90      0.93       117\n",
      "          26       0.69      0.70      0.69       112\n",
      "          27       0.84      0.80      0.82       118\n",
      "          28       0.86      0.80      0.83        88\n",
      "          29       0.85      0.93      0.89       104\n",
      "          30       0.90      0.90      0.90        89\n",
      "          31       0.59      0.66      0.62        99\n",
      "          32       0.79      0.80      0.80        86\n",
      "          33       0.73      0.74      0.73        99\n",
      "          34       0.83      0.78      0.80       104\n",
      "          35       0.87      0.85      0.86        89\n",
      "          36       0.68      0.71      0.70       105\n",
      "          37       0.73      0.71      0.72        96\n",
      "          38       0.82      0.84      0.83        95\n",
      "          39       0.76      0.92      0.83        90\n",
      "          40       0.77      0.82      0.79       108\n",
      "          41       0.71      0.68      0.69        98\n",
      "          42       0.85      0.87      0.86        93\n",
      "          43       0.74      0.70      0.72       101\n",
      "          44       0.82      0.85      0.84       117\n",
      "          45       0.90      0.97      0.93        86\n",
      "          46       0.94      0.88      0.91        88\n",
      "          47       0.75      0.79      0.77        91\n",
      "          48       0.73      0.70      0.72       108\n",
      "          49       0.86      0.77      0.81       114\n",
      "          50       0.49      0.52      0.51       107\n",
      "          51       0.98      1.00      0.99        44\n",
      "          52       0.89      0.88      0.88        74\n",
      "          53       0.74      0.70      0.72        96\n",
      "\n",
      "    accuracy                           0.80      5296\n",
      "   macro avg       0.80      0.80      0.80      5296\n",
      "weighted avg       0.80      0.80      0.80      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 94,  accuracy score is 1.0\n",
      "at random state 94, confusion matrix is [[82  0  0 ...  0  0  0]\n",
      " [ 0 63  0 ...  0  0  0]\n",
      " [ 0  0 93 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 46  0  0]\n",
      " [ 0  0  0 ...  1 54  0]\n",
      " [ 0  0  3 ...  0  0 63]]\n",
      "at random state 94, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.78      0.80       105\n",
      "           1       0.67      0.70      0.68        90\n",
      "           2       0.91      0.87      0.89       107\n",
      "           3       0.84      0.81      0.82       100\n",
      "           4       0.89      0.92      0.91        99\n",
      "           5       0.70      0.73      0.72       114\n",
      "           6       0.95      0.94      0.94        98\n",
      "           7       0.77      0.83      0.80        95\n",
      "           8       0.95      0.88      0.92       117\n",
      "           9       0.76      0.85      0.80        86\n",
      "          10       0.72      0.70      0.71       103\n",
      "          11       0.80      0.88      0.84        92\n",
      "          12       0.80      0.78      0.79        98\n",
      "          13       0.70      0.75      0.73       109\n",
      "          14       0.78      0.74      0.76       114\n",
      "          15       0.96      0.94      0.95        81\n",
      "          16       0.83      0.87      0.85       103\n",
      "          17       0.84      0.77      0.80       103\n",
      "          18       0.90      0.79      0.84        89\n",
      "          19       0.77      0.78      0.77       113\n",
      "          20       0.92      0.81      0.86        96\n",
      "          21       0.70      0.52      0.60       101\n",
      "          22       0.96      0.88      0.92       113\n",
      "          23       0.69      0.84      0.76        89\n",
      "          24       0.58      0.55      0.57       103\n",
      "          25       0.94      0.94      0.94        86\n",
      "          26       0.62      0.58      0.60        99\n",
      "          27       0.80      0.75      0.78       110\n",
      "          28       0.81      0.86      0.83        99\n",
      "          29       0.89      0.90      0.89        89\n",
      "          30       0.86      0.95      0.90        94\n",
      "          31       0.59      0.73      0.65        79\n",
      "          32       0.85      0.82      0.83        95\n",
      "          33       0.75      0.81      0.78       101\n",
      "          34       0.83      0.83      0.83       111\n",
      "          35       0.83      0.88      0.86       111\n",
      "          36       0.60      0.62      0.61        88\n",
      "          37       0.81      0.75      0.78       100\n",
      "          38       0.73      0.83      0.78        98\n",
      "          39       0.84      0.93      0.88        97\n",
      "          40       0.74      0.81      0.78        96\n",
      "          41       0.72      0.72      0.72       102\n",
      "          42       0.86      0.85      0.86        96\n",
      "          43       0.80      0.77      0.78       122\n",
      "          44       0.82      0.86      0.84       114\n",
      "          45       0.88      0.97      0.92       101\n",
      "          46       0.89      0.84      0.86        85\n",
      "          47       0.77      0.70      0.73       103\n",
      "          48       0.67      0.71      0.69        90\n",
      "          49       0.81      0.81      0.81       101\n",
      "          50       0.46      0.40      0.43       100\n",
      "          51       0.98      1.00      0.99        46\n",
      "          52       0.92      0.83      0.87        65\n",
      "          53       0.74      0.63      0.68       100\n",
      "\n",
      "    accuracy                           0.79      5296\n",
      "   macro avg       0.80      0.80      0.79      5296\n",
      "weighted avg       0.79      0.79      0.79      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 95,  accuracy score is 1.0\n",
      "at random state 95, confusion matrix is [[ 86   0   0 ...   0   0   0]\n",
      " [  0  77   0 ...   0   0   2]\n",
      " [  0   0 102 ...   0   0   1]\n",
      " ...\n",
      " [  0   0   0 ...  46   0   0]\n",
      " [  0   0   0 ...   0  74   0]\n",
      " [  0   0   0 ...   0   0  67]]\n",
      "at random state 95, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.80      0.80       107\n",
      "           1       0.71      0.70      0.70       110\n",
      "           2       0.96      0.91      0.94       112\n",
      "           3       0.82      0.77      0.79        98\n",
      "           4       0.82      0.89      0.85        98\n",
      "           5       0.73      0.72      0.72       103\n",
      "           6       0.93      0.92      0.92        86\n",
      "           7       0.80      0.82      0.81        94\n",
      "           8       0.90      0.91      0.90        87\n",
      "           9       0.84      0.77      0.81       102\n",
      "          10       0.67      0.62      0.64       100\n",
      "          11       0.88      0.84      0.86       109\n",
      "          12       0.81      0.84      0.82       123\n",
      "          13       0.74      0.75      0.75        88\n",
      "          14       0.82      0.78      0.80        95\n",
      "          15       0.96      0.93      0.95        84\n",
      "          16       0.89      0.88      0.88        99\n",
      "          17       0.78      0.78      0.78       102\n",
      "          18       0.81      0.82      0.81        96\n",
      "          19       0.80      0.75      0.77       107\n",
      "          20       0.87      0.92      0.89        99\n",
      "          21       0.65      0.69      0.67       109\n",
      "          22       0.96      0.91      0.93       106\n",
      "          23       0.75      0.78      0.76       107\n",
      "          24       0.58      0.63      0.61        94\n",
      "          25       0.93      0.98      0.95        93\n",
      "          26       0.54      0.69      0.60        83\n",
      "          27       0.77      0.83      0.80       102\n",
      "          28       0.90      0.85      0.87       104\n",
      "          29       0.83      0.95      0.89        79\n",
      "          30       0.89      0.83      0.86       103\n",
      "          31       0.58      0.57      0.57        99\n",
      "          32       0.79      0.79      0.79        97\n",
      "          33       0.80      0.67      0.73        99\n",
      "          34       0.79      0.84      0.82        95\n",
      "          35       0.86      0.87      0.86        91\n",
      "          36       0.62      0.56      0.59       106\n",
      "          37       0.79      0.69      0.74       101\n",
      "          38       0.82      0.84      0.83       107\n",
      "          39       0.88      0.86      0.87       106\n",
      "          40       0.81      0.87      0.84       112\n",
      "          41       0.73      0.66      0.69        99\n",
      "          42       0.82      0.86      0.84        87\n",
      "          43       0.66      0.81      0.72        98\n",
      "          44       0.81      0.81      0.81        98\n",
      "          45       0.92      0.96      0.94        81\n",
      "          46       0.98      0.94      0.96        90\n",
      "          47       0.73      0.74      0.73        99\n",
      "          48       0.63      0.72      0.67       100\n",
      "          49       0.81      0.75      0.78       105\n",
      "          50       0.55      0.50      0.52       119\n",
      "          51       1.00      0.98      0.99        47\n",
      "          52       0.97      0.90      0.94        82\n",
      "          53       0.68      0.68      0.68        99\n",
      "\n",
      "    accuracy                           0.79      5296\n",
      "   macro avg       0.80      0.80      0.80      5296\n",
      "weighted avg       0.80      0.79      0.79      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 96,  accuracy score is 1.0\n",
      "at random state 96, confusion matrix is [[87  0  0 ...  0  0  0]\n",
      " [ 0 80  0 ...  0  0  0]\n",
      " [ 0  0 96 ...  0  0  1]\n",
      " ...\n",
      " [ 0  0  0 ... 47  0  0]\n",
      " [ 0  0  0 ...  0 72  0]\n",
      " [ 0  1  0 ...  0  0 62]]\n",
      "at random state 96, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.80      0.81       109\n",
      "           1       0.70      0.82      0.75        98\n",
      "           2       0.91      0.92      0.91       104\n",
      "           3       0.76      0.80      0.78       109\n",
      "           4       0.89      0.84      0.86       102\n",
      "           5       0.73      0.68      0.70        99\n",
      "           6       0.92      0.89      0.91        82\n",
      "           7       0.77      0.81      0.79        97\n",
      "           8       0.94      0.95      0.94        97\n",
      "           9       0.77      0.77      0.77       101\n",
      "          10       0.71      0.66      0.69       107\n",
      "          11       0.85      0.86      0.85       116\n",
      "          12       0.84      0.83      0.84       111\n",
      "          13       0.72      0.68      0.70        97\n",
      "          14       0.74      0.78      0.76        86\n",
      "          15       0.96      0.94      0.95        82\n",
      "          16       0.92      0.85      0.88       103\n",
      "          17       0.82      0.85      0.84       103\n",
      "          18       0.84      0.82      0.83       106\n",
      "          19       0.79      0.78      0.79       104\n",
      "          20       0.83      0.84      0.84        95\n",
      "          21       0.62      0.55      0.58        99\n",
      "          22       0.92      0.92      0.92        83\n",
      "          23       0.72      0.75      0.73        99\n",
      "          24       0.67      0.65      0.66        98\n",
      "          25       0.91      0.86      0.89        96\n",
      "          26       0.63      0.69      0.66        91\n",
      "          27       0.82      0.89      0.85        87\n",
      "          28       0.86      0.78      0.82       102\n",
      "          29       0.90      0.88      0.89        96\n",
      "          30       0.90      0.86      0.88       108\n",
      "          31       0.67      0.60      0.63        96\n",
      "          32       0.80      0.76      0.78       103\n",
      "          33       0.81      0.82      0.81        99\n",
      "          34       0.88      0.82      0.85        92\n",
      "          35       0.89      0.84      0.86       112\n",
      "          36       0.68      0.68      0.68        97\n",
      "          37       0.74      0.73      0.73        92\n",
      "          38       0.75      0.74      0.74        99\n",
      "          39       0.80      0.86      0.83       113\n",
      "          40       0.74      0.84      0.79       101\n",
      "          41       0.76      0.80      0.78       110\n",
      "          42       0.80      0.90      0.85        80\n",
      "          43       0.76      0.70      0.73       101\n",
      "          44       0.80      0.77      0.79       110\n",
      "          45       0.90      0.94      0.92       100\n",
      "          46       0.85      0.92      0.88        96\n",
      "          47       0.74      0.70      0.72       112\n",
      "          48       0.73      0.78      0.76       102\n",
      "          49       0.77      0.78      0.78        91\n",
      "          50       0.55      0.57      0.56       105\n",
      "          51       1.00      1.00      1.00        47\n",
      "          52       0.90      0.89      0.89        81\n",
      "          53       0.65      0.69      0.67        90\n",
      "\n",
      "    accuracy                           0.80      5296\n",
      "   macro avg       0.80      0.80      0.80      5296\n",
      "weighted avg       0.80      0.80      0.80      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 97,  accuracy score is 1.0\n",
      "at random state 97, confusion matrix is [[76  0  0 ...  0  0  0]\n",
      " [ 0 73  0 ...  0  0  2]\n",
      " [ 0  0 93 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 58  0  0]\n",
      " [ 0  0  0 ...  0 80  0]\n",
      " [ 0  0  1 ...  0  0 68]]\n",
      "at random state 97, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.78      0.76        97\n",
      "           1       0.72      0.68      0.70       107\n",
      "           2       0.95      0.93      0.94       100\n",
      "           3       0.85      0.76      0.80       107\n",
      "           4       0.85      0.87      0.86        99\n",
      "           5       0.69      0.71      0.70       111\n",
      "           6       0.92      0.94      0.93        77\n",
      "           7       0.85      0.85      0.85       110\n",
      "           8       0.97      0.95      0.96        93\n",
      "           9       0.79      0.77      0.78        82\n",
      "          10       0.77      0.70      0.73        94\n",
      "          11       0.85      0.85      0.85       118\n",
      "          12       0.84      0.76      0.80       114\n",
      "          13       0.78      0.77      0.78        99\n",
      "          14       0.83      0.74      0.78        88\n",
      "          15       0.98      0.93      0.95        97\n",
      "          16       0.83      0.89      0.86       107\n",
      "          17       0.84      0.82      0.83       111\n",
      "          18       0.84      0.84      0.84       101\n",
      "          19       0.79      0.85      0.82        94\n",
      "          20       0.85      0.87      0.86       103\n",
      "          21       0.63      0.62      0.62       103\n",
      "          22       0.92      0.94      0.93        95\n",
      "          23       0.71      0.76      0.73       103\n",
      "          24       0.58      0.65      0.61        88\n",
      "          25       0.90      0.91      0.91        91\n",
      "          26       0.61      0.66      0.64        89\n",
      "          27       0.78      0.76      0.77       109\n",
      "          28       0.84      0.78      0.81       103\n",
      "          29       0.86      0.94      0.90       104\n",
      "          30       0.90      0.87      0.88        99\n",
      "          31       0.66      0.58      0.61       106\n",
      "          32       0.76      0.70      0.73        88\n",
      "          33       0.71      0.78      0.74       115\n",
      "          34       0.79      0.84      0.81        91\n",
      "          35       0.87      0.92      0.89        96\n",
      "          36       0.69      0.81      0.75        95\n",
      "          37       0.77      0.80      0.78        95\n",
      "          38       0.80      0.82      0.81        98\n",
      "          39       0.86      0.83      0.84        88\n",
      "          40       0.85      0.80      0.83       106\n",
      "          41       0.81      0.70      0.75       112\n",
      "          42       0.85      0.86      0.85        84\n",
      "          43       0.77      0.64      0.70        94\n",
      "          44       0.84      0.82      0.83       111\n",
      "          45       0.92      0.86      0.89        95\n",
      "          46       0.84      0.95      0.89        88\n",
      "          47       0.71      0.79      0.75        90\n",
      "          48       0.70      0.79      0.74        98\n",
      "          49       0.81      0.73      0.77       106\n",
      "          50       0.49      0.54      0.51        99\n",
      "          51       0.98      0.98      0.98        59\n",
      "          52       0.90      0.90      0.90        89\n",
      "          53       0.68      0.68      0.68       100\n",
      "\n",
      "    accuracy                           0.80      5296\n",
      "   macro avg       0.80      0.80      0.80      5296\n",
      "weighted avg       0.80      0.80      0.80      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 98,  accuracy score is 1.0\n",
      "at random state 98, confusion matrix is [[74  0  0 ...  0  0  0]\n",
      " [ 0 57  0 ...  0  0  0]\n",
      " [ 0  0 97 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 61  0  0]\n",
      " [ 0  0  0 ...  0 75  0]\n",
      " [ 0  1  3 ...  0  0 64]]\n",
      "at random state 98, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.83      0.80        89\n",
      "           1       0.66      0.66      0.66        87\n",
      "           2       0.87      0.90      0.89       108\n",
      "           3       0.75      0.80      0.77       103\n",
      "           4       0.81      0.87      0.84       120\n",
      "           5       0.70      0.76      0.73        99\n",
      "           6       0.89      0.91      0.90        81\n",
      "           7       0.82      0.81      0.81       110\n",
      "           8       0.90      0.94      0.92        93\n",
      "           9       0.78      0.69      0.73        97\n",
      "          10       0.75      0.71      0.73        99\n",
      "          11       0.87      0.84      0.85        92\n",
      "          12       0.90      0.81      0.85       107\n",
      "          13       0.57      0.74      0.64        81\n",
      "          14       0.82      0.78      0.80       108\n",
      "          15       0.84      0.97      0.90        88\n",
      "          16       0.88      0.89      0.89       112\n",
      "          17       0.77      0.85      0.81        93\n",
      "          18       0.87      0.93      0.90        96\n",
      "          19       0.84      0.76      0.80       131\n",
      "          20       0.79      0.83      0.81        98\n",
      "          21       0.63      0.73      0.67        88\n",
      "          22       0.88      0.91      0.89        93\n",
      "          23       0.75      0.78      0.76       114\n",
      "          24       0.63      0.57      0.60       103\n",
      "          25       0.95      0.80      0.87        87\n",
      "          26       0.67      0.63      0.65       106\n",
      "          27       0.78      0.80      0.79        96\n",
      "          28       0.81      0.82      0.81        96\n",
      "          29       0.94      0.92      0.93       110\n",
      "          30       0.87      0.80      0.83       106\n",
      "          31       0.58      0.71      0.64        86\n",
      "          32       0.88      0.82      0.85       104\n",
      "          33       0.80      0.78      0.79       101\n",
      "          34       0.85      0.72      0.78       109\n",
      "          35       0.84      0.88      0.86        85\n",
      "          36       0.64      0.76      0.69        88\n",
      "          37       0.73      0.75      0.74       102\n",
      "          38       0.77      0.85      0.81       105\n",
      "          39       0.90      0.81      0.85       104\n",
      "          40       0.80      0.76      0.78        96\n",
      "          41       0.78      0.69      0.74        98\n",
      "          42       0.88      0.88      0.88        95\n",
      "          43       0.73      0.67      0.70       105\n",
      "          44       0.68      0.67      0.67        97\n",
      "          45       0.96      0.89      0.92        97\n",
      "          46       0.90      0.96      0.93        84\n",
      "          47       0.80      0.72      0.76        89\n",
      "          48       0.66      0.61      0.64       110\n",
      "          49       0.84      0.71      0.77       110\n",
      "          50       0.47      0.49      0.48       102\n",
      "          51       1.00      1.00      1.00        61\n",
      "          52       0.91      0.90      0.91        83\n",
      "          53       0.68      0.68      0.68        94\n",
      "\n",
      "    accuracy                           0.79      5296\n",
      "   macro avg       0.79      0.79      0.79      5296\n",
      "weighted avg       0.79      0.79      0.79      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 99,  accuracy score is 1.0\n",
      "at random state 99, confusion matrix is [[77  0  0 ...  0  0  0]\n",
      " [ 0 66  0 ...  0  0  1]\n",
      " [ 0  0 72 ...  0  0  1]\n",
      " ...\n",
      " [ 0  0  0 ... 55  0  0]\n",
      " [ 0  0  0 ...  0 80  0]\n",
      " [ 0  0  1 ...  0  0 70]]\n",
      "at random state 99, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.75      0.78       102\n",
      "           1       0.65      0.66      0.66       100\n",
      "           2       0.90      0.91      0.91        79\n",
      "           3       0.82      0.80      0.81        97\n",
      "           4       0.88      0.90      0.89       109\n",
      "           5       0.68      0.83      0.75        92\n",
      "           6       0.93      0.97      0.95        91\n",
      "           7       0.78      0.84      0.81        90\n",
      "           8       0.94      0.93      0.93       112\n",
      "           9       0.82      0.79      0.81       101\n",
      "          10       0.68      0.69      0.69        95\n",
      "          11       0.91      0.82      0.86       105\n",
      "          12       0.89      0.87      0.88       107\n",
      "          13       0.73      0.78      0.75        98\n",
      "          14       0.95      0.71      0.81       109\n",
      "          15       0.97      0.96      0.97        81\n",
      "          16       0.87      0.90      0.89        91\n",
      "          17       0.81      0.85      0.83       104\n",
      "          18       0.83      0.83      0.83        94\n",
      "          19       0.70      0.78      0.74       102\n",
      "          20       0.88      0.83      0.85       109\n",
      "          21       0.61      0.70      0.65       103\n",
      "          22       0.94      0.92      0.93        97\n",
      "          23       0.74      0.81      0.77        96\n",
      "          24       0.66      0.59      0.62       108\n",
      "          25       0.94      0.95      0.95       103\n",
      "          26       0.68      0.68      0.68        93\n",
      "          27       0.85      0.81      0.83       108\n",
      "          28       0.83      0.83      0.83        82\n",
      "          29       0.87      0.91      0.89        96\n",
      "          30       0.93      0.89      0.91        98\n",
      "          31       0.55      0.73      0.63        82\n",
      "          32       0.76      0.78      0.77        94\n",
      "          33       0.78      0.79      0.78       107\n",
      "          34       0.80      0.79      0.80       114\n",
      "          35       0.88      0.89      0.89       111\n",
      "          36       0.74      0.67      0.70       105\n",
      "          37       0.75      0.73      0.74        88\n",
      "          38       0.80      0.79      0.79       105\n",
      "          39       0.82      0.81      0.82        93\n",
      "          40       0.80      0.88      0.84       105\n",
      "          41       0.86      0.77      0.81       109\n",
      "          42       0.87      0.86      0.86        83\n",
      "          43       0.74      0.76      0.75        92\n",
      "          44       0.79      0.81      0.80       118\n",
      "          45       0.90      0.95      0.93        87\n",
      "          46       0.95      0.85      0.90        97\n",
      "          47       0.77      0.76      0.76       113\n",
      "          48       0.74      0.75      0.74        93\n",
      "          49       0.84      0.75      0.79       105\n",
      "          50       0.55      0.54      0.55        96\n",
      "          51       1.00      1.00      1.00        55\n",
      "          52       0.95      0.94      0.95        85\n",
      "          53       0.71      0.65      0.68       107\n",
      "\n",
      "    accuracy                           0.81      5296\n",
      "   macro avg       0.81      0.81      0.81      5296\n",
      "weighted avg       0.81      0.81      0.81      5296\n",
      "\n",
      "\n",
      "\n",
      "Max accuracy at random state 99 = 0.8066465256797583\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc=DecisionTreeClassifier()\n",
    "model_selection(dtc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a523483c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.69367992 0.64525994 0.67108618 0.68689444 0.62621112 0.51810301\n",
      " 0.56858746 0.58235594 0.56807751]\n",
      "0.6178061680629334\n",
      "0.05800956580795669\n"
     ]
    }
   ],
   "source": [
    "score=cross_val_score(dtc,xx,yy,cv=9)\n",
    "print(score)\n",
    "print(score.mean())\n",
    "print(score.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16803ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a66729f5",
   "metadata": {},
   "source": [
    "# RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3b7fb9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 0,  accuracy score is 1.0\n",
      "at random state 0, confusion matrix is [[ 90   0   0 ...   0   0   0]\n",
      " [  0  86   0 ...   0   0   0]\n",
      " [  0   0 100 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  49   0   0]\n",
      " [  0   0   0 ...   0  76   0]\n",
      " [  0   1   2 ...   0   0  73]]\n",
      "at random state 0, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.97      0.90        93\n",
      "           1       0.80      0.84      0.82       102\n",
      "           2       0.95      1.00      0.98       100\n",
      "           3       0.92      0.84      0.87       104\n",
      "           4       0.98      0.97      0.98       104\n",
      "           5       0.89      0.81      0.85        97\n",
      "           6       0.97      1.00      0.98        84\n",
      "           7       0.87      0.88      0.87       106\n",
      "           8       0.98      0.96      0.97       119\n",
      "           9       0.91      0.92      0.91        93\n",
      "          10       0.89      0.90      0.89       113\n",
      "          11       0.93      0.95      0.94       104\n",
      "          12       0.88      0.98      0.93        97\n",
      "          13       0.90      0.82      0.86        89\n",
      "          14       0.90      0.89      0.90        93\n",
      "          15       0.97      0.99      0.98        79\n",
      "          16       0.95      0.96      0.95        98\n",
      "          17       0.88      0.95      0.91        95\n",
      "          18       0.88      0.93      0.90        97\n",
      "          19       0.84      0.92      0.88        87\n",
      "          20       0.97      0.91      0.94       100\n",
      "          21       0.87      0.81      0.84       105\n",
      "          22       0.98      1.00      0.99       102\n",
      "          23       0.81      0.90      0.85       100\n",
      "          24       0.76      0.71      0.73       112\n",
      "          25       0.98      0.99      0.98        97\n",
      "          26       0.91      0.84      0.87       104\n",
      "          27       0.86      0.90      0.88        96\n",
      "          28       0.97      0.95      0.96        94\n",
      "          29       0.98      0.98      0.98       100\n",
      "          30       0.98      0.94      0.96       105\n",
      "          31       0.76      0.70      0.73       105\n",
      "          32       0.93      0.87      0.90       109\n",
      "          33       0.88      0.95      0.91       100\n",
      "          34       0.95      0.92      0.94       111\n",
      "          35       0.97      0.95      0.96        91\n",
      "          36       0.90      0.79      0.84       104\n",
      "          37       0.86      0.87      0.86        99\n",
      "          38       0.92      0.91      0.92       116\n",
      "          39       0.94      0.91      0.93       105\n",
      "          40       0.87      0.93      0.90       112\n",
      "          41       0.83      0.88      0.86       102\n",
      "          42       0.91      0.99      0.94        78\n",
      "          43       0.87      0.88      0.88        93\n",
      "          44       0.91      0.86      0.89       100\n",
      "          45       0.97      0.99      0.98        85\n",
      "          46       0.99      1.00      0.99        96\n",
      "          47       0.90      0.88      0.89       102\n",
      "          48       0.90      0.86      0.88        98\n",
      "          49       0.88      0.89      0.88        97\n",
      "          50       0.65      0.69      0.67       109\n",
      "          51       0.98      1.00      0.99        49\n",
      "          52       0.99      0.97      0.98        78\n",
      "          53       0.95      0.83      0.88        88\n",
      "\n",
      "    accuracy                           0.90      5296\n",
      "   macro avg       0.91      0.91      0.90      5296\n",
      "weighted avg       0.90      0.90      0.90      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 1,  accuracy score is 1.0\n",
      "at random state 1, confusion matrix is [[ 90   0   0 ...   0   0   0]\n",
      " [  0  95   0 ...   0   0   0]\n",
      " [  0   0 116 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  46   0   0]\n",
      " [  0   0   0 ...   0  67   0]\n",
      " [  0   0   0 ...   0   0  91]]\n",
      "at random state 1, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.92      0.88        98\n",
      "           1       0.86      0.88      0.87       108\n",
      "           2       0.94      0.95      0.95       122\n",
      "           3       0.94      0.95      0.94        92\n",
      "           4       0.95      0.96      0.96       114\n",
      "           5       0.85      0.79      0.82       107\n",
      "           6       0.99      0.98      0.98        91\n",
      "           7       0.94      0.85      0.89       102\n",
      "           8       0.97      0.97      0.97        98\n",
      "           9       0.92      0.91      0.92       102\n",
      "          10       0.85      0.86      0.86       114\n",
      "          11       0.93      0.90      0.92       104\n",
      "          12       0.97      0.97      0.97       118\n",
      "          13       0.80      0.80      0.80       101\n",
      "          14       0.93      0.92      0.92        96\n",
      "          15       0.96      0.95      0.95        78\n",
      "          16       0.97      0.95      0.96        96\n",
      "          17       0.89      0.92      0.90       106\n",
      "          18       0.86      0.90      0.88        97\n",
      "          19       0.90      0.91      0.90       106\n",
      "          20       0.97      0.96      0.96        90\n",
      "          21       0.83      0.89      0.86       101\n",
      "          22       0.97      0.99      0.98        97\n",
      "          23       0.92      0.93      0.92        99\n",
      "          24       0.73      0.72      0.73        94\n",
      "          25       0.92      1.00      0.96        96\n",
      "          26       0.83      0.79      0.81        97\n",
      "          27       0.87      0.91      0.89        98\n",
      "          28       0.97      0.93      0.95       102\n",
      "          29       0.98      0.94      0.96        84\n",
      "          30       0.97      0.98      0.98       104\n",
      "          31       0.67      0.82      0.74        96\n",
      "          32       0.91      0.86      0.89        94\n",
      "          33       0.89      0.95      0.92        96\n",
      "          34       0.98      0.93      0.95        99\n",
      "          35       0.91      0.95      0.93        99\n",
      "          36       0.82      0.86      0.84        97\n",
      "          37       0.81      0.87      0.84        99\n",
      "          38       0.88      0.88      0.88       114\n",
      "          39       0.92      0.96      0.94       113\n",
      "          40       0.94      0.94      0.94       109\n",
      "          41       0.89      0.91      0.90        96\n",
      "          42       0.95      0.93      0.94        90\n",
      "          43       0.89      0.83      0.86        98\n",
      "          44       0.88      0.91      0.89        85\n",
      "          45       0.97      1.00      0.99        70\n",
      "          46       0.96      0.96      0.96       101\n",
      "          47       0.92      0.88      0.90        94\n",
      "          48       0.90      0.80      0.85        90\n",
      "          49       0.91      0.87      0.89       110\n",
      "          50       0.73      0.57      0.64       115\n",
      "          51       1.00      1.00      1.00        46\n",
      "          52       0.99      1.00      0.99        67\n",
      "          53       0.88      0.86      0.87       106\n",
      "\n",
      "    accuracy                           0.90      5296\n",
      "   macro avg       0.90      0.90      0.90      5296\n",
      "weighted avg       0.90      0.90      0.90      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 2,  accuracy score is 1.0\n",
      "at random state 2, confusion matrix is [[97  0  0 ...  0  0  0]\n",
      " [ 0 79  0 ...  0  0  0]\n",
      " [ 0  0 93 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 43  0  0]\n",
      " [ 0  0  0 ...  2 84  0]\n",
      " [ 0  0  1 ...  0  0 72]]\n",
      "at random state 2, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.91      0.89       107\n",
      "           1       0.81      0.81      0.81        98\n",
      "           2       0.95      0.99      0.97        94\n",
      "           3       0.92      0.90      0.91        99\n",
      "           4       0.96      0.94      0.95       113\n",
      "           5       0.89      0.88      0.89        93\n",
      "           6       0.99      1.00      0.99        82\n",
      "           7       0.94      0.88      0.91       107\n",
      "           8       0.92      0.99      0.96        86\n",
      "           9       0.92      0.93      0.92        97\n",
      "          10       0.85      0.86      0.86       101\n",
      "          11       0.94      0.93      0.94       105\n",
      "          12       0.97      0.98      0.98       108\n",
      "          13       0.89      0.81      0.84       108\n",
      "          14       0.93      0.94      0.94       100\n",
      "          15       0.98      0.99      0.98        80\n",
      "          16       0.98      0.94      0.96       106\n",
      "          17       0.94      0.90      0.92       106\n",
      "          18       0.90      0.91      0.90        98\n",
      "          19       0.85      0.89      0.87        97\n",
      "          20       0.96      0.95      0.96       104\n",
      "          21       0.87      0.88      0.88       104\n",
      "          22       0.97      0.99      0.98        90\n",
      "          23       0.84      0.88      0.86       110\n",
      "          24       0.79      0.72      0.75       104\n",
      "          25       0.97      0.98      0.98       117\n",
      "          26       0.72      0.85      0.78        75\n",
      "          27       0.89      0.91      0.90       119\n",
      "          28       0.96      0.95      0.95        97\n",
      "          29       0.98      0.96      0.97       101\n",
      "          30       0.97      0.98      0.98       111\n",
      "          31       0.77      0.82      0.79       104\n",
      "          32       0.95      0.90      0.92       100\n",
      "          33       0.89      0.88      0.88        91\n",
      "          34       0.97      0.96      0.96        99\n",
      "          35       0.93      0.94      0.94        90\n",
      "          36       0.90      0.77      0.83       107\n",
      "          37       0.81      0.90      0.85       103\n",
      "          38       0.91      0.89      0.90       104\n",
      "          39       0.97      0.94      0.95       112\n",
      "          40       0.86      0.94      0.90        98\n",
      "          41       0.88      0.93      0.91       104\n",
      "          42       0.96      0.99      0.98        82\n",
      "          43       0.86      0.92      0.89        99\n",
      "          44       0.91      0.94      0.93        99\n",
      "          45       1.00      1.00      1.00        90\n",
      "          46       0.97      0.99      0.98        92\n",
      "          47       0.93      0.88      0.90        99\n",
      "          48       0.90      0.85      0.88        89\n",
      "          49       0.87      0.87      0.87       102\n",
      "          50       0.70      0.68      0.69        94\n",
      "          51       0.96      1.00      0.98        43\n",
      "          52       1.00      0.98      0.99        86\n",
      "          53       0.89      0.78      0.83        92\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.91      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 3,  accuracy score is 1.0\n",
      "at random state 3, confusion matrix is [[ 89   0   0 ...   0   0   0]\n",
      " [  0  88   0 ...   0   0   0]\n",
      " [  0   0 106 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  50   0   0]\n",
      " [  0   0   0 ...   0  71   0]\n",
      " [  0   1   1 ...   0   0  89]]\n",
      "at random state 3, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.88      0.86       101\n",
      "           1       0.87      0.82      0.85       107\n",
      "           2       0.95      0.98      0.97       108\n",
      "           3       0.90      0.93      0.91       107\n",
      "           4       0.97      0.95      0.96       103\n",
      "           5       0.85      0.84      0.85       108\n",
      "           6       0.95      1.00      0.98        79\n",
      "           7       0.91      0.94      0.92        82\n",
      "           8       0.98      0.96      0.97       108\n",
      "           9       0.89      0.97      0.93        98\n",
      "          10       0.83      0.87      0.85        99\n",
      "          11       0.93      0.97      0.95        87\n",
      "          12       0.93      0.96      0.95       104\n",
      "          13       0.90      0.93      0.92        91\n",
      "          14       0.92      0.94      0.93       101\n",
      "          15       0.99      0.97      0.98        89\n",
      "          16       0.93      0.93      0.93       105\n",
      "          17       0.93      0.91      0.92       100\n",
      "          18       0.95      0.95      0.95        95\n",
      "          19       0.88      0.85      0.86        94\n",
      "          20       0.96      0.92      0.94       105\n",
      "          21       0.81      0.85      0.83        97\n",
      "          22       0.99      1.00      0.99        95\n",
      "          23       0.89      0.86      0.88       106\n",
      "          24       0.69      0.72      0.71        97\n",
      "          25       0.98      0.99      0.98        84\n",
      "          26       0.81      0.85      0.83        98\n",
      "          27       0.86      0.91      0.88        99\n",
      "          28       0.96      0.95      0.96       113\n",
      "          29       0.98      1.00      0.99        93\n",
      "          30       0.97      0.98      0.98       107\n",
      "          31       0.72      0.78      0.75        99\n",
      "          32       0.94      0.87      0.90        98\n",
      "          33       0.90      0.91      0.90       113\n",
      "          34       0.96      0.92      0.94        86\n",
      "          35       0.98      0.99      0.99       103\n",
      "          36       0.88      0.85      0.87       108\n",
      "          37       0.91      0.85      0.88        95\n",
      "          38       0.90      0.90      0.90        91\n",
      "          39       0.93      0.92      0.92       109\n",
      "          40       0.90      0.91      0.90        95\n",
      "          41       0.88      0.90      0.89        99\n",
      "          42       0.96      0.98      0.97        84\n",
      "          43       0.86      0.88      0.87       107\n",
      "          44       0.91      0.91      0.91       101\n",
      "          45       0.98      0.98      0.98       102\n",
      "          46       0.98      0.98      0.98        92\n",
      "          47       0.90      0.91      0.90       104\n",
      "          48       0.93      0.83      0.88       109\n",
      "          49       0.93      0.89      0.91       111\n",
      "          50       0.75      0.60      0.67       110\n",
      "          51       1.00      1.00      1.00        50\n",
      "          52       1.00      1.00      1.00        71\n",
      "          53       0.86      0.90      0.88        99\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.91      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 4,  accuracy score is 1.0\n",
      "at random state 4, confusion matrix is [[ 87   0   0 ...   0   0   0]\n",
      " [  0  81   0 ...   0   0   0]\n",
      " [  0   0 103 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  53   0   0]\n",
      " [  0   0   0 ...   1  86   0]\n",
      " [  0   2   2 ...   0   0  90]]\n",
      "at random state 4, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.92      0.88        95\n",
      "           1       0.84      0.86      0.85        94\n",
      "           2       0.95      0.99      0.97       104\n",
      "           3       0.96      0.84      0.89       111\n",
      "           4       0.95      0.97      0.96        94\n",
      "           5       0.92      0.79      0.85       113\n",
      "           6       0.99      0.99      0.99       100\n",
      "           7       0.93      0.84      0.88       100\n",
      "           8       0.94      0.95      0.95       101\n",
      "           9       0.94      0.81      0.87        97\n",
      "          10       0.87      0.88      0.87       107\n",
      "          11       0.93      0.94      0.94       106\n",
      "          12       0.97      0.91      0.94        91\n",
      "          13       0.83      0.92      0.88        99\n",
      "          14       0.91      0.96      0.94        85\n",
      "          15       1.00      0.99      0.99        82\n",
      "          16       0.98      0.94      0.96       107\n",
      "          17       0.92      0.95      0.93        94\n",
      "          18       0.94      0.95      0.95       110\n",
      "          19       0.85      0.90      0.88        90\n",
      "          20       0.94      0.93      0.93       108\n",
      "          21       0.86      0.83      0.84        98\n",
      "          22       0.99      0.98      0.99       101\n",
      "          23       0.85      0.93      0.89       100\n",
      "          24       0.78      0.71      0.74        87\n",
      "          25       0.97      1.00      0.98        98\n",
      "          26       0.87      0.80      0.83       103\n",
      "          27       0.91      0.90      0.91        96\n",
      "          28       0.97      0.95      0.96       105\n",
      "          29       0.98      0.99      0.98        82\n",
      "          30       0.98      0.96      0.97       110\n",
      "          31       0.75      0.84      0.79        93\n",
      "          32       0.91      0.90      0.90        96\n",
      "          33       0.87      0.98      0.92        90\n",
      "          34       0.97      0.96      0.97       112\n",
      "          35       0.94      0.98      0.96       104\n",
      "          36       0.86      0.86      0.86       108\n",
      "          37       0.82      0.92      0.87       101\n",
      "          38       0.87      0.92      0.90       103\n",
      "          39       0.91      0.95      0.93       102\n",
      "          40       0.91      0.90      0.91        93\n",
      "          41       0.87      0.93      0.90        95\n",
      "          42       0.96      0.94      0.95        95\n",
      "          43       0.88      0.84      0.86        94\n",
      "          44       0.94      0.90      0.92       115\n",
      "          45       0.98      0.94      0.96        84\n",
      "          46       0.97      0.99      0.98        84\n",
      "          47       0.85      0.88      0.86       105\n",
      "          48       0.90      0.87      0.88       104\n",
      "          49       0.88      0.90      0.89       105\n",
      "          50       0.68      0.71      0.70        91\n",
      "          51       0.98      1.00      0.99        53\n",
      "          52       0.98      0.99      0.98        87\n",
      "          53       0.90      0.79      0.84       114\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.91      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 5,  accuracy score is 1.0\n",
      "at random state 5, confusion matrix is [[ 85   0   0 ...   0   0   0]\n",
      " [  0  81   0 ...   0   0   1]\n",
      " [  0   0 105 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  56   0   0]\n",
      " [  0   0   0 ...   0  69   0]\n",
      " [  0   0   1 ...   0   0  78]]\n",
      "at random state 5, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.92      0.89        92\n",
      "           1       0.81      0.79      0.80       102\n",
      "           2       0.97      0.98      0.98       107\n",
      "           3       0.95      0.86      0.90       105\n",
      "           4       0.94      0.95      0.95       109\n",
      "           5       0.88      0.81      0.85       102\n",
      "           6       0.97      1.00      0.98        87\n",
      "           7       0.91      0.96      0.93       102\n",
      "           8       0.98      1.00      0.99        97\n",
      "           9       0.89      0.95      0.92        85\n",
      "          10       0.86      0.88      0.87        98\n",
      "          11       0.93      0.89      0.91       107\n",
      "          12       0.95      0.98      0.96       106\n",
      "          13       0.93      0.88      0.91       104\n",
      "          14       0.93      0.89      0.91        99\n",
      "          15       0.97      0.99      0.98        73\n",
      "          16       0.95      0.90      0.93       114\n",
      "          17       0.95      0.94      0.94       100\n",
      "          18       0.83      0.92      0.87        98\n",
      "          19       0.90      0.90      0.90       103\n",
      "          20       0.99      0.92      0.95       100\n",
      "          21       0.81      0.77      0.79       107\n",
      "          22       0.98      0.99      0.98        87\n",
      "          23       0.82      0.96      0.88       102\n",
      "          24       0.76      0.73      0.74       101\n",
      "          25       0.99      0.97      0.98       105\n",
      "          26       0.89      0.81      0.85       113\n",
      "          27       0.91      0.92      0.91        98\n",
      "          28       0.93      0.96      0.95       102\n",
      "          29       0.98      0.96      0.97       103\n",
      "          30       0.95      0.98      0.97        99\n",
      "          31       0.77      0.78      0.77       108\n",
      "          32       0.88      0.90      0.89        91\n",
      "          33       0.83      0.93      0.87        97\n",
      "          34       0.95      0.96      0.95        97\n",
      "          35       0.97      0.98      0.98       110\n",
      "          36       0.85      0.89      0.87       106\n",
      "          37       0.93      0.91      0.92       109\n",
      "          38       0.87      0.95      0.91        96\n",
      "          39       0.91      0.93      0.92        86\n",
      "          40       0.90      0.87      0.89       109\n",
      "          41       0.87      0.90      0.88       105\n",
      "          42       0.90      0.96      0.93        97\n",
      "          43       0.89      0.85      0.87        93\n",
      "          44       0.92      0.90      0.91        88\n",
      "          45       0.99      0.99      0.99        93\n",
      "          46       0.99      0.99      0.99        90\n",
      "          47       0.94      0.85      0.89        91\n",
      "          48       0.82      0.81      0.82        93\n",
      "          49       0.90      0.84      0.87       105\n",
      "          50       0.58      0.66      0.62        93\n",
      "          51       1.00      1.00      1.00        56\n",
      "          52       1.00      0.99      0.99        70\n",
      "          53       0.93      0.74      0.82       106\n",
      "\n",
      "    accuracy                           0.90      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.90      0.90      0.90      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 6,  accuracy score is 1.0\n",
      "at random state 6, confusion matrix is [[ 88   0   0 ...   0   0   0]\n",
      " [  0 103   0 ...   0   0   0]\n",
      " [  0   0 111 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  55   0   0]\n",
      " [  0   0   0 ...   0  79   0]\n",
      " [  0   2   0 ...   0   0  86]]\n",
      "at random state 6, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.86      0.85       102\n",
      "           1       0.84      0.89      0.87       116\n",
      "           2       0.97      0.97      0.97       114\n",
      "           3       0.88      0.84      0.86        77\n",
      "           4       0.98      0.94      0.96        90\n",
      "           5       0.87      0.87      0.87        92\n",
      "           6       0.95      0.99      0.97        80\n",
      "           7       0.92      0.91      0.92       119\n",
      "           8       1.00      0.97      0.98       101\n",
      "           9       0.91      0.87      0.89       106\n",
      "          10       0.90      0.92      0.91       104\n",
      "          11       0.94      0.95      0.95       118\n",
      "          12       0.88      0.97      0.92        94\n",
      "          13       0.88      0.88      0.88        98\n",
      "          14       0.90      0.93      0.92        90\n",
      "          15       0.97      1.00      0.99        72\n",
      "          16       0.99      0.96      0.97       100\n",
      "          17       0.92      0.95      0.93        96\n",
      "          18       0.86      0.91      0.89       104\n",
      "          19       0.93      0.89      0.91        98\n",
      "          20       0.96      0.98      0.97       100\n",
      "          21       0.87      0.89      0.88       108\n",
      "          22       0.99      0.98      0.98        91\n",
      "          23       0.89      0.90      0.90       110\n",
      "          24       0.88      0.71      0.78       113\n",
      "          25       0.99      0.99      0.99        83\n",
      "          26       0.92      0.91      0.91        95\n",
      "          27       0.93      0.91      0.92        95\n",
      "          28       0.96      0.98      0.97        95\n",
      "          29       0.99      0.98      0.98        98\n",
      "          30       0.95      0.96      0.95        97\n",
      "          31       0.76      0.82      0.79       102\n",
      "          32       0.94      0.94      0.94        99\n",
      "          33       0.94      0.89      0.91       119\n",
      "          34       0.99      0.94      0.97       104\n",
      "          35       0.96      0.97      0.96        99\n",
      "          36       0.86      0.81      0.83       114\n",
      "          37       0.86      0.91      0.89       102\n",
      "          38       0.91      0.93      0.92       113\n",
      "          39       0.96      0.94      0.95        99\n",
      "          40       0.92      0.92      0.92        95\n",
      "          41       0.88      0.88      0.88       112\n",
      "          42       0.95      0.98      0.97        99\n",
      "          43       0.90      0.88      0.89        99\n",
      "          44       0.92      0.93      0.92        94\n",
      "          45       0.97      0.98      0.97        93\n",
      "          46       0.99      0.96      0.98        83\n",
      "          47       0.91      0.89      0.90       106\n",
      "          48       0.92      0.90      0.91        86\n",
      "          49       0.85      0.90      0.87       105\n",
      "          50       0.57      0.65      0.61        84\n",
      "          51       1.00      1.00      1.00        55\n",
      "          52       1.00      1.00      1.00        79\n",
      "          53       0.92      0.87      0.90        99\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.92      0.92      0.92      5296\n",
      "weighted avg       0.92      0.91      0.92      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 7,  accuracy score is 1.0\n",
      "at random state 7, confusion matrix is [[ 87   0   0 ...   0   0   0]\n",
      " [  0 100   0 ...   0   0   0]\n",
      " [  0   0 105 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  45   0   0]\n",
      " [  0   0   0 ...   1  78   0]\n",
      " [  0   1   1 ...   0   0  92]]\n",
      "at random state 7, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.90      0.86        97\n",
      "           1       0.85      0.89      0.87       112\n",
      "           2       0.95      0.97      0.96       108\n",
      "           3       0.97      0.88      0.92        97\n",
      "           4       0.97      0.96      0.97       104\n",
      "           5       0.86      0.77      0.81        94\n",
      "           6       0.99      0.98      0.98        85\n",
      "           7       0.87      0.87      0.87       102\n",
      "           8       0.94      0.97      0.96       110\n",
      "           9       0.88      0.93      0.90       114\n",
      "          10       0.88      0.88      0.88       104\n",
      "          11       0.97      0.92      0.94        95\n",
      "          12       0.95      0.95      0.95       116\n",
      "          13       0.82      0.83      0.83       103\n",
      "          14       0.88      0.87      0.87        97\n",
      "          15       0.99      0.99      0.99        80\n",
      "          16       0.98      0.96      0.97       104\n",
      "          17       0.87      0.93      0.90        85\n",
      "          18       0.91      0.94      0.93       109\n",
      "          19       0.83      0.79      0.81        99\n",
      "          20       0.95      0.94      0.94        97\n",
      "          21       0.78      0.81      0.80        97\n",
      "          22       0.99      0.98      0.98        99\n",
      "          23       0.84      0.94      0.89       103\n",
      "          24       0.76      0.74      0.75        92\n",
      "          25       0.99      0.99      0.99        98\n",
      "          26       0.87      0.85      0.86       100\n",
      "          27       0.88      0.93      0.91        89\n",
      "          28       0.97      0.99      0.98       101\n",
      "          29       1.00      0.98      0.99        88\n",
      "          30       0.98      0.96      0.97       107\n",
      "          31       0.82      0.81      0.81        93\n",
      "          32       0.94      0.90      0.92       105\n",
      "          33       0.81      0.93      0.86        85\n",
      "          34       0.97      0.92      0.94       122\n",
      "          35       0.92      0.97      0.95       110\n",
      "          36       0.84      0.91      0.88        89\n",
      "          37       0.83      0.89      0.86       104\n",
      "          38       0.87      0.89      0.88        98\n",
      "          39       0.89      0.87      0.88        98\n",
      "          40       0.89      0.94      0.91        95\n",
      "          41       0.91      0.84      0.88        96\n",
      "          42       0.95      0.95      0.95        83\n",
      "          43       0.90      0.88      0.89       104\n",
      "          44       0.93      0.88      0.90       106\n",
      "          45       0.96      1.00      0.98        92\n",
      "          46       0.99      0.95      0.97       102\n",
      "          47       0.88      0.89      0.89        95\n",
      "          48       0.91      0.85      0.87        91\n",
      "          49       0.86      0.86      0.86        96\n",
      "          50       0.73      0.68      0.71       104\n",
      "          51       0.96      1.00      0.98        45\n",
      "          52       0.99      0.97      0.98        80\n",
      "          53       0.95      0.79      0.86       117\n",
      "\n",
      "    accuracy                           0.90      5296\n",
      "   macro avg       0.90      0.90      0.90      5296\n",
      "weighted avg       0.90      0.90      0.90      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 8,  accuracy score is 1.0\n",
      "at random state 8, confusion matrix is [[ 76   0   0 ...   0   0   0]\n",
      " [  0 105   0 ...   0   0   0]\n",
      " [  0   0  98 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  61   0   0]\n",
      " [  0   0   0 ...   0  78   0]\n",
      " [  0   1   3 ...   0   0  81]]\n",
      "at random state 8, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.89      0.88        85\n",
      "           1       0.85      0.88      0.87       119\n",
      "           2       0.96      0.99      0.98        99\n",
      "           3       0.97      0.84      0.90       100\n",
      "           4       0.96      0.96      0.96       119\n",
      "           5       0.84      0.84      0.84        96\n",
      "           6       0.99      0.99      0.99        92\n",
      "           7       0.88      0.89      0.88        97\n",
      "           8       0.98      0.97      0.97        98\n",
      "           9       0.91      0.88      0.90        92\n",
      "          10       0.86      0.81      0.84       107\n",
      "          11       0.93      0.95      0.94       109\n",
      "          12       0.89      0.97      0.93        86\n",
      "          13       0.89      0.84      0.87       101\n",
      "          14       0.89      0.93      0.91       100\n",
      "          15       0.99      1.00      0.99        84\n",
      "          16       0.96      0.96      0.96       114\n",
      "          17       0.98      0.94      0.96       104\n",
      "          18       0.90      0.92      0.91        99\n",
      "          19       0.88      0.89      0.89       111\n",
      "          20       0.97      0.87      0.92        87\n",
      "          21       0.86      0.80      0.83        98\n",
      "          22       0.99      0.99      0.99       103\n",
      "          23       0.84      0.96      0.90        95\n",
      "          24       0.84      0.65      0.73       110\n",
      "          25       1.00      1.00      1.00       114\n",
      "          26       0.82      0.84      0.83        97\n",
      "          27       0.91      0.93      0.92       105\n",
      "          28       0.95      0.92      0.94       113\n",
      "          29       0.98      0.93      0.95       101\n",
      "          30       0.94      0.97      0.95        98\n",
      "          31       0.70      0.71      0.70        87\n",
      "          32       0.90      0.92      0.91        88\n",
      "          33       0.86      0.93      0.89        94\n",
      "          34       0.97      0.92      0.94        95\n",
      "          35       0.93      0.99      0.96        92\n",
      "          36       0.91      0.83      0.87       112\n",
      "          37       0.87      0.83      0.85       109\n",
      "          38       0.88      0.95      0.92       102\n",
      "          39       0.91      0.93      0.92       101\n",
      "          40       0.91      0.88      0.90       108\n",
      "          41       0.82      0.91      0.86       108\n",
      "          42       0.93      0.99      0.96        95\n",
      "          43       0.88      0.92      0.90        99\n",
      "          44       0.86      0.89      0.87       109\n",
      "          45       0.96      0.99      0.98        82\n",
      "          46       0.95      0.97      0.96        71\n",
      "          47       0.91      0.87      0.89        93\n",
      "          48       0.95      0.88      0.91        91\n",
      "          49       0.86      0.87      0.86        92\n",
      "          50       0.55      0.76      0.64        87\n",
      "          51       1.00      1.00      1.00        61\n",
      "          52       1.00      0.99      0.99        79\n",
      "          53       0.92      0.75      0.83       108\n",
      "\n",
      "    accuracy                           0.90      5296\n",
      "   macro avg       0.91      0.91      0.90      5296\n",
      "weighted avg       0.91      0.90      0.90      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 9,  accuracy score is 1.0\n",
      "at random state 9, confusion matrix is [[ 80   0   0 ...   0   0   0]\n",
      " [  0  84   0 ...   0   0   1]\n",
      " [  0   0 102 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  47   0   0]\n",
      " [  0   0   0 ...   0  81   0]\n",
      " [  0   0   1 ...   0   0  87]]\n",
      "at random state 9, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.92      0.89        87\n",
      "           1       0.92      0.78      0.84       108\n",
      "           2       0.94      0.98      0.96       104\n",
      "           3       0.90      0.81      0.85        94\n",
      "           4       0.95      0.98      0.97       106\n",
      "           5       0.81      0.75      0.78       106\n",
      "           6       0.97      0.99      0.98        93\n",
      "           7       0.90      0.83      0.86        95\n",
      "           8       0.98      0.99      0.98        93\n",
      "           9       0.88      0.97      0.92        92\n",
      "          10       0.84      0.85      0.85       107\n",
      "          11       0.91      0.94      0.93       105\n",
      "          12       0.94      0.96      0.95       102\n",
      "          13       0.90      0.84      0.87       113\n",
      "          14       0.84      0.94      0.88       109\n",
      "          15       0.95      0.98      0.97        59\n",
      "          16       0.98      0.95      0.97       104\n",
      "          17       0.97      0.94      0.95        98\n",
      "          18       0.93      0.91      0.92       108\n",
      "          19       0.87      0.84      0.85        86\n",
      "          20       0.95      0.95      0.95       116\n",
      "          21       0.68      0.86      0.76        92\n",
      "          22       1.00      1.00      1.00       105\n",
      "          23       0.88      0.87      0.87       104\n",
      "          24       0.75      0.71      0.73        96\n",
      "          25       0.98      0.99      0.99       108\n",
      "          26       0.82      0.87      0.84        90\n",
      "          27       0.94      0.95      0.94       110\n",
      "          28       0.96      0.96      0.96        91\n",
      "          29       0.98      0.98      0.98        83\n",
      "          30       0.97      0.96      0.96        96\n",
      "          31       0.78      0.69      0.73       106\n",
      "          32       0.92      0.89      0.91        94\n",
      "          33       0.85      0.85      0.85       111\n",
      "          34       0.95      0.96      0.96       104\n",
      "          35       0.97      0.98      0.98       101\n",
      "          36       0.89      0.88      0.89       101\n",
      "          37       0.80      0.87      0.84       102\n",
      "          38       0.88      0.92      0.90       106\n",
      "          39       0.91      0.93      0.92        94\n",
      "          40       0.93      0.92      0.92       106\n",
      "          41       0.85      0.89      0.87       103\n",
      "          42       0.91      0.97      0.94        89\n",
      "          43       0.94      0.91      0.92        99\n",
      "          44       0.90      0.88      0.89       105\n",
      "          45       1.00      0.97      0.98        87\n",
      "          46       1.00      0.98      0.99        91\n",
      "          47       0.90      0.84      0.87        99\n",
      "          48       0.89      0.88      0.88        97\n",
      "          49       0.83      0.85      0.84       103\n",
      "          50       0.60      0.66      0.63       104\n",
      "          51       1.00      1.00      1.00        47\n",
      "          52       0.96      1.00      0.98        81\n",
      "          53       0.95      0.82      0.88       106\n",
      "\n",
      "    accuracy                           0.90      5296\n",
      "   macro avg       0.90      0.90      0.90      5296\n",
      "weighted avg       0.90      0.90      0.90      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 10,  accuracy score is 1.0\n",
      "at random state 10, confusion matrix is [[ 89   0   0 ...   0   0   0]\n",
      " [  0  85   0 ...   0   0   0]\n",
      " [  0   0 105 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  51   0   0]\n",
      " [  0   0   0 ...   0  68   0]\n",
      " [  0   1   1 ...   0   0  60]]\n",
      "at random state 10, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.89      0.88       100\n",
      "           1       0.83      0.88      0.85        97\n",
      "           2       0.97      0.97      0.97       108\n",
      "           3       0.95      0.84      0.89       102\n",
      "           4       0.94      0.97      0.95        97\n",
      "           5       0.87      0.84      0.86       109\n",
      "           6       0.98      0.99      0.98        81\n",
      "           7       0.92      0.89      0.90        99\n",
      "           8       0.96      0.97      0.97       109\n",
      "           9       0.92      0.93      0.92        97\n",
      "          10       0.78      0.82      0.80       101\n",
      "          11       0.94      0.96      0.95       108\n",
      "          12       0.96      0.97      0.96        92\n",
      "          13       0.92      0.90      0.91       115\n",
      "          14       0.88      0.89      0.88        96\n",
      "          15       0.97      1.00      0.99        75\n",
      "          16       0.94      0.96      0.95       107\n",
      "          17       0.90      0.92      0.91        93\n",
      "          18       0.89      0.93      0.91       104\n",
      "          19       0.90      0.89      0.89       119\n",
      "          20       0.97      0.95      0.96       115\n",
      "          21       0.82      0.85      0.84       115\n",
      "          22       0.99      0.99      0.99       107\n",
      "          23       0.84      0.90      0.87       109\n",
      "          24       0.74      0.66      0.70        83\n",
      "          25       0.99      0.98      0.98        89\n",
      "          26       0.85      0.83      0.84        96\n",
      "          27       0.92      0.93      0.92       122\n",
      "          28       0.95      0.96      0.96       100\n",
      "          29       0.99      0.95      0.97       104\n",
      "          30       0.99      0.92      0.95       100\n",
      "          31       0.78      0.81      0.79       105\n",
      "          32       0.94      0.89      0.92       116\n",
      "          33       0.90      0.89      0.90       107\n",
      "          34       0.95      0.94      0.95        89\n",
      "          35       0.91      0.97      0.94        88\n",
      "          36       0.90      0.82      0.86        91\n",
      "          37       0.81      0.91      0.86        98\n",
      "          38       0.90      0.90      0.90       105\n",
      "          39       0.97      0.90      0.93       109\n",
      "          40       0.88      0.93      0.90        94\n",
      "          41       0.90      0.91      0.91       101\n",
      "          42       0.97      0.97      0.97        90\n",
      "          43       0.91      0.89      0.90        96\n",
      "          44       0.88      0.86      0.87        87\n",
      "          45       0.98      0.99      0.98        83\n",
      "          46       0.96      0.95      0.96       104\n",
      "          47       0.93      0.92      0.92        97\n",
      "          48       0.90      0.89      0.90       112\n",
      "          49       0.83      0.89      0.86        84\n",
      "          50       0.66      0.64      0.65       100\n",
      "          51       0.98      1.00      0.99        51\n",
      "          52       0.96      0.99      0.97        69\n",
      "          53       0.88      0.85      0.86        71\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.91      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 11,  accuracy score is 1.0\n",
      "at random state 11, confusion matrix is [[ 94   0   0 ...   0   0   0]\n",
      " [  0  75   0 ...   0   0   0]\n",
      " [  0   0 110 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  59   0   0]\n",
      " [  0   0   0 ...   1  82   0]\n",
      " [  0   1   0 ...   0   0  99]]\n",
      "at random state 11, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.92      0.91       102\n",
      "           1       0.83      0.86      0.85        87\n",
      "           2       0.99      0.97      0.98       113\n",
      "           3       0.92      0.86      0.89        95\n",
      "           4       0.97      0.97      0.97       115\n",
      "           5       0.86      0.83      0.84        88\n",
      "           6       0.99      0.97      0.98        77\n",
      "           7       0.91      0.90      0.91        93\n",
      "           8       0.96      0.96      0.96       103\n",
      "           9       0.94      0.93      0.94       118\n",
      "          10       0.81      0.87      0.84        98\n",
      "          11       0.93      0.96      0.94       107\n",
      "          12       0.94      0.97      0.95       105\n",
      "          13       0.86      0.91      0.88        99\n",
      "          14       0.88      0.92      0.90       101\n",
      "          15       0.99      0.99      0.99        79\n",
      "          16       0.99      0.95      0.97        99\n",
      "          17       0.88      0.91      0.89        93\n",
      "          18       0.87      0.88      0.88        94\n",
      "          19       0.92      0.90      0.91       100\n",
      "          20       0.97      0.96      0.96        92\n",
      "          21       0.81      0.83      0.82        92\n",
      "          22       0.96      1.00      0.98       101\n",
      "          23       0.85      0.92      0.89        93\n",
      "          24       0.69      0.75      0.72       102\n",
      "          25       1.00      0.98      0.99       102\n",
      "          26       0.86      0.78      0.82       102\n",
      "          27       0.90      0.91      0.91        91\n",
      "          28       0.94      0.97      0.95        94\n",
      "          29       0.97      0.94      0.96       109\n",
      "          30       0.97      0.97      0.97       103\n",
      "          31       0.77      0.69      0.73       108\n",
      "          32       0.96      0.87      0.91       107\n",
      "          33       0.89      0.91      0.90       112\n",
      "          34       0.98      0.96      0.97       107\n",
      "          35       0.95      0.96      0.96       100\n",
      "          36       0.83      0.83      0.83        96\n",
      "          37       0.84      0.90      0.87       105\n",
      "          38       0.89      0.90      0.90       100\n",
      "          39       0.89      0.93      0.91        89\n",
      "          40       0.91      0.94      0.92       109\n",
      "          41       0.93      0.88      0.91       111\n",
      "          42       0.93      0.99      0.96        78\n",
      "          43       0.93      0.88      0.90        97\n",
      "          44       0.92      0.92      0.92        96\n",
      "          45       0.96      0.99      0.97        91\n",
      "          46       0.97      0.96      0.96       114\n",
      "          47       0.90      0.85      0.88        87\n",
      "          48       0.87      0.83      0.85        90\n",
      "          49       0.88      0.91      0.90        91\n",
      "          50       0.66      0.64      0.65       103\n",
      "          51       0.95      1.00      0.98        59\n",
      "          52       0.98      0.99      0.98        83\n",
      "          53       0.93      0.85      0.89       116\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.91      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 12,  accuracy score is 1.0\n",
      "at random state 12, confusion matrix is [[95  0  0 ...  0  0  0]\n",
      " [ 0 80  0 ...  0  0  1]\n",
      " [ 0  0 91 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 51  0  0]\n",
      " [ 0  0  0 ...  0 80  0]\n",
      " [ 0  1  6 ...  0  0 80]]\n",
      "at random state 12, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.94      0.90       101\n",
      "           1       0.82      0.85      0.83        94\n",
      "           2       0.91      0.99      0.95        92\n",
      "           3       0.88      0.81      0.84        98\n",
      "           4       0.97      0.96      0.96        98\n",
      "           5       0.87      0.78      0.82       107\n",
      "           6       0.99      0.95      0.97        88\n",
      "           7       0.90      0.94      0.92       111\n",
      "           8       0.97      0.98      0.97        99\n",
      "           9       0.89      0.94      0.92       104\n",
      "          10       0.84      0.89      0.87       104\n",
      "          11       0.93      0.94      0.94       108\n",
      "          12       0.96      0.98      0.97       107\n",
      "          13       0.88      0.83      0.86       109\n",
      "          14       0.92      0.92      0.92       105\n",
      "          15       0.93      0.96      0.95        83\n",
      "          16       0.94      0.96      0.95       102\n",
      "          17       0.93      0.96      0.94        89\n",
      "          18       0.93      0.94      0.93        96\n",
      "          19       0.81      0.93      0.87       100\n",
      "          20       0.97      0.91      0.94        98\n",
      "          21       0.82      0.87      0.85        92\n",
      "          22       0.98      0.99      0.99       101\n",
      "          23       0.84      0.89      0.86       109\n",
      "          24       0.76      0.81      0.79       104\n",
      "          25       0.96      0.96      0.96        99\n",
      "          26       0.92      0.83      0.87       110\n",
      "          27       0.93      0.92      0.93       114\n",
      "          28       0.97      0.99      0.98        94\n",
      "          29       1.00      0.92      0.96       105\n",
      "          30       0.97      0.95      0.96       103\n",
      "          31       0.73      0.83      0.78       105\n",
      "          32       0.95      0.86      0.90       102\n",
      "          33       0.94      0.93      0.93       110\n",
      "          34       0.95      0.92      0.93       101\n",
      "          35       0.92      0.95      0.94       102\n",
      "          36       0.80      0.89      0.84        88\n",
      "          37       0.91      0.85      0.88       106\n",
      "          38       0.84      0.91      0.87        88\n",
      "          39       0.91      0.84      0.88        89\n",
      "          40       0.91      0.94      0.92        96\n",
      "          41       0.84      0.94      0.89        98\n",
      "          42       0.96      0.98      0.97        92\n",
      "          43       0.94      0.87      0.90        97\n",
      "          44       0.86      0.89      0.88       103\n",
      "          45       0.92      0.99      0.95        72\n",
      "          46       0.98      0.97      0.97        94\n",
      "          47       0.91      0.80      0.85       103\n",
      "          48       0.87      0.82      0.85       100\n",
      "          49       0.91      0.88      0.89        97\n",
      "          50       0.69      0.60      0.64        91\n",
      "          51       1.00      1.00      1.00        51\n",
      "          52       0.98      0.99      0.98        81\n",
      "          53       0.91      0.75      0.82       106\n",
      "\n",
      "    accuracy                           0.90      5296\n",
      "   macro avg       0.91      0.91      0.90      5296\n",
      "weighted avg       0.90      0.90      0.90      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 13,  accuracy score is 1.0\n",
      "at random state 13, confusion matrix is [[103   0   0 ...   0   0   0]\n",
      " [  0  80   0 ...   0   0   1]\n",
      " [  0   0 111 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  45   0   0]\n",
      " [  0   0   0 ...   1  74   0]\n",
      " [  0   0   2 ...   0   0  82]]\n",
      "at random state 13, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.92      0.91       112\n",
      "           1       0.81      0.81      0.81        99\n",
      "           2       0.97      0.96      0.97       116\n",
      "           3       0.93      0.87      0.90       115\n",
      "           4       0.98      0.97      0.97        98\n",
      "           5       0.78      0.86      0.82        84\n",
      "           6       0.97      1.00      0.99       102\n",
      "           7       0.93      0.90      0.91        88\n",
      "           8       0.99      0.97      0.98       116\n",
      "           9       0.91      0.91      0.91        99\n",
      "          10       0.89      0.90      0.90       121\n",
      "          11       0.92      0.94      0.93        87\n",
      "          12       0.93      0.98      0.95        86\n",
      "          13       0.89      0.89      0.89        95\n",
      "          14       0.92      0.95      0.93        93\n",
      "          15       0.99      0.96      0.97        79\n",
      "          16       0.98      0.92      0.95        93\n",
      "          17       0.94      0.96      0.95       113\n",
      "          18       0.89      0.88      0.89        92\n",
      "          19       0.90      0.90      0.90       104\n",
      "          20       0.98      0.91      0.94        95\n",
      "          21       0.84      0.83      0.84       115\n",
      "          22       0.98      0.99      0.99       110\n",
      "          23       0.79      0.87      0.83       101\n",
      "          24       0.74      0.72      0.73        98\n",
      "          25       0.95      0.95      0.95       105\n",
      "          26       0.83      0.88      0.86        94\n",
      "          27       0.88      0.92      0.90       103\n",
      "          28       0.94      0.93      0.94       101\n",
      "          29       0.97      0.96      0.96        94\n",
      "          30       0.98      0.98      0.98        96\n",
      "          31       0.77      0.80      0.79       101\n",
      "          32       0.92      0.89      0.90       106\n",
      "          33       0.84      0.96      0.90        94\n",
      "          34       0.96      0.93      0.94        94\n",
      "          35       0.96      0.95      0.95       100\n",
      "          36       0.82      0.84      0.83        90\n",
      "          37       0.84      0.93      0.88        97\n",
      "          38       0.86      0.92      0.89       110\n",
      "          39       0.89      0.93      0.91       104\n",
      "          40       0.88      0.94      0.91       109\n",
      "          41       0.88      0.92      0.90        98\n",
      "          42       0.96      0.99      0.98        80\n",
      "          43       0.91      0.84      0.88        96\n",
      "          44       0.94      0.86      0.90       109\n",
      "          45       0.96      0.99      0.97        79\n",
      "          46       0.99      0.97      0.98        78\n",
      "          47       0.97      0.82      0.89       108\n",
      "          48       0.96      0.83      0.89       115\n",
      "          49       0.89      0.84      0.87       100\n",
      "          50       0.69      0.66      0.67       109\n",
      "          51       0.98      1.00      0.99        45\n",
      "          52       1.00      0.97      0.99        76\n",
      "          53       0.93      0.87      0.90        94\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.91      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 14,  accuracy score is 1.0\n",
      "at random state 14, confusion matrix is [[102   0   0 ...   0   0   0]\n",
      " [  0 101   0 ...   0   0   0]\n",
      " [  0   0  90 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  57   0   0]\n",
      " [  0   0   0 ...   0  70   0]\n",
      " [  0   0   4 ...   0   0  80]]\n",
      "at random state 14, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.93      0.90       110\n",
      "           1       0.82      0.91      0.86       111\n",
      "           2       0.92      0.99      0.95        91\n",
      "           3       0.91      0.85      0.88        94\n",
      "           4       0.98      0.96      0.97       108\n",
      "           5       0.86      0.80      0.83        96\n",
      "           6       0.98      1.00      0.99        92\n",
      "           7       0.90      0.88      0.89        85\n",
      "           8       0.98      0.98      0.98       100\n",
      "           9       0.86      0.93      0.89       100\n",
      "          10       0.84      0.82      0.83       105\n",
      "          11       0.95      0.92      0.93        98\n",
      "          12       0.93      0.96      0.95        85\n",
      "          13       0.90      0.90      0.90       102\n",
      "          14       0.94      0.93      0.94       101\n",
      "          15       0.99      0.99      0.99        86\n",
      "          16       0.93      0.95      0.94        88\n",
      "          17       0.93      0.90      0.92        94\n",
      "          18       0.92      0.95      0.93       102\n",
      "          19       0.88      0.95      0.92       108\n",
      "          20       0.98      0.88      0.93        95\n",
      "          21       0.78      0.82      0.80        93\n",
      "          22       0.99      0.98      0.99       105\n",
      "          23       0.86      0.92      0.89        97\n",
      "          24       0.79      0.73      0.76       104\n",
      "          25       0.98      0.99      0.98        94\n",
      "          26       0.86      0.81      0.83       114\n",
      "          27       0.91      0.92      0.91        97\n",
      "          28       0.97      0.95      0.96       106\n",
      "          29       0.96      0.97      0.96        99\n",
      "          30       0.97      0.96      0.96        95\n",
      "          31       0.72      0.84      0.77       111\n",
      "          32       0.93      0.94      0.93        96\n",
      "          33       0.88      0.85      0.86       104\n",
      "          34       0.96      0.93      0.94        97\n",
      "          35       0.95      0.94      0.95       104\n",
      "          36       0.88      0.86      0.87       107\n",
      "          37       0.91      0.93      0.92       123\n",
      "          38       0.90      0.93      0.91       101\n",
      "          39       0.95      0.93      0.94       102\n",
      "          40       0.92      0.96      0.94       113\n",
      "          41       0.89      0.89      0.89        93\n",
      "          42       0.94      0.96      0.95        91\n",
      "          43       0.89      0.87      0.88       101\n",
      "          44       0.87      0.90      0.89       104\n",
      "          45       0.95      0.99      0.97        78\n",
      "          46       0.98      0.93      0.95        98\n",
      "          47       0.90      0.83      0.86        96\n",
      "          48       0.92      0.87      0.89        98\n",
      "          49       0.87      0.86      0.87        96\n",
      "          50       0.66      0.60      0.63       101\n",
      "          51       1.00      1.00      1.00        57\n",
      "          52       0.99      1.00      0.99        70\n",
      "          53       0.91      0.80      0.85       100\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.91      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 15,  accuracy score is 1.0\n",
      "at random state 15, confusion matrix is [[ 74   0   0 ...   0   0   0]\n",
      " [  0  89   0 ...   0   0   0]\n",
      " [  0   0 104 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  48   0   0]\n",
      " [  0   0   0 ...   0  86   0]\n",
      " [  0   1   1 ...   0   0  87]]\n",
      "at random state 15, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.87        87\n",
      "           1       0.91      0.82      0.86       108\n",
      "           2       0.97      1.00      0.99       104\n",
      "           3       0.93      0.83      0.88        98\n",
      "           4       0.94      0.95      0.94        92\n",
      "           5       0.88      0.79      0.83       105\n",
      "           6       1.00      0.99      0.99        94\n",
      "           7       0.91      0.89      0.90       103\n",
      "           8       0.99      0.97      0.98       113\n",
      "           9       0.91      0.92      0.91       109\n",
      "          10       0.79      0.88      0.83        96\n",
      "          11       0.96      0.95      0.96       108\n",
      "          12       0.96      0.93      0.95       118\n",
      "          13       0.88      0.91      0.90       105\n",
      "          14       0.87      0.96      0.91        93\n",
      "          15       0.96      0.99      0.97        75\n",
      "          16       0.90      0.97      0.93        91\n",
      "          17       0.91      0.91      0.91        94\n",
      "          18       0.91      0.95      0.93        94\n",
      "          19       0.89      0.92      0.90        95\n",
      "          20       0.94      0.94      0.94        93\n",
      "          21       0.81      0.83      0.82       104\n",
      "          22       0.97      1.00      0.98        97\n",
      "          23       0.85      0.89      0.87       111\n",
      "          24       0.69      0.64      0.67        90\n",
      "          25       0.98      0.99      0.98        98\n",
      "          26       0.81      0.74      0.77       100\n",
      "          27       0.88      0.91      0.89        97\n",
      "          28       0.93      0.97      0.95        98\n",
      "          29       0.99      0.97      0.98       106\n",
      "          30       0.96      0.97      0.96        95\n",
      "          31       0.77      0.74      0.75       106\n",
      "          32       0.97      0.83      0.90       115\n",
      "          33       0.94      0.97      0.96       110\n",
      "          34       0.98      0.93      0.95       109\n",
      "          35       0.96      0.97      0.97       105\n",
      "          36       0.79      0.92      0.85        97\n",
      "          37       0.83      0.92      0.87        88\n",
      "          38       0.91      0.89      0.90       109\n",
      "          39       0.94      0.94      0.94        89\n",
      "          40       0.91      0.94      0.93       102\n",
      "          41       0.90      0.98      0.94        96\n",
      "          42       0.96      0.96      0.96        84\n",
      "          43       0.93      0.80      0.86       112\n",
      "          44       0.89      0.89      0.89       103\n",
      "          45       1.00      0.99      0.99        82\n",
      "          46       0.98      0.96      0.97        93\n",
      "          47       0.90      0.90      0.90        92\n",
      "          48       0.89      0.80      0.84        99\n",
      "          49       0.79      0.95      0.87        98\n",
      "          50       0.58      0.62      0.60        95\n",
      "          51       1.00      1.00      1.00        48\n",
      "          52       0.98      1.00      0.99        86\n",
      "          53       0.95      0.81      0.87       107\n",
      "\n",
      "    accuracy                           0.90      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.90      0.90      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 16,  accuracy score is 1.0\n",
      "at random state 16, confusion matrix is [[ 77   0   0 ...   0   0   0]\n",
      " [  0  79   0 ...   0   0   0]\n",
      " [  0   0 103 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  53   0   0]\n",
      " [  0   0   0 ...   0  77   0]\n",
      " [  0   0   1 ...   0   0  83]]\n",
      "at random state 16, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.94      0.85        82\n",
      "           1       0.79      0.89      0.84        89\n",
      "           2       0.96      0.98      0.97       105\n",
      "           3       0.90      0.82      0.86        84\n",
      "           4       0.98      0.99      0.98        99\n",
      "           5       0.86      0.78      0.82       114\n",
      "           6       0.98      0.96      0.97        83\n",
      "           7       0.89      0.92      0.90        96\n",
      "           8       0.94      0.96      0.95        98\n",
      "           9       0.90      0.93      0.91       111\n",
      "          10       0.87      0.86      0.86       112\n",
      "          11       0.95      0.93      0.94        97\n",
      "          12       0.97      0.94      0.96       116\n",
      "          13       0.88      0.88      0.88       105\n",
      "          14       0.88      0.90      0.89        93\n",
      "          15       1.00      1.00      1.00        85\n",
      "          16       0.90      0.93      0.92        88\n",
      "          17       0.92      0.93      0.93        90\n",
      "          18       0.92      0.97      0.94        93\n",
      "          19       0.90      0.89      0.90       104\n",
      "          20       0.96      0.96      0.96       104\n",
      "          21       0.89      0.83      0.86       115\n",
      "          22       0.96      1.00      0.98       110\n",
      "          23       0.84      0.85      0.85       108\n",
      "          24       0.78      0.68      0.73       103\n",
      "          25       0.99      1.00      1.00       101\n",
      "          26       0.95      0.81      0.88       113\n",
      "          27       0.86      0.88      0.87        94\n",
      "          28       0.98      0.97      0.98       114\n",
      "          29       0.99      0.97      0.98        88\n",
      "          30       1.00      0.99      0.99        87\n",
      "          31       0.77      0.70      0.73        99\n",
      "          32       0.98      0.90      0.94       109\n",
      "          33       0.89      0.98      0.93        81\n",
      "          34       0.94      0.94      0.94       107\n",
      "          35       0.97      0.97      0.97       106\n",
      "          36       0.86      0.81      0.83        98\n",
      "          37       0.86      0.88      0.87       115\n",
      "          38       0.92      0.93      0.92       107\n",
      "          39       0.96      0.92      0.94       101\n",
      "          40       0.91      0.91      0.91        95\n",
      "          41       0.83      0.90      0.86       102\n",
      "          42       0.91      0.94      0.93        88\n",
      "          43       0.87      0.94      0.90        78\n",
      "          44       0.94      0.95      0.94       110\n",
      "          45       0.97      0.99      0.98        90\n",
      "          46       0.98      0.96      0.97        92\n",
      "          47       0.88      0.91      0.90        90\n",
      "          48       0.95      0.90      0.93       114\n",
      "          49       0.90      0.89      0.90       111\n",
      "          50       0.58      0.66      0.62        95\n",
      "          51       0.96      1.00      0.98        53\n",
      "          52       0.99      1.00      0.99        77\n",
      "          53       0.91      0.86      0.88        97\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.91      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 17,  accuracy score is 1.0\n",
      "at random state 17, confusion matrix is [[101   0   0 ...   0   0   0]\n",
      " [  0  78   0 ...   0   0   0]\n",
      " [  0   0 108 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  60   0   0]\n",
      " [  0   0   0 ...   0  63   0]\n",
      " [  0   0   2 ...   0   0  87]]\n",
      "at random state 17, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.90      0.89       112\n",
      "           1       0.80      0.81      0.80        96\n",
      "           2       0.94      0.96      0.95       113\n",
      "           3       0.88      0.88      0.88        86\n",
      "           4       0.95      0.98      0.96        95\n",
      "           5       0.86      0.73      0.79       104\n",
      "           6       0.96      0.99      0.97        92\n",
      "           7       0.89      0.93      0.91        99\n",
      "           8       0.97      0.98      0.98       105\n",
      "           9       0.85      0.96      0.90        91\n",
      "          10       0.87      0.84      0.85       107\n",
      "          11       0.93      0.95      0.94       107\n",
      "          12       0.94      0.99      0.96       102\n",
      "          13       0.89      0.88      0.88        98\n",
      "          14       0.92      0.92      0.92       116\n",
      "          15       0.98      0.99      0.98        88\n",
      "          16       0.94      0.95      0.95       109\n",
      "          17       0.91      0.92      0.92        90\n",
      "          18       0.90      0.90      0.90        90\n",
      "          19       0.91      0.93      0.92       110\n",
      "          20       0.93      0.90      0.92       104\n",
      "          21       0.79      0.70      0.74        96\n",
      "          22       0.95      0.98      0.97        99\n",
      "          23       0.88      0.87      0.88       105\n",
      "          24       0.75      0.74      0.75        90\n",
      "          25       0.99      0.99      0.99        93\n",
      "          26       0.82      0.86      0.84        97\n",
      "          27       0.91      0.92      0.92       105\n",
      "          28       0.96      0.94      0.95        97\n",
      "          29       1.00      0.98      0.99        88\n",
      "          30       0.99      0.99      0.99       107\n",
      "          31       0.75      0.71      0.73       101\n",
      "          32       0.91      0.87      0.89        94\n",
      "          33       0.89      0.88      0.88        91\n",
      "          34       0.97      0.83      0.90       124\n",
      "          35       0.93      0.95      0.94       105\n",
      "          36       0.83      0.80      0.82       100\n",
      "          37       0.90      0.91      0.90        96\n",
      "          38       0.89      0.93      0.91       109\n",
      "          39       0.93      0.92      0.92       108\n",
      "          40       0.86      0.94      0.90        97\n",
      "          41       0.89      0.91      0.90       105\n",
      "          42       0.96      0.98      0.97        89\n",
      "          43       0.96      0.90      0.93        97\n",
      "          44       0.92      0.88      0.90        95\n",
      "          45       0.99      0.99      0.99        83\n",
      "          46       0.99      0.99      0.99        91\n",
      "          47       0.88      0.88      0.88        88\n",
      "          48       0.89      0.83      0.86       105\n",
      "          49       0.85      0.91      0.88       107\n",
      "          50       0.58      0.68      0.62        90\n",
      "          51       0.98      1.00      0.99        60\n",
      "          52       1.00      0.98      0.99        64\n",
      "          53       0.88      0.82      0.85       106\n",
      "\n",
      "    accuracy                           0.90      5296\n",
      "   macro avg       0.90      0.90      0.90      5296\n",
      "weighted avg       0.90      0.90      0.90      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 18,  accuracy score is 1.0\n",
      "at random state 18, confusion matrix is [[ 87   0   0 ...   0   0   0]\n",
      " [  0  91   0 ...   0   0   0]\n",
      " [  0   0 104 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  54   0   0]\n",
      " [  0   0   0 ...   1  63   0]\n",
      " [  0   0   2 ...   0   0  72]]\n",
      "at random state 18, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.88      0.87        99\n",
      "           1       0.84      0.88      0.86       104\n",
      "           2       0.98      0.97      0.98       107\n",
      "           3       0.87      0.82      0.84        88\n",
      "           4       0.97      0.97      0.97        90\n",
      "           5       0.85      0.78      0.82        97\n",
      "           6       0.99      0.97      0.98        99\n",
      "           7       0.90      0.95      0.93       104\n",
      "           8       0.98      0.97      0.97        86\n",
      "           9       0.94      0.87      0.90       103\n",
      "          10       0.86      0.89      0.87        98\n",
      "          11       0.94      0.96      0.95       106\n",
      "          12       0.95      0.97      0.96       109\n",
      "          13       0.90      0.87      0.88       101\n",
      "          14       0.89      0.96      0.92        93\n",
      "          15       0.95      1.00      0.98       104\n",
      "          16       0.96      0.97      0.97       102\n",
      "          17       0.91      0.94      0.93       102\n",
      "          18       0.90      0.93      0.92        89\n",
      "          19       0.89      0.91      0.90       116\n",
      "          20       0.99      0.97      0.98       100\n",
      "          21       0.81      0.87      0.84        93\n",
      "          22       1.00      0.99      0.99        88\n",
      "          23       0.84      0.89      0.86       109\n",
      "          24       0.78      0.74      0.76       109\n",
      "          25       0.99      0.98      0.99       111\n",
      "          26       0.79      0.87      0.83        98\n",
      "          27       0.89      0.89      0.89       117\n",
      "          28       0.97      0.94      0.96       106\n",
      "          29       0.99      0.97      0.98        86\n",
      "          30       0.96      0.97      0.96        98\n",
      "          31       0.82      0.78      0.80       100\n",
      "          32       0.90      0.96      0.92        90\n",
      "          33       0.87      0.93      0.90        95\n",
      "          34       0.95      0.98      0.97       102\n",
      "          35       0.92      0.99      0.95       100\n",
      "          36       0.86      0.87      0.87       110\n",
      "          37       0.88      0.88      0.88        96\n",
      "          38       0.90      0.96      0.93        99\n",
      "          39       0.97      0.91      0.94        97\n",
      "          40       0.95      0.94      0.94       111\n",
      "          41       0.88      0.92      0.90       105\n",
      "          42       0.97      0.97      0.97        96\n",
      "          43       0.95      0.88      0.91       105\n",
      "          44       0.96      0.91      0.93       100\n",
      "          45       0.96      1.00      0.98        85\n",
      "          46       1.00      0.99      0.99        85\n",
      "          47       0.95      0.88      0.92       104\n",
      "          48       0.88      0.79      0.83        99\n",
      "          49       0.85      0.87      0.86        94\n",
      "          50       0.66      0.63      0.65        93\n",
      "          51       0.98      1.00      0.99        54\n",
      "          52       0.98      0.97      0.98        65\n",
      "          53       0.87      0.73      0.79        99\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.91      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 19,  accuracy score is 1.0\n",
      "at random state 19, confusion matrix is [[ 95   0   0 ...   0   0   0]\n",
      " [  0  88   0 ...   0   0   0]\n",
      " [  0   0 108 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  47   0   0]\n",
      " [  0   0   0 ...   2  82   0]\n",
      " [  0   1   3 ...   0   0  81]]\n",
      "at random state 19, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.87      0.88       109\n",
      "           1       0.81      0.82      0.81       107\n",
      "           2       0.96      0.99      0.97       109\n",
      "           3       0.91      0.85      0.88       124\n",
      "           4       0.95      0.93      0.94        97\n",
      "           5       0.85      0.85      0.85       100\n",
      "           6       0.94      0.99      0.97        86\n",
      "           7       0.90      0.93      0.91        95\n",
      "           8       0.98      0.99      0.99       101\n",
      "           9       0.88      0.94      0.91        97\n",
      "          10       0.86      0.87      0.87        86\n",
      "          11       0.89      0.95      0.92       105\n",
      "          12       0.99      0.95      0.97       113\n",
      "          13       0.89      0.83      0.86       102\n",
      "          14       0.89      0.94      0.91        93\n",
      "          15       0.97      0.97      0.97        78\n",
      "          16       0.96      0.97      0.97       104\n",
      "          17       0.96      0.90      0.93       109\n",
      "          18       0.92      0.90      0.91       111\n",
      "          19       0.89      0.87      0.88        98\n",
      "          20       0.96      0.93      0.94        96\n",
      "          21       0.83      0.83      0.83       103\n",
      "          22       0.97      0.99      0.98        91\n",
      "          23       0.80      0.84      0.82        88\n",
      "          24       0.79      0.73      0.76       122\n",
      "          25       1.00      1.00      1.00        87\n",
      "          26       0.89      0.85      0.87       109\n",
      "          27       0.93      0.94      0.94       105\n",
      "          28       0.93      0.97      0.95        92\n",
      "          29       0.99      0.96      0.97        94\n",
      "          30       0.96      0.94      0.95       103\n",
      "          31       0.71      0.85      0.77        98\n",
      "          32       0.91      0.88      0.89       121\n",
      "          33       0.86      0.96      0.91        92\n",
      "          34       0.94      0.95      0.95        83\n",
      "          35       0.97      0.97      0.97        98\n",
      "          36       0.91      0.82      0.86       107\n",
      "          37       0.88      0.86      0.87       103\n",
      "          38       0.87      0.96      0.91        94\n",
      "          39       0.91      0.93      0.92        92\n",
      "          40       0.87      0.91      0.89        99\n",
      "          41       0.90      0.87      0.88       100\n",
      "          42       0.91      0.98      0.95        88\n",
      "          43       0.84      0.88      0.86        91\n",
      "          44       0.90      0.86      0.88       101\n",
      "          45       0.96      0.98      0.97        92\n",
      "          46       0.98      0.98      0.98        89\n",
      "          47       0.91      0.88      0.90        93\n",
      "          48       0.80      0.83      0.82        89\n",
      "          49       0.79      0.88      0.83        92\n",
      "          50       0.74      0.60      0.66       125\n",
      "          51       0.92      1.00      0.96        47\n",
      "          52       0.98      0.95      0.96        86\n",
      "          53       0.88      0.79      0.84       102\n",
      "\n",
      "    accuracy                           0.90      5296\n",
      "   macro avg       0.90      0.90      0.90      5296\n",
      "weighted avg       0.90      0.90      0.90      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 20,  accuracy score is 1.0\n",
      "at random state 20, confusion matrix is [[ 78   0   0 ...   0   0   0]\n",
      " [  0  81   0 ...   0   0   0]\n",
      " [  0   0 105 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  53   0   0]\n",
      " [  0   0   0 ...   0  69   0]\n",
      " [  0   0   2 ...   0   0  78]]\n",
      "at random state 20, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.94      0.90        83\n",
      "           1       0.83      0.88      0.85        92\n",
      "           2       0.95      0.97      0.96       108\n",
      "           3       0.91      0.88      0.89       112\n",
      "           4       0.97      0.98      0.98       102\n",
      "           5       0.93      0.80      0.86       114\n",
      "           6       1.00      0.95      0.98        87\n",
      "           7       0.85      0.93      0.88        95\n",
      "           8       0.97      0.97      0.97       119\n",
      "           9       0.86      0.92      0.89       100\n",
      "          10       0.94      0.88      0.91       116\n",
      "          11       0.97      0.95      0.96       111\n",
      "          12       0.93      0.98      0.95        85\n",
      "          13       0.91      0.82      0.86       106\n",
      "          14       0.93      0.92      0.93       106\n",
      "          15       0.98      0.98      0.98        94\n",
      "          16       0.92      0.96      0.94        93\n",
      "          17       0.91      0.87      0.89       118\n",
      "          18       0.93      0.94      0.93        97\n",
      "          19       0.85      0.90      0.88        91\n",
      "          20       0.99      0.92      0.95       108\n",
      "          21       0.86      0.76      0.81        97\n",
      "          22       0.97      1.00      0.99       107\n",
      "          23       0.84      0.91      0.87        87\n",
      "          24       0.74      0.73      0.73        92\n",
      "          25       0.95      0.98      0.96        91\n",
      "          26       0.80      0.85      0.82        96\n",
      "          27       0.95      0.89      0.92       102\n",
      "          28       0.94      0.98      0.96       105\n",
      "          29       1.00      0.95      0.97       111\n",
      "          30       0.98      0.98      0.98       101\n",
      "          31       0.77      0.74      0.76       100\n",
      "          32       0.90      0.86      0.88       107\n",
      "          33       0.87      0.93      0.90       107\n",
      "          34       0.96      0.93      0.94        82\n",
      "          35       0.95      0.94      0.95        86\n",
      "          36       0.92      0.92      0.92       106\n",
      "          37       0.88      0.79      0.83       101\n",
      "          38       0.85      0.92      0.88        86\n",
      "          39       0.89      0.93      0.91        97\n",
      "          40       0.90      0.92      0.91       106\n",
      "          41       0.87      0.91      0.89        95\n",
      "          42       0.94      0.94      0.94        87\n",
      "          43       0.94      0.88      0.91        99\n",
      "          44       0.90      0.91      0.90        99\n",
      "          45       0.96      0.99      0.97        93\n",
      "          46       0.97      0.96      0.96        91\n",
      "          47       0.93      0.86      0.90        96\n",
      "          48       0.90      0.87      0.88       106\n",
      "          49       0.83      0.94      0.88       109\n",
      "          50       0.59      0.66      0.62        98\n",
      "          51       0.98      1.00      0.99        53\n",
      "          52       0.96      0.99      0.97        70\n",
      "          53       0.90      0.81      0.85        96\n",
      "\n",
      "    accuracy                           0.90      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.90      0.90      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 21,  accuracy score is 1.0\n",
      "at random state 21, confusion matrix is [[84  0  0 ...  0  0  0]\n",
      " [ 0 80  0 ...  0  0  0]\n",
      " [ 0  0 92 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 53  0  0]\n",
      " [ 0  0  0 ...  1 76  0]\n",
      " [ 0  0  0 ...  0  0 84]]\n",
      "at random state 21, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.90      0.88        93\n",
      "           1       0.87      0.88      0.87        91\n",
      "           2       0.94      0.99      0.96        93\n",
      "           3       0.91      0.88      0.89        89\n",
      "           4       0.97      0.94      0.95        99\n",
      "           5       0.87      0.84      0.85       108\n",
      "           6       0.96      0.99      0.97        79\n",
      "           7       0.87      0.91      0.89       110\n",
      "           8       0.95      1.00      0.97       115\n",
      "           9       0.93      0.93      0.93       101\n",
      "          10       0.82      0.87      0.84        92\n",
      "          11       0.93      0.95      0.94       104\n",
      "          12       0.93      0.95      0.94        97\n",
      "          13       0.91      0.85      0.88       107\n",
      "          14       0.90      0.94      0.92       118\n",
      "          15       0.99      0.98      0.98        92\n",
      "          16       0.98      0.94      0.96        97\n",
      "          17       0.96      0.91      0.93        96\n",
      "          18       0.94      0.92      0.93        98\n",
      "          19       0.90      0.92      0.91       109\n",
      "          20       0.97      0.95      0.96       103\n",
      "          21       0.85      0.90      0.87        98\n",
      "          22       0.99      0.98      0.98        95\n",
      "          23       0.87      0.89      0.88       108\n",
      "          24       0.83      0.74      0.78       105\n",
      "          25       0.99      1.00      1.00       100\n",
      "          26       0.78      0.76      0.77        99\n",
      "          27       0.90      0.90      0.90        99\n",
      "          28       0.99      0.93      0.96       102\n",
      "          29       0.99      0.98      0.98        89\n",
      "          30       0.97      0.97      0.97        95\n",
      "          31       0.75      0.72      0.73       103\n",
      "          32       0.93      0.89      0.91        93\n",
      "          33       0.92      0.93      0.92        99\n",
      "          34       0.95      0.96      0.95       118\n",
      "          35       0.95      0.98      0.97       115\n",
      "          36       0.89      0.90      0.89        97\n",
      "          37       0.91      0.83      0.87       100\n",
      "          38       0.85      0.94      0.90        86\n",
      "          39       0.93      0.89      0.91       115\n",
      "          40       0.92      0.95      0.93       102\n",
      "          41       0.92      0.93      0.93       107\n",
      "          42       0.99      0.97      0.98        89\n",
      "          43       0.91      0.85      0.88        96\n",
      "          44       0.89      0.88      0.89        75\n",
      "          45       0.99      0.99      0.99        93\n",
      "          46       0.93      0.99      0.96       100\n",
      "          47       0.93      0.87      0.90       108\n",
      "          48       0.86      0.84      0.85        86\n",
      "          49       0.88      0.92      0.90       110\n",
      "          50       0.61      0.67      0.64        97\n",
      "          51       0.98      1.00      0.99        53\n",
      "          52       0.99      0.99      0.99        77\n",
      "          53       0.88      0.88      0.88        96\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.91      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 22,  accuracy score is 1.0\n",
      "at random state 22, confusion matrix is [[88  0  0 ...  0  0  0]\n",
      " [ 0 98  0 ...  0  0  0]\n",
      " [ 0  0 94 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 48  0  0]\n",
      " [ 0  0  0 ...  0 80  0]\n",
      " [ 0  0  1 ...  0  0 82]]\n",
      "at random state 22, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.96      0.93        92\n",
      "           1       0.82      0.86      0.84       114\n",
      "           2       0.97      0.97      0.97        97\n",
      "           3       0.92      0.90      0.91        94\n",
      "           4       0.95      1.00      0.98       101\n",
      "           5       0.89      0.75      0.81       102\n",
      "           6       0.99      0.99      0.99        84\n",
      "           7       0.87      0.90      0.88        97\n",
      "           8       0.96      0.98      0.97       107\n",
      "           9       0.90      0.89      0.90       103\n",
      "          10       0.87      0.90      0.89       103\n",
      "          11       0.96      0.95      0.96       109\n",
      "          12       0.95      0.98      0.97       100\n",
      "          13       0.85      0.90      0.87        96\n",
      "          14       0.86      0.89      0.88       110\n",
      "          15       0.99      0.99      0.99        78\n",
      "          16       0.96      0.96      0.96       105\n",
      "          17       0.93      0.89      0.91        98\n",
      "          18       0.91      0.96      0.93       100\n",
      "          19       0.95      0.87      0.91       102\n",
      "          20       1.00      0.97      0.99       107\n",
      "          21       0.84      0.86      0.85       100\n",
      "          22       0.96      1.00      0.98        88\n",
      "          23       0.88      0.88      0.88       102\n",
      "          24       0.79      0.78      0.79        93\n",
      "          25       0.98      0.99      0.98        94\n",
      "          26       0.82      0.82      0.82       108\n",
      "          27       0.95      0.93      0.94       112\n",
      "          28       0.96      0.96      0.96       103\n",
      "          29       0.98      0.97      0.97        92\n",
      "          30       0.98      0.96      0.97       114\n",
      "          31       0.82      0.77      0.80       101\n",
      "          32       0.91      0.93      0.92        96\n",
      "          33       0.87      0.89      0.88        94\n",
      "          34       0.94      0.95      0.95       100\n",
      "          35       0.97      0.99      0.98        97\n",
      "          36       0.87      0.82      0.85        90\n",
      "          37       0.87      0.96      0.91       111\n",
      "          38       0.93      0.87      0.90       111\n",
      "          39       0.95      0.95      0.95        96\n",
      "          40       0.89      0.91      0.90       104\n",
      "          41       0.90      0.86      0.88        97\n",
      "          42       0.92      0.96      0.94        93\n",
      "          43       0.93      0.92      0.93       101\n",
      "          44       0.89      0.85      0.87        88\n",
      "          45       0.98      0.98      0.98        85\n",
      "          46       0.98      0.98      0.98       103\n",
      "          47       0.93      0.86      0.89       105\n",
      "          48       0.87      0.86      0.86       107\n",
      "          49       0.83      0.89      0.86       101\n",
      "          50       0.60      0.61      0.60        85\n",
      "          51       1.00      1.00      1.00        48\n",
      "          52       0.99      1.00      0.99        80\n",
      "          53       0.93      0.84      0.88        98\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.91      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 23,  accuracy score is 1.0\n",
      "at random state 23, confusion matrix is [[96  0  0 ...  0  0  0]\n",
      " [ 0 86  0 ...  0  0  0]\n",
      " [ 0  0 93 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 51  0  0]\n",
      " [ 0  0  0 ...  1 87  0]\n",
      " [ 0  0  1 ...  0  0 99]]\n",
      "at random state 23, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.91      0.89       106\n",
      "           1       0.84      0.91      0.87        95\n",
      "           2       0.98      0.98      0.98        95\n",
      "           3       0.90      0.84      0.87       110\n",
      "           4       0.90      0.98      0.94        94\n",
      "           5       0.90      0.86      0.88       100\n",
      "           6       0.94      0.98      0.96        85\n",
      "           7       0.89      0.93      0.91        94\n",
      "           8       0.96      0.98      0.97       124\n",
      "           9       0.89      0.89      0.89        89\n",
      "          10       0.81      0.86      0.83       111\n",
      "          11       0.94      0.95      0.94       106\n",
      "          12       0.92      0.98      0.95       103\n",
      "          13       0.90      0.84      0.87       105\n",
      "          14       0.90      0.95      0.92        81\n",
      "          15       0.95      0.99      0.97        73\n",
      "          16       0.95      0.95      0.95       106\n",
      "          17       0.94      0.90      0.92        92\n",
      "          18       0.92      0.92      0.92       106\n",
      "          19       0.87      0.93      0.90       101\n",
      "          20       0.98      0.94      0.96       101\n",
      "          21       0.71      0.85      0.78        88\n",
      "          22       0.98      0.99      0.99       106\n",
      "          23       0.86      0.87      0.87       102\n",
      "          24       0.73      0.69      0.71       108\n",
      "          25       0.99      0.94      0.97       104\n",
      "          26       0.80      0.81      0.81       108\n",
      "          27       0.84      0.89      0.87        94\n",
      "          28       0.94      0.99      0.96        92\n",
      "          29       1.00      0.98      0.99        99\n",
      "          30       0.98      0.93      0.96       105\n",
      "          31       0.73      0.81      0.76       103\n",
      "          32       0.97      0.87      0.92       102\n",
      "          33       0.85      0.92      0.88       100\n",
      "          34       0.97      0.95      0.96       100\n",
      "          35       0.91      0.98      0.95        97\n",
      "          36       0.91      0.86      0.88       104\n",
      "          37       0.85      0.86      0.86        94\n",
      "          38       0.90      0.90      0.90       106\n",
      "          39       0.91      0.86      0.88       101\n",
      "          40       0.95      0.94      0.94        93\n",
      "          41       0.85      0.90      0.87        96\n",
      "          42       0.96      0.95      0.95        95\n",
      "          43       0.88      0.85      0.87        81\n",
      "          44       0.98      0.89      0.93        89\n",
      "          45       0.96      0.98      0.97        93\n",
      "          46       0.99      0.97      0.98       105\n",
      "          47       0.93      0.84      0.88       100\n",
      "          48       0.94      0.88      0.91        83\n",
      "          49       0.92      0.90      0.91       103\n",
      "          50       0.67      0.59      0.63       112\n",
      "          51       0.98      1.00      0.99        51\n",
      "          52       1.00      0.99      0.99        88\n",
      "          53       0.93      0.85      0.89       117\n",
      "\n",
      "    accuracy                           0.90      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.90      0.90      0.90      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 24,  accuracy score is 1.0\n",
      "at random state 24, confusion matrix is [[102   0   0 ...   0   0   0]\n",
      " [  0  77   0 ...   0   0   2]\n",
      " [  0   0 102 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  58   0   0]\n",
      " [  0   0   0 ...   1  69   0]\n",
      " [  0   0   1 ...   0   0  79]]\n",
      "at random state 24, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.93      0.88       110\n",
      "           1       0.74      0.86      0.79        90\n",
      "           2       0.97      1.00      0.99       102\n",
      "           3       0.95      0.85      0.90       103\n",
      "           4       0.96      0.97      0.97       112\n",
      "           5       0.81      0.78      0.80       111\n",
      "           6       0.98      0.97      0.97       100\n",
      "           7       0.96      0.90      0.93       107\n",
      "           8       0.98      0.98      0.98        98\n",
      "           9       0.91      0.91      0.91       116\n",
      "          10       0.80      0.85      0.82        99\n",
      "          11       0.96      0.95      0.96        83\n",
      "          12       0.96      0.97      0.96        97\n",
      "          13       0.90      0.84      0.87        96\n",
      "          14       0.89      0.93      0.91       100\n",
      "          15       0.96      1.00      0.98        64\n",
      "          16       0.96      0.97      0.96        99\n",
      "          17       0.95      0.93      0.94        95\n",
      "          18       0.97      0.94      0.96       106\n",
      "          19       0.88      0.88      0.88       108\n",
      "          20       0.94      0.94      0.94       101\n",
      "          21       0.90      0.84      0.87       102\n",
      "          22       0.97      0.97      0.97       103\n",
      "          23       0.85      0.90      0.87        96\n",
      "          24       0.81      0.74      0.77       103\n",
      "          25       0.98      0.97      0.98       103\n",
      "          26       0.84      0.76      0.80       100\n",
      "          27       0.93      0.94      0.93       110\n",
      "          28       0.97      0.93      0.95        92\n",
      "          29       0.98      0.99      0.98        86\n",
      "          30       0.97      0.96      0.97       102\n",
      "          31       0.78      0.75      0.76        91\n",
      "          32       0.92      0.83      0.87       100\n",
      "          33       0.81      0.92      0.86        89\n",
      "          34       0.96      0.93      0.94        96\n",
      "          35       0.94      0.95      0.94        94\n",
      "          36       0.90      0.86      0.88       101\n",
      "          37       0.82      0.93      0.87        91\n",
      "          38       0.93      0.94      0.93       118\n",
      "          39       0.92      0.95      0.94       122\n",
      "          40       0.92      0.94      0.93       101\n",
      "          41       0.84      0.88      0.86        91\n",
      "          42       0.93      1.00      0.96       101\n",
      "          43       0.86      0.87      0.87        86\n",
      "          44       0.95      0.91      0.93        99\n",
      "          45       0.96      0.97      0.96        90\n",
      "          46       0.98      0.98      0.98       101\n",
      "          47       0.86      0.81      0.84       102\n",
      "          48       0.91      0.86      0.88       104\n",
      "          49       0.85      0.86      0.86       103\n",
      "          50       0.62      0.66      0.64        99\n",
      "          51       0.98      1.00      0.99        58\n",
      "          52       0.97      0.99      0.98        70\n",
      "          53       0.89      0.83      0.86        95\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.90      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 25,  accuracy score is 1.0\n",
      "at random state 25, confusion matrix is [[105   0   0 ...   0   0   0]\n",
      " [  0  83   0 ...   0   0   1]\n",
      " [  0   0 109 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  45   0   0]\n",
      " [  0   0   0 ...   0  80   0]\n",
      " [  0   0   0 ...   0   0  75]]\n",
      "at random state 25, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.95      0.92       111\n",
      "           1       0.82      0.86      0.84        97\n",
      "           2       0.97      0.96      0.97       113\n",
      "           3       0.92      0.82      0.87       114\n",
      "           4       0.95      0.97      0.96        95\n",
      "           5       0.90      0.85      0.88        99\n",
      "           6       0.99      1.00      0.99        82\n",
      "           7       0.92      0.89      0.91        92\n",
      "           8       0.96      0.98      0.97        99\n",
      "           9       0.92      0.86      0.89       112\n",
      "          10       0.87      0.79      0.83       112\n",
      "          11       0.94      0.95      0.94        96\n",
      "          12       0.91      0.98      0.95       107\n",
      "          13       0.86      0.92      0.89       102\n",
      "          14       0.86      0.89      0.87       101\n",
      "          15       0.97      1.00      0.98        65\n",
      "          16       0.98      0.96      0.97       103\n",
      "          17       0.89      0.90      0.90        92\n",
      "          18       0.94      0.88      0.91       101\n",
      "          19       0.92      0.92      0.92       103\n",
      "          20       0.98      0.91      0.94        89\n",
      "          21       0.84      0.85      0.84        99\n",
      "          22       0.99      0.98      0.98        99\n",
      "          23       0.81      0.92      0.86       107\n",
      "          24       0.76      0.74      0.75        96\n",
      "          25       0.97      1.00      0.98        86\n",
      "          26       0.81      0.81      0.81       113\n",
      "          27       0.90      0.89      0.89        96\n",
      "          28       0.99      0.94      0.97       107\n",
      "          29       1.00      0.99      1.00       110\n",
      "          30       0.98      0.95      0.96        84\n",
      "          31       0.75      0.83      0.79       103\n",
      "          32       0.88      0.91      0.89       101\n",
      "          33       0.86      0.89      0.88       103\n",
      "          34       0.99      0.97      0.98        95\n",
      "          35       0.94      0.94      0.94       103\n",
      "          36       0.84      0.83      0.84       115\n",
      "          37       0.86      0.90      0.88       107\n",
      "          38       0.93      0.91      0.92       100\n",
      "          39       0.95      0.89      0.92        99\n",
      "          40       0.85      0.91      0.88        93\n",
      "          41       0.93      0.84      0.88        94\n",
      "          42       0.90      0.99      0.94        88\n",
      "          43       0.82      0.78      0.80       108\n",
      "          44       0.93      0.94      0.94       106\n",
      "          45       0.95      0.98      0.97        85\n",
      "          46       0.98      0.97      0.97        86\n",
      "          47       0.90      0.91      0.90       108\n",
      "          48       0.84      0.85      0.84       105\n",
      "          49       0.89      0.91      0.90       112\n",
      "          50       0.71      0.63      0.67        95\n",
      "          51       0.98      1.00      0.99        45\n",
      "          52       0.99      0.99      0.99        81\n",
      "          53       0.91      0.91      0.91        82\n",
      "\n",
      "    accuracy                           0.90      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.90      0.90      0.90      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 26,  accuracy score is 1.0\n",
      "at random state 26, confusion matrix is [[104   0   0 ...   0   0   0]\n",
      " [  0  79   0 ...   0   0   0]\n",
      " [  0   0 108 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  51   0   0]\n",
      " [  0   0   0 ...   1  67   0]\n",
      " [  0   2   2 ...   0   0  75]]\n",
      "at random state 26, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.93      0.92       112\n",
      "           1       0.79      0.76      0.77       104\n",
      "           2       0.96      0.99      0.97       109\n",
      "           3       0.95      0.92      0.93       108\n",
      "           4       0.95      0.99      0.97        96\n",
      "           5       0.88      0.81      0.84        94\n",
      "           6       0.98      0.99      0.98        87\n",
      "           7       0.89      0.84      0.86       102\n",
      "           8       0.98      0.98      0.98       101\n",
      "           9       0.89      0.91      0.90        97\n",
      "          10       0.88      0.83      0.85       116\n",
      "          11       0.95      0.92      0.94       115\n",
      "          12       0.98      0.97      0.97        88\n",
      "          13       0.93      0.88      0.91       113\n",
      "          14       0.90      0.96      0.93        94\n",
      "          15       0.99      0.98      0.98        85\n",
      "          16       0.97      0.98      0.97        87\n",
      "          17       0.94      0.90      0.92       112\n",
      "          18       0.88      0.94      0.91       101\n",
      "          19       0.91      0.91      0.91       115\n",
      "          20       0.97      0.94      0.96        83\n",
      "          21       0.75      0.79      0.77       105\n",
      "          22       0.97      1.00      0.98        88\n",
      "          23       0.81      0.94      0.87        86\n",
      "          24       0.83      0.71      0.77       108\n",
      "          25       0.99      0.97      0.98       106\n",
      "          26       0.87      0.80      0.83       105\n",
      "          27       0.88      0.85      0.86        92\n",
      "          28       0.97      0.97      0.97       115\n",
      "          29       0.99      0.98      0.99       101\n",
      "          30       0.99      0.98      0.99       107\n",
      "          31       0.73      0.70      0.71       106\n",
      "          32       0.93      0.85      0.89        94\n",
      "          33       0.86      0.94      0.90       105\n",
      "          34       0.98      0.98      0.98        82\n",
      "          35       0.96      0.98      0.97       102\n",
      "          36       0.84      0.87      0.85       106\n",
      "          37       0.80      0.91      0.85        94\n",
      "          38       0.95      0.89      0.92       112\n",
      "          39       0.91      0.94      0.93        99\n",
      "          40       0.86      0.92      0.89        99\n",
      "          41       0.84      0.93      0.88        94\n",
      "          42       0.98      0.94      0.96        86\n",
      "          43       0.90      0.86      0.88       104\n",
      "          44       0.93      0.93      0.93       109\n",
      "          45       0.97      1.00      0.98        85\n",
      "          46       0.98      0.97      0.97        87\n",
      "          47       0.92      0.93      0.92        98\n",
      "          48       0.88      0.84      0.86       105\n",
      "          49       0.85      0.91      0.88        94\n",
      "          50       0.57      0.63      0.60        89\n",
      "          51       0.96      1.00      0.98        51\n",
      "          52       0.97      0.99      0.98        68\n",
      "          53       0.88      0.79      0.83        95\n",
      "\n",
      "    accuracy                           0.90      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.90      0.90      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 27,  accuracy score is 1.0\n",
      "at random state 27, confusion matrix is [[103   0   0 ...   0   0   0]\n",
      " [  0  83   0 ...   0   0   0]\n",
      " [  0   0 109 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  57   0   0]\n",
      " [  0   0   0 ...   1  90   0]\n",
      " [  0   0   0 ...   0   0  82]]\n",
      "at random state 27, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.90      0.91       115\n",
      "           1       0.85      0.81      0.83       102\n",
      "           2       0.98      0.97      0.98       112\n",
      "           3       0.94      0.90      0.92        99\n",
      "           4       0.96      0.94      0.95       112\n",
      "           5       0.89      0.85      0.87        92\n",
      "           6       0.98      0.99      0.98        86\n",
      "           7       0.90      0.92      0.91        87\n",
      "           8       0.97      0.99      0.98       108\n",
      "           9       0.88      0.89      0.89        95\n",
      "          10       0.83      0.84      0.84       103\n",
      "          11       0.89      0.93      0.91       102\n",
      "          12       0.96      0.95      0.95       111\n",
      "          13       0.88      0.86      0.87       103\n",
      "          14       0.89      0.94      0.92       106\n",
      "          15       0.97      0.97      0.97        71\n",
      "          16       0.98      0.90      0.94       106\n",
      "          17       0.93      0.94      0.93       108\n",
      "          18       0.89      0.89      0.89       103\n",
      "          19       0.88      0.90      0.89       101\n",
      "          20       0.99      0.92      0.96        92\n",
      "          21       0.83      0.77      0.80       110\n",
      "          22       0.95      0.96      0.95        97\n",
      "          23       0.90      0.88      0.89       101\n",
      "          24       0.70      0.73      0.71        95\n",
      "          25       0.98      0.97      0.97       100\n",
      "          26       0.84      0.81      0.83        86\n",
      "          27       0.88      0.96      0.92        93\n",
      "          28       0.99      0.96      0.97        90\n",
      "          29       0.99      0.97      0.98        93\n",
      "          30       0.95      0.98      0.97        99\n",
      "          31       0.79      0.78      0.79        92\n",
      "          32       0.97      0.92      0.95       106\n",
      "          33       0.89      0.90      0.90        94\n",
      "          34       0.94      0.94      0.94       106\n",
      "          35       0.90      0.97      0.94        98\n",
      "          36       0.86      0.86      0.86        93\n",
      "          37       0.92      0.88      0.90       102\n",
      "          38       0.85      0.94      0.89        96\n",
      "          39       0.84      0.93      0.89        91\n",
      "          40       0.90      0.92      0.91       104\n",
      "          41       0.91      0.90      0.90       108\n",
      "          42       0.94      0.96      0.95        91\n",
      "          43       0.90      0.94      0.92       111\n",
      "          44       0.94      0.96      0.95       112\n",
      "          45       0.99      0.99      0.99        87\n",
      "          46       0.98      0.98      0.98        90\n",
      "          47       0.90      0.85      0.87        98\n",
      "          48       0.84      0.81      0.83        96\n",
      "          49       0.86      0.93      0.89        98\n",
      "          50       0.67      0.63      0.65        99\n",
      "          51       0.98      1.00      0.99        57\n",
      "          52       1.00      0.99      0.99        91\n",
      "          53       0.88      0.84      0.86        98\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.91      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 28,  accuracy score is 1.0\n",
      "at random state 28, confusion matrix is [[104   0   0 ...   0   0   0]\n",
      " [  0  81   0 ...   0   0   0]\n",
      " [  0   0  92 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  46   0   0]\n",
      " [  0   0   0 ...   1  77   0]\n",
      " [  0   0   1 ...   0   0  69]]\n",
      "at random state 28, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.89      0.90       117\n",
      "           1       0.79      0.86      0.83        94\n",
      "           2       0.94      0.99      0.96        93\n",
      "           3       0.84      0.90      0.87        97\n",
      "           4       0.94      0.97      0.95       105\n",
      "           5       0.84      0.81      0.82        88\n",
      "           6       0.96      0.99      0.97        96\n",
      "           7       0.92      0.98      0.95        90\n",
      "           8       0.96      0.95      0.95        96\n",
      "           9       0.92      0.95      0.93       105\n",
      "          10       0.77      0.85      0.81        93\n",
      "          11       0.94      0.93      0.94       105\n",
      "          12       0.94      0.93      0.93       107\n",
      "          13       0.87      0.84      0.86        96\n",
      "          14       0.89      0.92      0.91       100\n",
      "          15       0.99      0.96      0.97        80\n",
      "          16       0.96      0.96      0.96        95\n",
      "          17       0.93      0.94      0.94       103\n",
      "          18       0.91      0.97      0.94       111\n",
      "          19       0.87      0.82      0.85        97\n",
      "          20       0.99      0.95      0.97       100\n",
      "          21       0.86      0.89      0.88       111\n",
      "          22       0.98      0.99      0.99       107\n",
      "          23       0.88      0.80      0.83        88\n",
      "          24       0.83      0.70      0.76       113\n",
      "          25       0.97      0.99      0.98       100\n",
      "          26       0.85      0.81      0.83       108\n",
      "          27       0.92      0.92      0.92        93\n",
      "          28       0.99      0.95      0.97       113\n",
      "          29       0.97      1.00      0.98        97\n",
      "          30       0.97      0.92      0.94        96\n",
      "          31       0.75      0.74      0.75       104\n",
      "          32       0.94      0.91      0.93       105\n",
      "          33       0.91      0.90      0.90       105\n",
      "          34       0.94      0.95      0.95       108\n",
      "          35       0.97      0.99      0.98        90\n",
      "          36       0.86      0.88      0.87        94\n",
      "          37       0.87      0.94      0.90       101\n",
      "          38       0.95      0.87      0.91       110\n",
      "          39       0.90      0.93      0.91        98\n",
      "          40       0.95      0.94      0.95       103\n",
      "          41       0.88      0.94      0.91        95\n",
      "          42       0.96      0.97      0.96        92\n",
      "          43       0.94      0.88      0.91        95\n",
      "          44       0.90      0.88      0.89       106\n",
      "          45       0.95      1.00      0.97        89\n",
      "          46       1.00      0.97      0.98       100\n",
      "          47       0.88      0.84      0.86        97\n",
      "          48       0.97      0.84      0.90       106\n",
      "          49       0.86      0.90      0.88       110\n",
      "          50       0.55      0.66      0.60        85\n",
      "          51       0.98      1.00      0.99        46\n",
      "          52       0.99      0.99      0.99        78\n",
      "          53       0.84      0.81      0.83        85\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.91      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 29,  accuracy score is 1.0\n",
      "at random state 29, confusion matrix is [[92  0  0 ...  0  0  0]\n",
      " [ 0 85  0 ...  0  0  0]\n",
      " [ 0  0 79 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 55  0  0]\n",
      " [ 0  0  0 ...  0 78  0]\n",
      " [ 0  1  0 ...  0  0 79]]\n",
      "at random state 29, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.92      0.90       100\n",
      "           1       0.73      0.89      0.80        95\n",
      "           2       0.96      0.99      0.98        80\n",
      "           3       0.92      0.87      0.90        93\n",
      "           4       0.90      0.96      0.93        96\n",
      "           5       0.90      0.77      0.83       101\n",
      "           6       0.97      0.98      0.98       101\n",
      "           7       0.93      0.93      0.93        86\n",
      "           8       0.96      0.97      0.96        96\n",
      "           9       0.95      0.92      0.93       125\n",
      "          10       0.84      0.88      0.86       109\n",
      "          11       0.88      0.93      0.90        99\n",
      "          12       0.89      0.99      0.94        90\n",
      "          13       0.88      0.90      0.89       101\n",
      "          14       0.91      0.97      0.94        97\n",
      "          15       0.99      1.00      0.99        87\n",
      "          16       0.92      0.95      0.94        87\n",
      "          17       0.96      0.90      0.93       101\n",
      "          18       0.92      0.87      0.90       108\n",
      "          19       0.88      0.88      0.88       104\n",
      "          20       0.98      0.95      0.97       107\n",
      "          21       0.93      0.79      0.85       111\n",
      "          22       0.97      1.00      0.99       104\n",
      "          23       0.87      0.89      0.88        87\n",
      "          24       0.76      0.71      0.73        96\n",
      "          25       1.00      0.97      0.99       105\n",
      "          26       0.87      0.79      0.83       110\n",
      "          27       0.83      0.96      0.89        91\n",
      "          28       0.96      0.93      0.94       121\n",
      "          29       0.96      0.96      0.96       101\n",
      "          30       0.97      0.95      0.96       104\n",
      "          31       0.81      0.74      0.78        93\n",
      "          32       0.93      0.92      0.92       109\n",
      "          33       0.85      0.96      0.90        95\n",
      "          34       0.96      0.89      0.92        89\n",
      "          35       0.94      0.97      0.96       106\n",
      "          36       0.89      0.80      0.84       109\n",
      "          37       0.85      0.92      0.88        90\n",
      "          38       0.92      0.90      0.91       105\n",
      "          39       0.91      0.96      0.93       102\n",
      "          40       0.88      1.00      0.94       104\n",
      "          41       0.86      0.87      0.86        98\n",
      "          42       0.97      0.95      0.96       113\n",
      "          43       0.87      0.93      0.90        96\n",
      "          44       0.94      0.91      0.92        87\n",
      "          45       0.96      0.98      0.97        91\n",
      "          46       0.99      0.95      0.97        97\n",
      "          47       0.92      0.89      0.90        85\n",
      "          48       0.90      0.81      0.85       101\n",
      "          49       0.88      0.89      0.88        94\n",
      "          50       0.70      0.69      0.70       108\n",
      "          51       1.00      1.00      1.00        55\n",
      "          52       0.99      1.00      0.99        78\n",
      "          53       0.92      0.81      0.86        98\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.91      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 30,  accuracy score is 1.0\n",
      "at random state 30, confusion matrix is [[92  0  0 ...  0  0  0]\n",
      " [ 0 86  0 ...  0  0  0]\n",
      " [ 0  0 99 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 56  0  0]\n",
      " [ 0  0  0 ...  0 79  0]\n",
      " [ 0  2  3 ...  0  0 89]]\n",
      "at random state 30, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.90      0.88       102\n",
      "           1       0.83      0.83      0.83       103\n",
      "           2       0.92      0.98      0.95       101\n",
      "           3       0.88      0.88      0.88        96\n",
      "           4       0.94      0.96      0.95       119\n",
      "           5       0.85      0.80      0.82       109\n",
      "           6       0.97      1.00      0.98        85\n",
      "           7       0.89      0.88      0.88        90\n",
      "           8       0.97      0.97      0.97       109\n",
      "           9       0.85      0.86      0.86       108\n",
      "          10       0.87      0.85      0.86       110\n",
      "          11       0.93      0.96      0.94       103\n",
      "          12       0.97      0.94      0.96       105\n",
      "          13       0.85      0.86      0.86        94\n",
      "          14       0.90      0.93      0.92        99\n",
      "          15       1.00      0.98      0.99        85\n",
      "          16       0.96      0.95      0.96       116\n",
      "          17       0.95      0.92      0.93        96\n",
      "          18       0.88      0.91      0.90       100\n",
      "          19       0.87      0.96      0.91       101\n",
      "          20       0.97      0.92      0.94        99\n",
      "          21       0.73      0.83      0.78        93\n",
      "          22       0.97      0.99      0.98        99\n",
      "          23       0.82      0.87      0.84        84\n",
      "          24       0.87      0.79      0.83        99\n",
      "          25       0.98      0.98      0.98        96\n",
      "          26       0.84      0.76      0.80       101\n",
      "          27       0.89      0.93      0.91       101\n",
      "          28       0.96      0.92      0.94       103\n",
      "          29       0.95      0.99      0.97        94\n",
      "          30       0.98      0.94      0.96       108\n",
      "          31       0.80      0.80      0.80        94\n",
      "          32       0.90      0.92      0.91        97\n",
      "          33       0.91      0.85      0.88       107\n",
      "          34       0.97      0.94      0.96       103\n",
      "          35       0.96      0.96      0.96       114\n",
      "          36       0.81      0.82      0.81        92\n",
      "          37       0.84      0.91      0.87        91\n",
      "          38       0.84      0.91      0.88        94\n",
      "          39       0.95      0.97      0.96       107\n",
      "          40       0.89      0.91      0.90        85\n",
      "          41       0.87      0.93      0.90        99\n",
      "          42       0.92      0.98      0.95        87\n",
      "          43       0.90      0.85      0.88        96\n",
      "          44       0.92      0.84      0.88       119\n",
      "          45       0.94      0.97      0.95        86\n",
      "          46       0.95      0.96      0.95        90\n",
      "          47       0.92      0.81      0.86        84\n",
      "          48       0.92      0.78      0.84        99\n",
      "          49       0.86      0.83      0.85       109\n",
      "          50       0.66      0.75      0.71        89\n",
      "          51       1.00      1.00      1.00        56\n",
      "          52       1.00      0.99      0.99        80\n",
      "          53       0.92      0.81      0.86       110\n",
      "\n",
      "    accuracy                           0.90      5296\n",
      "   macro avg       0.90      0.90      0.90      5296\n",
      "weighted avg       0.90      0.90      0.90      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 31,  accuracy score is 1.0\n",
      "at random state 31, confusion matrix is [[97  0  0 ...  0  0  0]\n",
      " [ 0 87  0 ...  0  0  0]\n",
      " [ 0  0 92 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 47  0  0]\n",
      " [ 0  0  0 ...  0 71  0]\n",
      " [ 0  0  0 ...  0  0 83]]\n",
      "at random state 31, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.92       104\n",
      "           1       0.83      0.87      0.85       100\n",
      "           2       0.98      0.99      0.98        93\n",
      "           3       0.82      0.90      0.86        83\n",
      "           4       0.96      0.92      0.94       101\n",
      "           5       0.89      0.81      0.84       108\n",
      "           6       1.00      0.99      0.99        99\n",
      "           7       0.89      0.93      0.91        96\n",
      "           8       0.95      0.99      0.97        94\n",
      "           9       0.90      0.89      0.89        98\n",
      "          10       0.77      0.89      0.83       102\n",
      "          11       0.88      0.94      0.91       107\n",
      "          12       0.96      0.97      0.96        98\n",
      "          13       0.87      0.87      0.87       112\n",
      "          14       0.91      0.93      0.92        98\n",
      "          15       0.99      1.00      0.99        83\n",
      "          16       0.96      0.97      0.97       110\n",
      "          17       0.90      0.93      0.92        99\n",
      "          18       0.93      0.84      0.88       122\n",
      "          19       0.90      0.91      0.90        95\n",
      "          20       0.97      0.94      0.95       108\n",
      "          21       0.78      0.88      0.83        94\n",
      "          22       0.96      1.00      0.98       105\n",
      "          23       0.90      0.82      0.86       118\n",
      "          24       0.81      0.68      0.74        99\n",
      "          25       0.99      0.98      0.98        94\n",
      "          26       0.93      0.73      0.82       104\n",
      "          27       0.88      0.94      0.91       115\n",
      "          28       0.97      0.93      0.95        90\n",
      "          29       0.95      0.99      0.97       101\n",
      "          30       0.97      0.93      0.95        94\n",
      "          31       0.74      0.84      0.79        96\n",
      "          32       0.96      0.89      0.92       106\n",
      "          33       0.87      0.90      0.88        97\n",
      "          34       0.94      0.99      0.97       104\n",
      "          35       0.94      0.92      0.93        99\n",
      "          36       0.94      0.85      0.89        94\n",
      "          37       0.89      0.88      0.89       104\n",
      "          38       0.85      0.95      0.90        92\n",
      "          39       0.88      0.95      0.91        93\n",
      "          40       0.96      0.91      0.93        97\n",
      "          41       0.88      0.94      0.91       105\n",
      "          42       0.96      0.93      0.94        95\n",
      "          43       0.88      0.90      0.89        97\n",
      "          44       0.92      0.85      0.88       113\n",
      "          45       0.99      0.99      0.99        90\n",
      "          46       0.94      0.99      0.96        89\n",
      "          47       0.88      0.87      0.88       102\n",
      "          48       0.90      0.85      0.87        93\n",
      "          49       0.86      0.88      0.87        91\n",
      "          50       0.72      0.65      0.68       105\n",
      "          51       1.00      1.00      1.00        47\n",
      "          52       1.00      1.00      1.00        71\n",
      "          53       0.90      0.90      0.90        92\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.91      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 32,  accuracy score is 1.0\n",
      "at random state 32, confusion matrix is [[97  0  0 ...  0  0  0]\n",
      " [ 0 78  0 ...  0  0  0]\n",
      " [ 0  0 94 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 56  0  0]\n",
      " [ 0  0  0 ...  0 80  0]\n",
      " [ 0  0  0 ...  0  0 77]]\n",
      "at random state 32, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.93       104\n",
      "           1       0.79      0.89      0.83        88\n",
      "           2       0.99      0.99      0.99        95\n",
      "           3       0.92      0.78      0.84       101\n",
      "           4       0.94      0.93      0.94       101\n",
      "           5       0.87      0.80      0.83       103\n",
      "           6       0.96      1.00      0.98        70\n",
      "           7       0.86      0.96      0.91        97\n",
      "           8       0.96      0.99      0.98       102\n",
      "           9       0.87      0.92      0.89        92\n",
      "          10       0.85      0.88      0.86       107\n",
      "          11       0.94      0.91      0.92       107\n",
      "          12       0.93      0.98      0.95       101\n",
      "          13       0.86      0.88      0.87        88\n",
      "          14       0.94      0.91      0.92       114\n",
      "          15       1.00      0.97      0.99        77\n",
      "          16       0.96      0.98      0.97       104\n",
      "          17       0.95      0.91      0.93       102\n",
      "          18       0.88      0.93      0.90        83\n",
      "          19       0.89      0.88      0.89        95\n",
      "          20       0.97      0.93      0.95       108\n",
      "          21       0.88      0.81      0.84       100\n",
      "          22       0.99      1.00      0.99        94\n",
      "          23       0.77      0.92      0.84        93\n",
      "          24       0.75      0.73      0.74       100\n",
      "          25       0.96      0.99      0.98       103\n",
      "          26       0.83      0.74      0.78       105\n",
      "          27       0.91      0.91      0.91        95\n",
      "          28       0.95      0.99      0.97        92\n",
      "          29       0.99      0.97      0.98        96\n",
      "          30       0.93      0.94      0.93       106\n",
      "          31       0.79      0.73      0.76        99\n",
      "          32       0.93      0.88      0.90        99\n",
      "          33       0.90      0.92      0.91       112\n",
      "          34       0.98      0.98      0.98        96\n",
      "          35       0.97      0.96      0.97       103\n",
      "          36       0.88      0.76      0.82       110\n",
      "          37       0.90      0.86      0.88       110\n",
      "          38       0.87      0.99      0.93        96\n",
      "          39       0.89      0.93      0.91       110\n",
      "          40       0.82      0.89      0.85        95\n",
      "          41       0.86      0.84      0.85       103\n",
      "          42       0.92      0.95      0.93        82\n",
      "          43       0.85      0.88      0.86       105\n",
      "          44       0.95      0.86      0.90       100\n",
      "          45       0.98      0.97      0.97        99\n",
      "          46       0.94      0.99      0.96        89\n",
      "          47       0.96      0.88      0.92       102\n",
      "          48       0.95      0.83      0.89       118\n",
      "          49       0.87      0.92      0.90       112\n",
      "          50       0.66      0.75      0.70       104\n",
      "          51       1.00      1.00      1.00        56\n",
      "          52       1.00      0.98      0.99        82\n",
      "          53       0.89      0.85      0.87        91\n",
      "\n",
      "    accuracy                           0.90      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.90      0.90      0.90      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 33,  accuracy score is 1.0\n",
      "at random state 33, confusion matrix is [[ 92   0   0 ...   0   0   0]\n",
      " [  0  92   0 ...   0   0   0]\n",
      " [  0   0 114 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  58   0   0]\n",
      " [  0   0   0 ...   0  80   0]\n",
      " [  0   0   2 ...   0   0  84]]\n",
      "at random state 33, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.89      0.90       103\n",
      "           1       0.91      0.86      0.88       107\n",
      "           2       0.93      0.99      0.96       115\n",
      "           3       0.85      0.89      0.87        99\n",
      "           4       0.97      0.92      0.95       104\n",
      "           5       0.89      0.89      0.89       106\n",
      "           6       0.97      0.98      0.97        90\n",
      "           7       0.92      0.94      0.93       117\n",
      "           8       0.99      0.99      0.99        92\n",
      "           9       0.94      0.90      0.92        97\n",
      "          10       0.78      0.83      0.80        94\n",
      "          11       0.94      0.96      0.95       116\n",
      "          12       0.96      0.95      0.96       106\n",
      "          13       0.79      0.89      0.83        97\n",
      "          14       0.90      0.95      0.92       101\n",
      "          15       0.99      0.96      0.97        77\n",
      "          16       0.94      0.91      0.92        86\n",
      "          17       0.90      0.91      0.90        97\n",
      "          18       0.90      0.95      0.93        98\n",
      "          19       0.92      0.91      0.92       108\n",
      "          20       0.98      0.87      0.92       100\n",
      "          21       0.82      0.79      0.80        90\n",
      "          22       0.99      0.98      0.99       104\n",
      "          23       0.89      0.83      0.86       102\n",
      "          24       0.74      0.75      0.74       100\n",
      "          25       0.98      1.00      0.99       104\n",
      "          26       0.87      0.71      0.79       105\n",
      "          27       0.90      0.93      0.91        96\n",
      "          28       0.97      0.95      0.96       101\n",
      "          29       0.99      1.00      0.99        87\n",
      "          30       0.90      0.98      0.94        89\n",
      "          31       0.77      0.73      0.75        98\n",
      "          32       0.91      0.92      0.92       104\n",
      "          33       0.89      0.89      0.89        95\n",
      "          34       0.92      0.89      0.90        99\n",
      "          35       0.97      0.98      0.98       123\n",
      "          36       0.83      0.81      0.82       101\n",
      "          37       0.93      0.85      0.89        97\n",
      "          38       0.87      0.95      0.91        92\n",
      "          39       0.95      0.98      0.96        88\n",
      "          40       0.89      0.90      0.89        94\n",
      "          41       0.91      0.86      0.88       120\n",
      "          42       0.87      0.99      0.93        76\n",
      "          43       0.84      0.87      0.86        95\n",
      "          44       0.92      0.93      0.92       107\n",
      "          45       0.99      1.00      0.99        79\n",
      "          46       1.00      0.97      0.98        91\n",
      "          47       0.89      0.88      0.89       102\n",
      "          48       0.86      0.87      0.87        95\n",
      "          49       0.87      0.93      0.90       100\n",
      "          50       0.65      0.63      0.64       112\n",
      "          51       1.00      1.00      1.00        58\n",
      "          52       0.95      1.00      0.98        80\n",
      "          53       0.89      0.82      0.86       102\n",
      "\n",
      "    accuracy                           0.90      5296\n",
      "   macro avg       0.91      0.91      0.90      5296\n",
      "weighted avg       0.90      0.90      0.90      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 34,  accuracy score is 1.0\n",
      "at random state 34, confusion matrix is [[ 85   0   0 ...   0   0   0]\n",
      " [  0  93   0 ...   0   0   0]\n",
      " [  0   0 109 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  43   0   0]\n",
      " [  0   0   0 ...   0  76   0]\n",
      " [  0   0   2 ...   0   0  88]]\n",
      "at random state 34, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.91        95\n",
      "           1       0.76      0.94      0.84        99\n",
      "           2       0.97      0.99      0.98       110\n",
      "           3       0.98      0.86      0.92       101\n",
      "           4       0.95      0.94      0.95       112\n",
      "           5       0.88      0.79      0.83        95\n",
      "           6       0.94      1.00      0.97        83\n",
      "           7       0.91      0.94      0.93        87\n",
      "           8       0.97      0.97      0.97       101\n",
      "           9       0.94      0.92      0.93       106\n",
      "          10       0.89      0.84      0.87       100\n",
      "          11       0.95      0.93      0.94       108\n",
      "          12       0.94      0.95      0.94        99\n",
      "          13       0.88      0.84      0.86       106\n",
      "          14       0.82      0.96      0.89        98\n",
      "          15       0.98      0.95      0.96        85\n",
      "          16       0.96      0.96      0.96        98\n",
      "          17       0.94      0.90      0.92       109\n",
      "          18       0.88      0.92      0.90       100\n",
      "          19       0.87      0.90      0.88        97\n",
      "          20       0.94      0.95      0.94       111\n",
      "          21       0.84      0.77      0.81       105\n",
      "          22       0.98      1.00      0.99       104\n",
      "          23       0.91      0.93      0.92        97\n",
      "          24       0.78      0.81      0.79        85\n",
      "          25       0.98      1.00      0.99       101\n",
      "          26       0.81      0.84      0.83       101\n",
      "          27       0.88      0.92      0.90       101\n",
      "          28       0.97      0.93      0.95       122\n",
      "          29       0.97      0.98      0.98       102\n",
      "          30       0.91      0.96      0.94        98\n",
      "          31       0.80      0.77      0.79        97\n",
      "          32       0.92      0.87      0.90       119\n",
      "          33       0.92      0.96      0.94       106\n",
      "          34       0.98      0.95      0.96       115\n",
      "          35       0.92      0.96      0.94       100\n",
      "          36       0.83      0.80      0.82        97\n",
      "          37       0.89      0.84      0.86        86\n",
      "          38       0.85      0.93      0.89       113\n",
      "          39       0.93      0.92      0.92        97\n",
      "          40       0.87      0.97      0.92        80\n",
      "          41       0.92      0.87      0.90       108\n",
      "          42       0.96      0.98      0.97        96\n",
      "          43       0.85      0.82      0.83        94\n",
      "          44       0.91      0.93      0.92        99\n",
      "          45       0.99      0.97      0.98        93\n",
      "          46       0.95      0.98      0.96        91\n",
      "          47       0.91      0.84      0.87        98\n",
      "          48       0.90      0.80      0.85        89\n",
      "          49       0.77      0.90      0.83        78\n",
      "          50       0.69      0.69      0.69        93\n",
      "          51       1.00      1.00      1.00        43\n",
      "          52       1.00      0.99      0.99        77\n",
      "          53       0.93      0.79      0.85       111\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.91      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 35,  accuracy score is 1.0\n",
      "at random state 35, confusion matrix is [[ 88   0   0 ...   0   0   0]\n",
      " [  0  80   0 ...   0   0   0]\n",
      " [  0   0 113 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  43   0   0]\n",
      " [  0   0   0 ...   0  76   0]\n",
      " [  0   1   0 ...   0   0  75]]\n",
      "at random state 35, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.94      0.91        94\n",
      "           1       0.79      0.85      0.82        94\n",
      "           2       0.96      0.97      0.96       117\n",
      "           3       0.91      0.92      0.91        95\n",
      "           4       0.97      0.97      0.97        99\n",
      "           5       0.80      0.87      0.83        75\n",
      "           6       0.98      1.00      0.99       100\n",
      "           7       0.94      0.94      0.94        97\n",
      "           8       0.95      0.98      0.96        96\n",
      "           9       0.93      0.90      0.91       110\n",
      "          10       0.83      0.86      0.84       112\n",
      "          11       0.92      0.94      0.93       109\n",
      "          12       0.94      0.93      0.93       100\n",
      "          13       0.91      0.90      0.90       118\n",
      "          14       0.94      0.90      0.92        97\n",
      "          15       0.99      0.96      0.97        81\n",
      "          16       0.96      0.95      0.96       113\n",
      "          17       0.95      0.91      0.93       104\n",
      "          18       0.93      0.90      0.92       122\n",
      "          19       0.88      0.89      0.89        91\n",
      "          20       0.95      0.91      0.93        90\n",
      "          21       0.80      0.77      0.79       100\n",
      "          22       0.99      0.98      0.98        87\n",
      "          23       0.90      0.94      0.92       103\n",
      "          24       0.75      0.75      0.75       101\n",
      "          25       0.97      0.99      0.98       111\n",
      "          26       0.81      0.79      0.80       101\n",
      "          27       0.91      0.89      0.90       109\n",
      "          28       0.94      0.97      0.96       102\n",
      "          29       0.97      0.92      0.94       112\n",
      "          30       0.99      0.95      0.97       105\n",
      "          31       0.74      0.83      0.78       104\n",
      "          32       0.92      0.94      0.93       103\n",
      "          33       0.83      0.86      0.84        99\n",
      "          34       0.95      0.88      0.92        93\n",
      "          35       0.96      0.99      0.97       113\n",
      "          36       0.85      0.80      0.82        88\n",
      "          37       0.94      0.93      0.94       103\n",
      "          38       0.86      0.93      0.90        92\n",
      "          39       0.95      0.93      0.94       100\n",
      "          40       0.82      0.95      0.88        93\n",
      "          41       0.90      0.90      0.90       106\n",
      "          42       0.90      0.95      0.93        87\n",
      "          43       0.90      0.91      0.91        94\n",
      "          44       0.93      0.90      0.92        90\n",
      "          45       0.93      0.97      0.95        79\n",
      "          46       0.98      0.98      0.98        98\n",
      "          47       0.93      0.88      0.90        91\n",
      "          48       0.87      0.83      0.85       103\n",
      "          49       0.88      0.87      0.87        90\n",
      "          50       0.70      0.65      0.68       104\n",
      "          51       0.98      1.00      0.99        43\n",
      "          52       1.00      0.99      0.99        77\n",
      "          53       0.88      0.74      0.81       101\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.90      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 36,  accuracy score is 1.0\n",
      "at random state 36, confusion matrix is [[ 98   0   0 ...   0   0   0]\n",
      " [  0  78   0 ...   0   0   1]\n",
      " [  0   0 109 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  55   0   0]\n",
      " [  0   0   0 ...   0  72   0]\n",
      " [  0   2   0 ...   0   0  87]]\n",
      "at random state 36, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.91       108\n",
      "           1       0.78      0.84      0.81        93\n",
      "           2       0.96      0.99      0.98       110\n",
      "           3       0.87      0.83      0.85        96\n",
      "           4       0.96      0.97      0.96        98\n",
      "           5       0.91      0.84      0.87       112\n",
      "           6       0.97      0.97      0.97        88\n",
      "           7       0.91      0.92      0.91       106\n",
      "           8       0.97      0.96      0.96        99\n",
      "           9       0.92      0.93      0.93       102\n",
      "          10       0.89      0.86      0.88       116\n",
      "          11       0.91      0.96      0.93       114\n",
      "          12       0.97      0.97      0.97       105\n",
      "          13       0.89      0.86      0.88       106\n",
      "          14       0.93      0.94      0.93       108\n",
      "          15       0.95      0.97      0.96        78\n",
      "          16       0.98      0.93      0.96       104\n",
      "          17       0.94      0.93      0.93        97\n",
      "          18       0.91      0.89      0.90       101\n",
      "          19       0.85      0.89      0.87        99\n",
      "          20       0.95      0.95      0.95        94\n",
      "          21       0.78      0.82      0.80        97\n",
      "          22       0.97      1.00      0.98        96\n",
      "          23       0.83      0.86      0.85       106\n",
      "          24       0.74      0.74      0.74        92\n",
      "          25       0.97      0.98      0.97       115\n",
      "          26       0.92      0.88      0.90        99\n",
      "          27       0.90      0.92      0.91       113\n",
      "          28       0.96      0.95      0.96       107\n",
      "          29       0.99      0.98      0.98        90\n",
      "          30       0.99      0.96      0.98       107\n",
      "          31       0.71      0.82      0.76       100\n",
      "          32       0.89      0.85      0.87       103\n",
      "          33       0.84      0.95      0.89        80\n",
      "          34       0.95      0.90      0.92        97\n",
      "          35       0.91      0.95      0.93       106\n",
      "          36       0.87      0.78      0.82        78\n",
      "          37       0.89      0.87      0.88        93\n",
      "          38       0.90      0.94      0.92       106\n",
      "          39       0.89      0.94      0.91        80\n",
      "          40       0.90      0.92      0.91       102\n",
      "          41       0.91      0.91      0.91       106\n",
      "          42       1.00      0.98      0.99        93\n",
      "          43       0.92      0.89      0.90       106\n",
      "          44       0.92      0.92      0.92       108\n",
      "          45       0.97      0.96      0.97        78\n",
      "          46       0.94      0.94      0.94        82\n",
      "          47       0.90      0.90      0.90        88\n",
      "          48       0.91      0.83      0.87       100\n",
      "          49       0.82      0.93      0.87        96\n",
      "          50       0.72      0.62      0.67       110\n",
      "          51       1.00      1.00      1.00        55\n",
      "          52       1.00      1.00      1.00        72\n",
      "          53       0.93      0.86      0.89       101\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.91      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 37,  accuracy score is 1.0\n",
      "at random state 37, confusion matrix is [[102   0   0 ...   0   0   0]\n",
      " [  0  77   0 ...   0   0   0]\n",
      " [  0   0  94 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  50   0   0]\n",
      " [  0   0   0 ...   0  76   0]\n",
      " [  0   2   0 ...   0   0  78]]\n",
      "at random state 37, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.91      0.90       112\n",
      "           1       0.78      0.83      0.80        93\n",
      "           2       0.96      0.97      0.96        97\n",
      "           3       0.89      0.85      0.87       103\n",
      "           4       0.98      0.94      0.96       101\n",
      "           5       0.85      0.85      0.85       102\n",
      "           6       0.99      1.00      0.99        78\n",
      "           7       0.91      0.92      0.92       105\n",
      "           8       0.97      0.98      0.98       101\n",
      "           9       0.89      0.90      0.89        94\n",
      "          10       0.90      0.95      0.93        85\n",
      "          11       0.94      0.96      0.95        97\n",
      "          12       0.91      0.99      0.95        90\n",
      "          13       0.82      0.91      0.86        89\n",
      "          14       0.94      0.87      0.91        93\n",
      "          15       0.97      0.99      0.98        77\n",
      "          16       0.97      0.93      0.95       115\n",
      "          17       0.91      0.93      0.92        80\n",
      "          18       0.91      0.95      0.93       111\n",
      "          19       0.91      0.92      0.92       106\n",
      "          20       0.96      0.90      0.93        91\n",
      "          21       0.75      0.84      0.80       102\n",
      "          22       0.98      1.00      0.99        99\n",
      "          23       0.88      0.88      0.88       113\n",
      "          24       0.79      0.79      0.79        99\n",
      "          25       0.99      0.98      0.99       111\n",
      "          26       0.89      0.82      0.86       101\n",
      "          27       0.92      0.93      0.92        95\n",
      "          28       0.95      0.96      0.96       103\n",
      "          29       0.99      0.98      0.99       102\n",
      "          30       0.95      0.96      0.95       109\n",
      "          31       0.76      0.81      0.78       101\n",
      "          32       0.94      0.91      0.93       103\n",
      "          33       0.86      0.89      0.87        96\n",
      "          34       0.99      0.91      0.95       101\n",
      "          35       0.94      0.95      0.95       102\n",
      "          36       0.88      0.82      0.85       111\n",
      "          37       0.92      0.92      0.92       101\n",
      "          38       0.91      0.92      0.91        93\n",
      "          39       0.85      0.97      0.91        95\n",
      "          40       0.94      0.91      0.92       108\n",
      "          41       0.91      0.92      0.91       116\n",
      "          42       0.98      0.94      0.96        98\n",
      "          43       0.89      0.83      0.86       102\n",
      "          44       0.95      0.91      0.93       100\n",
      "          45       0.95      0.98      0.97        86\n",
      "          46       0.97      0.98      0.97        97\n",
      "          47       0.90      0.84      0.87       116\n",
      "          48       0.94      0.86      0.90       105\n",
      "          49       0.86      0.88      0.87        86\n",
      "          50       0.68      0.67      0.67       102\n",
      "          51       1.00      1.00      1.00        50\n",
      "          52       1.00      1.00      1.00        76\n",
      "          53       0.90      0.80      0.85        97\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.91      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 38,  accuracy score is 1.0\n",
      "at random state 38, confusion matrix is [[ 87   0   0 ...   0   0   0]\n",
      " [  0  79   0 ...   0   0   0]\n",
      " [  0   0 100 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  53   0   0]\n",
      " [  0   0   0 ...   0  88   0]\n",
      " [  0   1   0 ...   0   0  84]]\n",
      "at random state 38, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.94      0.89        93\n",
      "           1       0.82      0.84      0.83        94\n",
      "           2       0.99      0.98      0.99       102\n",
      "           3       0.92      0.87      0.89       106\n",
      "           4       0.95      0.94      0.94        94\n",
      "           5       0.88      0.75      0.81        97\n",
      "           6       0.99      0.97      0.98        78\n",
      "           7       0.94      0.93      0.93        96\n",
      "           8       0.95      0.96      0.96       107\n",
      "           9       0.90      0.89      0.90        82\n",
      "          10       0.82      0.83      0.82        90\n",
      "          11       0.95      0.94      0.94       111\n",
      "          12       0.93      0.94      0.94       107\n",
      "          13       0.93      0.88      0.91       104\n",
      "          14       0.88      0.91      0.89        85\n",
      "          15       1.00      0.97      0.99        76\n",
      "          16       0.97      0.94      0.96       108\n",
      "          17       0.91      0.89      0.90       100\n",
      "          18       0.88      0.93      0.91        98\n",
      "          19       0.81      0.89      0.85        97\n",
      "          20       0.97      0.91      0.94        91\n",
      "          21       0.83      0.85      0.84       110\n",
      "          22       0.96      1.00      0.98        95\n",
      "          23       0.83      0.87      0.85       111\n",
      "          24       0.74      0.73      0.73       107\n",
      "          25       0.98      0.98      0.98        96\n",
      "          26       0.90      0.77      0.83       102\n",
      "          27       0.86      0.98      0.91        88\n",
      "          28       1.00      0.97      0.98       101\n",
      "          29       0.97      0.98      0.97        96\n",
      "          30       0.97      0.96      0.96        96\n",
      "          31       0.75      0.73      0.74       106\n",
      "          32       0.94      0.90      0.92       112\n",
      "          33       0.88      0.96      0.92       110\n",
      "          34       0.97      0.92      0.94       102\n",
      "          35       0.95      0.96      0.96       100\n",
      "          36       0.82      0.88      0.85        93\n",
      "          37       0.86      0.94      0.90        83\n",
      "          38       0.88      0.92      0.90       102\n",
      "          39       0.94      0.92      0.93       117\n",
      "          40       0.86      0.91      0.89        94\n",
      "          41       0.92      0.91      0.91        98\n",
      "          42       0.92      0.96      0.94        96\n",
      "          43       0.93      0.86      0.89       114\n",
      "          44       0.92      0.88      0.90       120\n",
      "          45       0.93      0.99      0.96        77\n",
      "          46       0.98      0.98      0.98       103\n",
      "          47       0.86      0.88      0.87       113\n",
      "          48       0.92      0.76      0.83        94\n",
      "          49       0.85      0.94      0.89        95\n",
      "          50       0.65      0.64      0.65       109\n",
      "          51       1.00      1.00      1.00        53\n",
      "          52       0.99      1.00      0.99        88\n",
      "          53       0.91      0.85      0.88        99\n",
      "\n",
      "    accuracy                           0.90      5296\n",
      "   macro avg       0.91      0.91      0.90      5296\n",
      "weighted avg       0.90      0.90      0.90      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 39,  accuracy score is 1.0\n",
      "at random state 39, confusion matrix is [[ 88   0   0 ...   0   0   0]\n",
      " [  0  84   0 ...   0   0   0]\n",
      " [  0   0 103 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  50   0   0]\n",
      " [  0   0   0 ...   1  71   0]\n",
      " [  0   0   2 ...   0   0  69]]\n",
      "at random state 39, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.95      0.92        93\n",
      "           1       0.80      0.86      0.83        98\n",
      "           2       0.94      0.98      0.96       105\n",
      "           3       0.94      0.84      0.89       100\n",
      "           4       0.94      0.95      0.94        96\n",
      "           5       0.84      0.80      0.82        99\n",
      "           6       0.94      0.99      0.97        85\n",
      "           7       0.90      0.94      0.92       108\n",
      "           8       0.96      0.99      0.98       101\n",
      "           9       0.89      0.91      0.90       105\n",
      "          10       0.85      0.92      0.88        89\n",
      "          11       0.93      0.98      0.95        89\n",
      "          12       0.95      0.96      0.96       109\n",
      "          13       0.89      0.86      0.87        92\n",
      "          14       0.87      0.97      0.91        94\n",
      "          15       0.98      0.98      0.98        88\n",
      "          16       0.96      0.94      0.95       118\n",
      "          17       0.94      0.89      0.92       109\n",
      "          18       0.94      0.88      0.91       111\n",
      "          19       0.95      0.85      0.90       115\n",
      "          20       0.96      0.91      0.93        97\n",
      "          21       0.80      0.83      0.82       103\n",
      "          22       0.97      0.99      0.98       106\n",
      "          23       0.88      0.92      0.90       107\n",
      "          24       0.71      0.76      0.73        88\n",
      "          25       0.93      0.98      0.95        95\n",
      "          26       0.90      0.83      0.86        98\n",
      "          27       0.85      0.90      0.87        96\n",
      "          28       0.98      0.99      0.99       100\n",
      "          29       0.99      0.93      0.96        84\n",
      "          30       0.96      0.94      0.95       113\n",
      "          31       0.73      0.78      0.76       111\n",
      "          32       0.94      0.83      0.88       106\n",
      "          33       0.87      0.98      0.92        86\n",
      "          34       0.95      0.92      0.93        99\n",
      "          35       0.98      0.95      0.97       110\n",
      "          36       0.83      0.85      0.84        91\n",
      "          37       0.84      0.89      0.86        87\n",
      "          38       0.87      0.89      0.88        91\n",
      "          39       0.95      0.93      0.94       113\n",
      "          40       0.85      0.92      0.88       107\n",
      "          41       0.86      0.86      0.86        97\n",
      "          42       0.94      0.95      0.94        93\n",
      "          43       0.86      0.83      0.85        90\n",
      "          44       0.85      0.86      0.86        95\n",
      "          45       0.94      0.99      0.96        92\n",
      "          46       0.97      0.99      0.98        93\n",
      "          47       0.84      0.87      0.86        86\n",
      "          48       0.90      0.83      0.86       111\n",
      "          49       0.88      0.86      0.87       110\n",
      "          50       0.71      0.62      0.66       120\n",
      "          51       0.98      1.00      0.99        50\n",
      "          52       1.00      0.95      0.97        75\n",
      "          53       0.88      0.75      0.81        92\n",
      "\n",
      "    accuracy                           0.90      5296\n",
      "   macro avg       0.90      0.90      0.90      5296\n",
      "weighted avg       0.90      0.90      0.90      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 40,  accuracy score is 1.0\n",
      "at random state 40, confusion matrix is [[92  0  0 ...  0  0  0]\n",
      " [ 0 88  0 ...  0  0  0]\n",
      " [ 0  0 97 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 46  0  0]\n",
      " [ 0  0  0 ...  0 94  0]\n",
      " [ 0  0  0 ...  0  0 94]]\n",
      "at random state 40, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.92      0.89       100\n",
      "           1       0.80      0.88      0.84       100\n",
      "           2       0.97      0.98      0.97        99\n",
      "           3       0.90      0.89      0.90        91\n",
      "           4       0.97      0.94      0.95       111\n",
      "           5       0.88      0.83      0.85       105\n",
      "           6       0.98      1.00      0.99        79\n",
      "           7       0.89      0.90      0.89        79\n",
      "           8       0.97      0.98      0.98       105\n",
      "           9       0.90      0.86      0.88        91\n",
      "          10       0.85      0.90      0.87        97\n",
      "          11       0.91      0.93      0.92       102\n",
      "          12       0.95      0.96      0.96       104\n",
      "          13       0.88      0.82      0.85       103\n",
      "          14       0.92      0.95      0.93        83\n",
      "          15       0.98      0.97      0.97        91\n",
      "          16       0.98      0.91      0.94       100\n",
      "          17       0.94      0.89      0.91       100\n",
      "          18       0.87      0.95      0.91        98\n",
      "          19       0.91      0.88      0.90       103\n",
      "          20       0.97      0.94      0.95        89\n",
      "          21       0.83      0.86      0.85       102\n",
      "          22       0.98      1.00      0.99       108\n",
      "          23       0.84      0.87      0.85        90\n",
      "          24       0.77      0.65      0.70        95\n",
      "          25       0.94      0.99      0.96        90\n",
      "          26       0.86      0.86      0.86       109\n",
      "          27       0.91      0.94      0.92        98\n",
      "          28       0.97      0.94      0.95       100\n",
      "          29       0.96      0.99      0.97        86\n",
      "          30       0.92      0.96      0.94        96\n",
      "          31       0.82      0.72      0.77        98\n",
      "          32       0.88      0.93      0.90        96\n",
      "          33       0.91      0.92      0.91       109\n",
      "          34       0.97      0.96      0.96       113\n",
      "          35       0.96      0.96      0.96       108\n",
      "          36       0.88      0.75      0.81       105\n",
      "          37       0.86      0.91      0.88       109\n",
      "          38       0.93      0.92      0.92       123\n",
      "          39       0.95      0.94      0.94       108\n",
      "          40       0.93      0.91      0.92       112\n",
      "          41       0.87      0.91      0.89       113\n",
      "          42       0.93      0.97      0.95        96\n",
      "          43       0.85      0.90      0.87        87\n",
      "          44       0.95      0.92      0.93       107\n",
      "          45       0.99      0.99      0.99        89\n",
      "          46       0.97      0.99      0.98       107\n",
      "          47       0.87      0.90      0.88        87\n",
      "          48       0.91      0.81      0.86        78\n",
      "          49       0.88      0.84      0.86       111\n",
      "          50       0.58      0.71      0.64        84\n",
      "          51       1.00      1.00      1.00        46\n",
      "          52       1.00      0.98      0.99        96\n",
      "          53       0.91      0.85      0.88       110\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.91      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 41,  accuracy score is 1.0\n",
      "at random state 41, confusion matrix is [[96  0  0 ...  0  0  0]\n",
      " [ 0 90  0 ...  0  0  1]\n",
      " [ 0  0 91 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 51  0  0]\n",
      " [ 0  0  0 ...  1 79  0]\n",
      " [ 0  1  2 ...  0  0 78]]\n",
      "at random state 41, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.91      0.87       106\n",
      "           1       0.78      0.81      0.79       111\n",
      "           2       0.96      0.98      0.97        93\n",
      "           3       0.83      0.95      0.88       101\n",
      "           4       0.98      0.98      0.98       100\n",
      "           5       0.81      0.84      0.82       105\n",
      "           6       1.00      1.00      1.00        89\n",
      "           7       0.91      0.90      0.90        99\n",
      "           8       0.98      0.96      0.97       114\n",
      "           9       0.86      0.90      0.88        89\n",
      "          10       0.88      0.84      0.86        90\n",
      "          11       0.94      0.96      0.95       121\n",
      "          12       0.96      0.95      0.96       103\n",
      "          13       0.86      0.90      0.88        93\n",
      "          14       0.90      0.91      0.91        93\n",
      "          15       0.97      1.00      0.99        77\n",
      "          16       0.98      0.90      0.94       113\n",
      "          17       0.96      0.94      0.95        99\n",
      "          18       0.93      0.88      0.91       104\n",
      "          19       0.89      0.86      0.88        96\n",
      "          20       0.98      0.98      0.98        89\n",
      "          21       0.85      0.81      0.83       102\n",
      "          22       0.95      0.99      0.97       101\n",
      "          23       0.89      0.83      0.86       123\n",
      "          24       0.78      0.79      0.78        98\n",
      "          25       0.96      0.99      0.97        88\n",
      "          26       0.83      0.87      0.85        97\n",
      "          27       0.91      0.86      0.88       104\n",
      "          28       0.95      0.97      0.96        96\n",
      "          29       0.99      0.97      0.98        94\n",
      "          30       0.98      0.97      0.97        95\n",
      "          31       0.73      0.83      0.78       105\n",
      "          32       0.91      0.89      0.90       114\n",
      "          33       0.83      0.88      0.86       104\n",
      "          34       0.94      0.93      0.94       101\n",
      "          35       0.96      0.91      0.93       102\n",
      "          36       0.88      0.81      0.84        83\n",
      "          37       0.87      0.88      0.87        89\n",
      "          38       0.85      0.94      0.89        95\n",
      "          39       0.93      0.92      0.92       100\n",
      "          40       0.91      0.93      0.92       116\n",
      "          41       0.83      0.92      0.87        93\n",
      "          42       0.99      0.96      0.97       100\n",
      "          43       0.89      0.92      0.91        92\n",
      "          44       0.92      0.91      0.92       101\n",
      "          45       0.97      0.99      0.98        84\n",
      "          46       0.97      0.98      0.97        91\n",
      "          47       0.95      0.85      0.90       102\n",
      "          48       0.89      0.83      0.86        92\n",
      "          49       0.90      0.87      0.89       111\n",
      "          50       0.75      0.62      0.68       110\n",
      "          51       0.96      1.00      0.98        51\n",
      "          52       0.99      0.99      0.99        80\n",
      "          53       0.90      0.80      0.85        97\n",
      "\n",
      "    accuracy                           0.90      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.90      0.90      0.90      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 42,  accuracy score is 1.0\n",
      "at random state 42, confusion matrix is [[94  0  0 ...  0  0  0]\n",
      " [ 0 86  0 ...  0  0  1]\n",
      " [ 0  0 99 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 50  0  0]\n",
      " [ 0  0  0 ...  1 59  0]\n",
      " [ 0  0  0 ...  0  0 75]]\n",
      "at random state 42, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.91      0.90       103\n",
      "           1       0.81      0.87      0.84        99\n",
      "           2       0.98      0.99      0.99       100\n",
      "           3       0.90      0.81      0.85       100\n",
      "           4       0.95      0.96      0.96       103\n",
      "           5       0.87      0.85      0.86       105\n",
      "           6       0.97      0.97      0.97        89\n",
      "           7       0.91      0.89      0.90       113\n",
      "           8       0.97      0.99      0.98        94\n",
      "           9       0.94      0.85      0.89       113\n",
      "          10       0.87      0.89      0.88        92\n",
      "          11       0.89      0.94      0.91       113\n",
      "          12       0.92      0.94      0.93       104\n",
      "          13       0.82      0.86      0.84        92\n",
      "          14       0.91      0.92      0.91       111\n",
      "          15       0.96      0.98      0.97        84\n",
      "          16       0.99      0.93      0.96       106\n",
      "          17       0.95      0.93      0.94        99\n",
      "          18       0.90      0.90      0.90       114\n",
      "          19       0.85      0.87      0.86        94\n",
      "          20       0.95      0.91      0.93       107\n",
      "          21       0.76      0.80      0.78        98\n",
      "          22       0.99      0.98      0.99       104\n",
      "          23       0.82      0.85      0.84        93\n",
      "          24       0.76      0.75      0.76        95\n",
      "          25       0.97      0.96      0.97       114\n",
      "          26       0.80      0.78      0.79        97\n",
      "          27       0.92      0.86      0.89       104\n",
      "          28       0.95      0.93      0.94        88\n",
      "          29       0.97      0.98      0.97        94\n",
      "          30       0.95      0.94      0.95       107\n",
      "          31       0.79      0.83      0.81       103\n",
      "          32       0.90      0.97      0.93        89\n",
      "          33       0.82      0.91      0.86       110\n",
      "          34       0.95      0.95      0.95        93\n",
      "          35       0.95      0.96      0.95        92\n",
      "          36       0.79      0.83      0.81        77\n",
      "          37       0.87      0.93      0.90        87\n",
      "          38       0.97      0.92      0.94       108\n",
      "          39       0.87      0.91      0.89        94\n",
      "          40       0.89      0.93      0.91       100\n",
      "          41       0.87      0.89      0.88       113\n",
      "          42       0.96      0.95      0.95       100\n",
      "          43       0.92      0.87      0.89        98\n",
      "          44       0.93      0.89      0.91       114\n",
      "          45       0.97      0.96      0.96        92\n",
      "          46       0.96      0.99      0.97        86\n",
      "          47       0.87      0.82      0.85       118\n",
      "          48       0.91      0.86      0.88        90\n",
      "          49       0.85      0.90      0.88       101\n",
      "          50       0.72      0.66      0.69       100\n",
      "          51       0.98      1.00      0.99        50\n",
      "          52       0.97      0.98      0.98        60\n",
      "          53       0.88      0.82      0.85        92\n",
      "\n",
      "    accuracy                           0.90      5296\n",
      "   macro avg       0.90      0.90      0.90      5296\n",
      "weighted avg       0.90      0.90      0.90      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 43,  accuracy score is 1.0\n",
      "at random state 43, confusion matrix is [[ 92   0   0 ...   0   0   0]\n",
      " [  0  78   0 ...   0   0   0]\n",
      " [  0   0 104 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  51   0   0]\n",
      " [  0   0   0 ...   0  77   0]\n",
      " [  0   2   1 ...   0   0  90]]\n",
      "at random state 43, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.88      0.87       104\n",
      "           1       0.81      0.82      0.82        95\n",
      "           2       0.96      0.99      0.98       105\n",
      "           3       0.96      0.80      0.87        90\n",
      "           4       0.91      0.95      0.93       100\n",
      "           5       0.87      0.86      0.86        97\n",
      "           6       0.94      1.00      0.97        82\n",
      "           7       0.92      0.88      0.90       101\n",
      "           8       0.94      0.99      0.96        93\n",
      "           9       0.94      0.87      0.90       114\n",
      "          10       0.82      0.85      0.83        80\n",
      "          11       0.95      0.94      0.94       109\n",
      "          12       0.95      0.98      0.97       100\n",
      "          13       0.86      0.88      0.87        96\n",
      "          14       0.81      0.93      0.87        85\n",
      "          15       0.99      0.95      0.97        78\n",
      "          16       0.91      0.93      0.92       101\n",
      "          17       0.89      0.91      0.90       102\n",
      "          18       0.88      0.89      0.89       101\n",
      "          19       0.92      0.88      0.90       109\n",
      "          20       0.98      0.95      0.96        96\n",
      "          21       0.80      0.83      0.81       118\n",
      "          22       0.98      0.99      0.98        93\n",
      "          23       0.85      0.95      0.90       110\n",
      "          24       0.70      0.74      0.72        97\n",
      "          25       0.98      0.99      0.98        89\n",
      "          26       0.80      0.84      0.82        87\n",
      "          27       0.88      0.92      0.90       105\n",
      "          28       0.99      0.94      0.96        99\n",
      "          29       0.98      0.95      0.96       100\n",
      "          30       0.99      0.90      0.94       109\n",
      "          31       0.73      0.79      0.76       112\n",
      "          32       0.92      0.86      0.89       107\n",
      "          33       0.82      0.93      0.87       107\n",
      "          34       0.96      0.87      0.91       110\n",
      "          35       0.92      1.00      0.96       100\n",
      "          36       0.90      0.86      0.88        97\n",
      "          37       0.84      0.90      0.87        99\n",
      "          38       0.91      0.90      0.90        99\n",
      "          39       0.93      0.94      0.93       109\n",
      "          40       0.92      0.94      0.93        98\n",
      "          41       0.90      0.89      0.89       109\n",
      "          42       0.94      0.96      0.95        93\n",
      "          43       0.93      0.90      0.92        90\n",
      "          44       0.96      0.93      0.94       108\n",
      "          45       0.96      0.99      0.98        81\n",
      "          46       0.98      0.98      0.98        88\n",
      "          47       0.92      0.86      0.89        90\n",
      "          48       0.90      0.85      0.87        94\n",
      "          49       0.86      0.87      0.87       110\n",
      "          50       0.62      0.55      0.58       106\n",
      "          51       0.98      1.00      0.99        51\n",
      "          52       0.97      0.99      0.98        78\n",
      "          53       0.89      0.78      0.83       115\n",
      "\n",
      "    accuracy                           0.90      5296\n",
      "   macro avg       0.90      0.90      0.90      5296\n",
      "weighted avg       0.90      0.90      0.90      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 44,  accuracy score is 1.0\n",
      "at random state 44, confusion matrix is [[98  0  0 ...  0  0  0]\n",
      " [ 0 80  0 ...  0  0  1]\n",
      " [ 0  0 99 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 58  0  0]\n",
      " [ 0  0  0 ...  0 60  0]\n",
      " [ 0  1  0 ...  0  0 73]]\n",
      "at random state 44, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.93      0.91       105\n",
      "           1       0.79      0.86      0.82        93\n",
      "           2       0.96      0.98      0.97       101\n",
      "           3       0.91      0.92      0.91        96\n",
      "           4       0.97      0.99      0.98        99\n",
      "           5       0.85      0.87      0.86       108\n",
      "           6       0.98      1.00      0.99        86\n",
      "           7       0.96      0.93      0.95       106\n",
      "           8       1.00      0.99      1.00       110\n",
      "           9       0.96      0.91      0.93       110\n",
      "          10       0.77      0.88      0.82       101\n",
      "          11       0.91      0.94      0.93        87\n",
      "          12       0.93      0.97      0.95       117\n",
      "          13       0.86      0.88      0.87        94\n",
      "          14       0.92      0.84      0.88        95\n",
      "          15       1.00      1.00      1.00        76\n",
      "          16       0.98      0.95      0.96       100\n",
      "          17       0.90      0.93      0.92        99\n",
      "          18       0.92      0.91      0.92       101\n",
      "          19       0.81      0.88      0.85        94\n",
      "          20       0.99      0.90      0.95       105\n",
      "          21       0.84      0.81      0.82       115\n",
      "          22       0.99      0.99      0.99        97\n",
      "          23       0.88      0.82      0.85        97\n",
      "          24       0.76      0.75      0.76       113\n",
      "          25       1.00      0.99      1.00       102\n",
      "          26       0.84      0.80      0.82       110\n",
      "          27       0.92      0.93      0.93       107\n",
      "          28       0.99      0.95      0.97       114\n",
      "          29       0.96      0.97      0.96        92\n",
      "          30       0.98      0.96      0.97        99\n",
      "          31       0.73      0.78      0.75       109\n",
      "          32       0.92      0.87      0.89       111\n",
      "          33       0.83      0.91      0.87       102\n",
      "          34       0.96      0.95      0.96       104\n",
      "          35       0.92      0.96      0.94        83\n",
      "          36       0.81      0.88      0.84        90\n",
      "          37       0.88      0.90      0.89        80\n",
      "          38       0.88      0.95      0.91        95\n",
      "          39       0.90      0.93      0.92        90\n",
      "          40       0.85      0.95      0.90        93\n",
      "          41       0.88      0.88      0.88        97\n",
      "          42       1.00      0.93      0.96        85\n",
      "          43       0.92      0.85      0.89       123\n",
      "          44       0.93      0.90      0.91       100\n",
      "          45       0.99      0.99      0.99        88\n",
      "          46       0.98      0.98      0.98        95\n",
      "          47       0.94      0.89      0.91       100\n",
      "          48       0.90      0.88      0.89        92\n",
      "          49       0.90      0.85      0.87       110\n",
      "          50       0.71      0.65      0.68       106\n",
      "          51       0.98      1.00      0.99        58\n",
      "          52       0.95      1.00      0.98        60\n",
      "          53       0.90      0.76      0.82        96\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.91      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 45,  accuracy score is 1.0\n",
      "at random state 45, confusion matrix is [[101   0   0 ...   0   0   0]\n",
      " [  0  89   0 ...   0   0   0]\n",
      " [  0   0  94 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  58   0   0]\n",
      " [  0   0   0 ...   1  74   0]\n",
      " [  0   0   1 ...   0   0  78]]\n",
      "at random state 45, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.94      0.91       107\n",
      "           1       0.84      0.90      0.87        99\n",
      "           2       0.95      0.97      0.96        97\n",
      "           3       0.94      0.85      0.89       106\n",
      "           4       0.94      0.95      0.95        84\n",
      "           5       0.90      0.82      0.86       103\n",
      "           6       0.98      0.99      0.98        95\n",
      "           7       0.88      0.89      0.89        84\n",
      "           8       0.97      0.97      0.97        93\n",
      "           9       0.88      0.90      0.89       102\n",
      "          10       0.80      0.89      0.84        85\n",
      "          11       0.93      0.93      0.93       107\n",
      "          12       0.98      0.96      0.97        97\n",
      "          13       0.85      0.82      0.83        98\n",
      "          14       0.85      0.91      0.88        93\n",
      "          15       1.00      0.95      0.97        79\n",
      "          16       0.96      0.97      0.96        94\n",
      "          17       0.88      0.88      0.88       104\n",
      "          18       0.86      0.94      0.90       110\n",
      "          19       0.89      0.85      0.87       111\n",
      "          20       0.98      0.93      0.95       109\n",
      "          21       0.80      0.79      0.80        97\n",
      "          22       0.96      0.99      0.97        90\n",
      "          23       0.84      0.92      0.88        99\n",
      "          24       0.71      0.77      0.74        93\n",
      "          25       0.97      0.99      0.98       101\n",
      "          26       0.94      0.85      0.89       107\n",
      "          27       0.91      0.91      0.91       107\n",
      "          28       0.97      0.93      0.95       104\n",
      "          29       0.96      0.96      0.96       113\n",
      "          30       0.98      0.93      0.96       116\n",
      "          31       0.76      0.80      0.78        98\n",
      "          32       0.92      0.90      0.91       111\n",
      "          33       0.85      0.92      0.88       104\n",
      "          34       0.96      0.91      0.93       107\n",
      "          35       0.97      0.92      0.94        91\n",
      "          36       0.83      0.88      0.86        85\n",
      "          37       0.87      0.91      0.89       104\n",
      "          38       0.92      0.94      0.93       100\n",
      "          39       0.92      0.91      0.92       108\n",
      "          40       0.87      0.97      0.92       101\n",
      "          41       0.90      0.86      0.88       101\n",
      "          42       0.95      0.98      0.97        99\n",
      "          43       0.90      0.88      0.89        93\n",
      "          44       0.91      0.89      0.90        91\n",
      "          45       0.96      0.99      0.97        90\n",
      "          46       0.97      0.99      0.98        87\n",
      "          47       0.90      0.86      0.88        86\n",
      "          48       0.92      0.81      0.86       105\n",
      "          49       0.91      0.91      0.91       112\n",
      "          50       0.69      0.65      0.67       108\n",
      "          51       0.97      1.00      0.98        58\n",
      "          52       0.97      0.97      0.97        76\n",
      "          53       0.92      0.80      0.86        97\n",
      "\n",
      "    accuracy                           0.90      5296\n",
      "   macro avg       0.91      0.91      0.90      5296\n",
      "weighted avg       0.91      0.90      0.90      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 46,  accuracy score is 1.0\n",
      "at random state 46, confusion matrix is [[ 95   0   0 ...   0   0   0]\n",
      " [  0  85   0 ...   0   0   0]\n",
      " [  0   0 106 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  36   0   0]\n",
      " [  0   0   0 ...   0  77   0]\n",
      " [  0   0   1 ...   0   0 100]]\n",
      "at random state 46, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.98      0.91        97\n",
      "           1       0.89      0.90      0.90        94\n",
      "           2       0.97      1.00      0.99       106\n",
      "           3       0.89      0.87      0.88       103\n",
      "           4       0.95      0.95      0.95       111\n",
      "           5       0.87      0.77      0.81       103\n",
      "           6       0.96      0.98      0.97        84\n",
      "           7       0.96      0.90      0.93       102\n",
      "           8       0.98      1.00      0.99        95\n",
      "           9       0.94      0.90      0.92        86\n",
      "          10       0.83      0.88      0.86       108\n",
      "          11       0.95      0.97      0.96        87\n",
      "          12       0.98      0.99      0.99       114\n",
      "          13       0.88      0.86      0.87        94\n",
      "          14       0.93      0.90      0.91       107\n",
      "          15       0.96      0.98      0.97        82\n",
      "          16       0.97      0.94      0.96       120\n",
      "          17       0.89      0.91      0.90        94\n",
      "          18       0.92      0.88      0.90        99\n",
      "          19       0.82      0.93      0.87        87\n",
      "          20       0.96      0.92      0.94        93\n",
      "          21       0.79      0.89      0.84        93\n",
      "          22       0.97      0.99      0.98        92\n",
      "          23       0.84      0.87      0.86       103\n",
      "          24       0.76      0.72      0.74       103\n",
      "          25       0.99      0.99      0.99       113\n",
      "          26       0.85      0.77      0.81       108\n",
      "          27       0.87      0.91      0.89        88\n",
      "          28       0.96      0.98      0.97       105\n",
      "          29       1.00      0.98      0.99        81\n",
      "          30       0.97      0.97      0.97       111\n",
      "          31       0.75      0.77      0.76       103\n",
      "          32       0.92      0.85      0.88       105\n",
      "          33       0.88      0.94      0.91       104\n",
      "          34       0.94      0.96      0.95       101\n",
      "          35       0.93      0.96      0.95       102\n",
      "          36       0.94      0.81      0.87        96\n",
      "          37       0.90      0.94      0.92       100\n",
      "          38       0.91      0.93      0.92       103\n",
      "          39       0.90      0.95      0.92        94\n",
      "          40       0.91      0.95      0.93       101\n",
      "          41       0.86      0.89      0.87        93\n",
      "          42       0.94      0.97      0.95        96\n",
      "          43       0.84      0.91      0.88        93\n",
      "          44       0.92      0.86      0.89       111\n",
      "          45       1.00      0.99      0.99        90\n",
      "          46       0.98      0.98      0.98        93\n",
      "          47       0.91      0.86      0.88        93\n",
      "          48       0.94      0.86      0.90       108\n",
      "          49       0.91      0.90      0.91       115\n",
      "          50       0.64      0.62      0.63       103\n",
      "          51       1.00      1.00      1.00        36\n",
      "          52       0.97      1.00      0.99        77\n",
      "          53       0.95      0.86      0.90       116\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.91      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 47,  accuracy score is 1.0\n",
      "at random state 47, confusion matrix is [[89  0  0 ...  0  0  0]\n",
      " [ 0 88  0 ...  0  0  0]\n",
      " [ 0  0 87 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 49  0  0]\n",
      " [ 0  0  0 ...  1 65  0]\n",
      " [ 0  2  0 ...  0  0 87]]\n",
      "at random state 47, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.92      0.86        97\n",
      "           1       0.79      0.85      0.81       104\n",
      "           2       0.93      0.97      0.95        90\n",
      "           3       0.89      0.84      0.87        90\n",
      "           4       0.95      0.94      0.95       104\n",
      "           5       0.88      0.83      0.86       125\n",
      "           6       0.98      0.97      0.97        97\n",
      "           7       0.94      0.90      0.92        98\n",
      "           8       0.95      0.97      0.96       106\n",
      "           9       0.88      0.90      0.89       105\n",
      "          10       0.84      0.84      0.84       104\n",
      "          11       0.89      0.94      0.91       111\n",
      "          12       0.99      0.97      0.98        99\n",
      "          13       0.87      0.81      0.84        97\n",
      "          14       0.88      0.93      0.91       114\n",
      "          15       0.96      0.99      0.98        80\n",
      "          16       0.96      0.95      0.95       110\n",
      "          17       0.89      0.93      0.91        95\n",
      "          18       0.90      0.90      0.90       101\n",
      "          19       0.85      0.88      0.87       102\n",
      "          20       0.95      0.92      0.93        96\n",
      "          21       0.75      0.82      0.78        92\n",
      "          22       0.97      0.99      0.98       106\n",
      "          23       0.83      0.90      0.87       100\n",
      "          24       0.84      0.71      0.77       112\n",
      "          25       0.98      0.97      0.97        99\n",
      "          26       0.87      0.79      0.83        97\n",
      "          27       0.86      0.94      0.90        94\n",
      "          28       0.98      0.91      0.94        87\n",
      "          29       0.98      0.99      0.98        96\n",
      "          30       0.95      0.94      0.95       101\n",
      "          31       0.72      0.89      0.80        82\n",
      "          32       0.95      0.91      0.93        99\n",
      "          33       0.91      0.90      0.90       105\n",
      "          34       0.99      0.91      0.95        96\n",
      "          35       0.96      0.97      0.96        96\n",
      "          36       0.86      0.82      0.84       106\n",
      "          37       0.83      0.93      0.88        95\n",
      "          38       0.90      0.93      0.92       107\n",
      "          39       0.92      0.94      0.93       104\n",
      "          40       0.86      0.93      0.89       110\n",
      "          41       0.82      0.88      0.85        93\n",
      "          42       0.96      0.93      0.94        82\n",
      "          43       0.86      0.84      0.85       105\n",
      "          44       0.94      0.89      0.91       118\n",
      "          45       0.97      1.00      0.98        97\n",
      "          46       0.95      0.96      0.96        81\n",
      "          47       0.89      0.78      0.83        92\n",
      "          48       0.89      0.77      0.82       103\n",
      "          49       0.93      0.89      0.91       115\n",
      "          50       0.68      0.67      0.67        84\n",
      "          51       0.96      1.00      0.98        49\n",
      "          52       0.98      0.98      0.98        66\n",
      "          53       0.92      0.85      0.88       102\n",
      "\n",
      "    accuracy                           0.90      5296\n",
      "   macro avg       0.90      0.90      0.90      5296\n",
      "weighted avg       0.90      0.90      0.90      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 48,  accuracy score is 1.0\n",
      "at random state 48, confusion matrix is [[101   0   0 ...   0   0   0]\n",
      " [  0  88   0 ...   0   0   0]\n",
      " [  0   0 104 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  56   0   0]\n",
      " [  0   0   0 ...   0  83   0]\n",
      " [  0   1   0 ...   0   0  69]]\n",
      "at random state 48, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.92      0.90       110\n",
      "           1       0.91      0.83      0.87       106\n",
      "           2       0.98      1.00      0.99       104\n",
      "           3       0.95      0.85      0.90        99\n",
      "           4       0.94      0.96      0.95        78\n",
      "           5       0.87      0.85      0.86       106\n",
      "           6       0.96      1.00      0.98        88\n",
      "           7       0.91      0.93      0.92        96\n",
      "           8       0.96      0.96      0.96        82\n",
      "           9       0.93      0.96      0.94        96\n",
      "          10       0.85      0.89      0.87       118\n",
      "          11       0.85      0.97      0.91        80\n",
      "          12       0.95      0.92      0.94        87\n",
      "          13       0.84      0.89      0.87       110\n",
      "          14       0.90      0.92      0.91       111\n",
      "          15       1.00      0.99      0.99        70\n",
      "          16       0.95      0.95      0.95       100\n",
      "          17       0.92      0.93      0.93        92\n",
      "          18       0.93      0.85      0.89       101\n",
      "          19       0.88      0.89      0.89       110\n",
      "          20       0.96      0.92      0.94        96\n",
      "          21       0.80      0.80      0.80        95\n",
      "          22       0.99      0.98      0.99       102\n",
      "          23       0.81      0.90      0.85        93\n",
      "          24       0.70      0.69      0.69        96\n",
      "          25       0.97      0.99      0.98        88\n",
      "          26       0.86      0.82      0.84       101\n",
      "          27       0.91      0.90      0.90       111\n",
      "          28       0.96      0.90      0.93       104\n",
      "          29       0.99      0.94      0.96       101\n",
      "          30       0.96      0.98      0.97        98\n",
      "          31       0.76      0.79      0.78       105\n",
      "          32       0.94      0.91      0.93       112\n",
      "          33       0.89      0.90      0.90       110\n",
      "          34       0.98      0.94      0.96       113\n",
      "          35       0.96      0.96      0.96       112\n",
      "          36       0.87      0.88      0.88       100\n",
      "          37       0.91      0.87      0.89       119\n",
      "          38       0.86      0.94      0.90        97\n",
      "          39       0.92      0.94      0.93        98\n",
      "          40       0.91      0.96      0.93       100\n",
      "          41       0.91      0.91      0.91       102\n",
      "          42       0.99      0.97      0.98        80\n",
      "          43       0.89      0.87      0.88       108\n",
      "          44       0.91      0.93      0.92       103\n",
      "          45       0.92      0.97      0.95        80\n",
      "          46       0.96      0.97      0.96        92\n",
      "          47       0.95      0.81      0.87       106\n",
      "          48       0.90      0.92      0.91        97\n",
      "          49       0.89      0.92      0.91       100\n",
      "          50       0.67      0.67      0.67       109\n",
      "          51       0.98      1.00      0.99        56\n",
      "          52       0.98      0.99      0.98        84\n",
      "          53       0.90      0.82      0.86        84\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.91      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 49,  accuracy score is 1.0\n",
      "at random state 49, confusion matrix is [[99  0  0 ...  0  0  0]\n",
      " [ 0 70  0 ...  0  0  0]\n",
      " [ 0  0 86 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 57  0  0]\n",
      " [ 0  0  0 ...  0 80  0]\n",
      " [ 0  0  2 ...  0  0 88]]\n",
      "at random state 49, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.88      0.86       112\n",
      "           1       0.77      0.74      0.76        94\n",
      "           2       0.93      0.99      0.96        87\n",
      "           3       0.91      0.89      0.90        94\n",
      "           4       0.97      0.96      0.96        89\n",
      "           5       0.89      0.79      0.84       111\n",
      "           6       0.95      0.99      0.97        91\n",
      "           7       0.94      0.94      0.94        85\n",
      "           8       1.00      0.97      0.98       119\n",
      "           9       0.87      0.94      0.90        97\n",
      "          10       0.82      0.82      0.82       100\n",
      "          11       0.95      0.94      0.94       114\n",
      "          12       0.99      0.95      0.97       104\n",
      "          13       0.84      0.86      0.85        92\n",
      "          14       0.84      0.90      0.87        82\n",
      "          15       0.99      0.96      0.98        85\n",
      "          16       0.98      0.94      0.96       101\n",
      "          17       0.94      0.94      0.94        98\n",
      "          18       0.91      0.92      0.91        99\n",
      "          19       0.84      0.91      0.87        88\n",
      "          20       0.95      0.93      0.94       103\n",
      "          21       0.81      0.82      0.81       103\n",
      "          22       0.96      0.99      0.97        92\n",
      "          23       0.83      0.82      0.83        97\n",
      "          24       0.71      0.67      0.69       104\n",
      "          25       0.97      0.98      0.98       100\n",
      "          26       0.77      0.80      0.78        84\n",
      "          27       0.90      0.91      0.91       103\n",
      "          28       0.98      0.94      0.96       102\n",
      "          29       0.98      0.98      0.98       106\n",
      "          30       0.97      1.00      0.99       103\n",
      "          31       0.71      0.74      0.73        98\n",
      "          32       0.88      0.91      0.90        90\n",
      "          33       0.90      0.93      0.91       113\n",
      "          34       0.96      0.89      0.92       106\n",
      "          35       0.98      0.97      0.98       103\n",
      "          36       0.91      0.85      0.88       108\n",
      "          37       0.90      0.88      0.89       108\n",
      "          38       0.87      0.93      0.90       106\n",
      "          39       0.93      0.90      0.92       103\n",
      "          40       0.95      0.88      0.91       106\n",
      "          41       0.89      0.89      0.89       110\n",
      "          42       0.89      1.00      0.94        87\n",
      "          43       0.87      0.91      0.89        80\n",
      "          44       0.89      0.89      0.89        84\n",
      "          45       0.97      0.96      0.96        89\n",
      "          46       0.98      0.99      0.98        90\n",
      "          47       0.93      0.85      0.89        95\n",
      "          48       0.88      0.84      0.86       104\n",
      "          49       0.85      0.90      0.88       110\n",
      "          50       0.64      0.65      0.65       123\n",
      "          51       0.98      1.00      0.99        57\n",
      "          52       0.99      1.00      0.99        80\n",
      "          53       0.89      0.82      0.85       107\n",
      "\n",
      "    accuracy                           0.90      5296\n",
      "   macro avg       0.90      0.90      0.90      5296\n",
      "weighted avg       0.90      0.90      0.90      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 50,  accuracy score is 1.0\n",
      "at random state 50, confusion matrix is [[86  0  0 ...  0  0  0]\n",
      " [ 0 82  0 ...  0  0  0]\n",
      " [ 0  0 93 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 61  0  0]\n",
      " [ 0  0  0 ...  0 70  0]\n",
      " [ 0  0  0 ...  0  0 72]]\n",
      "at random state 50, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.83      0.86       103\n",
      "           1       0.88      0.80      0.84       102\n",
      "           2       0.98      1.00      0.99        93\n",
      "           3       0.91      0.82      0.86        95\n",
      "           4       0.97      0.99      0.98       106\n",
      "           5       0.89      0.88      0.88        90\n",
      "           6       0.99      0.99      0.99        92\n",
      "           7       0.90      0.91      0.91       102\n",
      "           8       1.00      0.98      0.99       105\n",
      "           9       0.88      0.93      0.90        96\n",
      "          10       0.88      0.93      0.91       103\n",
      "          11       0.98      0.94      0.96       115\n",
      "          12       0.96      0.97      0.97       106\n",
      "          13       0.91      0.89      0.90       110\n",
      "          14       0.90      0.95      0.92        95\n",
      "          15       1.00      0.99      0.99        81\n",
      "          16       0.96      0.94      0.95       109\n",
      "          17       0.94      0.94      0.94        99\n",
      "          18       0.85      0.99      0.91        91\n",
      "          19       0.90      0.90      0.90        98\n",
      "          20       0.99      0.90      0.94        87\n",
      "          21       0.84      0.88      0.86       108\n",
      "          22       0.98      1.00      0.99        97\n",
      "          23       0.85      0.89      0.87        97\n",
      "          24       0.85      0.78      0.81       115\n",
      "          25       0.97      0.99      0.98       105\n",
      "          26       0.88      0.79      0.83        91\n",
      "          27       0.86      0.94      0.90        96\n",
      "          28       0.96      0.93      0.94       100\n",
      "          29       0.99      0.97      0.98       104\n",
      "          30       0.98      0.99      0.99       106\n",
      "          31       0.79      0.75      0.77       103\n",
      "          32       0.95      0.93      0.94       107\n",
      "          33       0.95      0.92      0.93        98\n",
      "          34       0.98      0.94      0.96       101\n",
      "          35       0.96      0.97      0.96        99\n",
      "          36       0.83      0.91      0.87        99\n",
      "          37       0.89      0.92      0.91        99\n",
      "          38       0.92      0.91      0.92       114\n",
      "          39       0.95      0.93      0.94       113\n",
      "          40       0.92      0.95      0.94       100\n",
      "          41       0.90      0.90      0.90       105\n",
      "          42       0.95      0.99      0.97        88\n",
      "          43       0.93      0.86      0.89        97\n",
      "          44       0.95      0.92      0.94       103\n",
      "          45       0.96      1.00      0.98        92\n",
      "          46       0.96      0.98      0.97        83\n",
      "          47       0.89      0.96      0.92        90\n",
      "          48       0.95      0.88      0.91       107\n",
      "          49       0.85      0.93      0.89        97\n",
      "          50       0.64      0.71      0.67        86\n",
      "          51       0.98      1.00      0.99        61\n",
      "          52       0.97      1.00      0.99        70\n",
      "          53       0.94      0.83      0.88        87\n",
      "\n",
      "    accuracy                           0.92      5296\n",
      "   macro avg       0.92      0.92      0.92      5296\n",
      "weighted avg       0.92      0.92      0.92      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 51,  accuracy score is 1.0\n",
      "at random state 51, confusion matrix is [[93  0  0 ...  0  0  0]\n",
      " [ 0 91  0 ...  0  0  0]\n",
      " [ 0  0 95 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 41  0  0]\n",
      " [ 0  0  0 ...  0 80  0]\n",
      " [ 0  0  2 ...  0  0 89]]\n",
      "at random state 51, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.87      0.87       107\n",
      "           1       0.86      0.81      0.83       112\n",
      "           2       0.97      0.98      0.97        97\n",
      "           3       0.90      0.87      0.89        95\n",
      "           4       0.99      0.97      0.98       114\n",
      "           5       0.89      0.80      0.85       116\n",
      "           6       0.97      1.00      0.99        76\n",
      "           7       0.92      0.91      0.92        94\n",
      "           8       0.97      0.97      0.97       102\n",
      "           9       0.89      0.93      0.91       100\n",
      "          10       0.83      0.90      0.87       114\n",
      "          11       0.92      0.96      0.94        96\n",
      "          12       0.93      0.97      0.95       119\n",
      "          13       0.84      0.87      0.85        97\n",
      "          14       0.94      0.87      0.90       102\n",
      "          15       0.98      0.99      0.98        81\n",
      "          16       0.96      0.93      0.94        99\n",
      "          17       0.92      0.93      0.92        99\n",
      "          18       0.90      0.92      0.91       103\n",
      "          19       0.88      0.89      0.89        92\n",
      "          20       0.96      0.96      0.96       103\n",
      "          21       0.80      0.84      0.82       105\n",
      "          22       1.00      1.00      1.00        96\n",
      "          23       0.83      0.91      0.87        94\n",
      "          24       0.68      0.73      0.70        91\n",
      "          25       0.98      0.98      0.98        87\n",
      "          26       0.85      0.76      0.80        94\n",
      "          27       0.92      0.87      0.89       102\n",
      "          28       0.98      0.97      0.97        90\n",
      "          29       1.00      0.97      0.98        97\n",
      "          30       0.97      0.96      0.96        95\n",
      "          31       0.74      0.81      0.77        83\n",
      "          32       0.90      0.95      0.92       101\n",
      "          33       0.94      0.91      0.93       102\n",
      "          34       0.97      0.92      0.94        97\n",
      "          35       0.92      0.96      0.94       102\n",
      "          36       0.88      0.77      0.82        86\n",
      "          37       0.88      0.87      0.87       106\n",
      "          38       0.86      0.96      0.91       106\n",
      "          39       0.93      0.94      0.94       102\n",
      "          40       0.92      0.92      0.92       109\n",
      "          41       0.91      0.91      0.91        96\n",
      "          42       0.96      0.95      0.96       106\n",
      "          43       0.86      0.87      0.86       110\n",
      "          44       0.97      0.88      0.92       111\n",
      "          45       0.98      0.99      0.98        87\n",
      "          46       0.97      0.98      0.97        98\n",
      "          47       0.93      0.87      0.90        95\n",
      "          48       0.86      0.84      0.85        99\n",
      "          49       0.80      0.91      0.85        91\n",
      "          50       0.68      0.65      0.66       108\n",
      "          51       1.00      1.00      1.00        41\n",
      "          52       0.99      1.00      0.99        80\n",
      "          53       0.86      0.80      0.83       111\n",
      "\n",
      "    accuracy                           0.90      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.90      0.90      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 52,  accuracy score is 1.0\n",
      "at random state 52, confusion matrix is [[ 98   0   0 ...   0   0   0]\n",
      " [  0  88   0 ...   0   0   0]\n",
      " [  0   0 105 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  57   0   0]\n",
      " [  0   0   0 ...   1  80   0]\n",
      " [  0   1   3 ...   0   0  83]]\n",
      "at random state 52, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.96      0.91       102\n",
      "           1       0.79      0.86      0.82       102\n",
      "           2       0.95      0.95      0.95       110\n",
      "           3       0.89      0.87      0.88        83\n",
      "           4       0.95      0.92      0.93       101\n",
      "           5       0.87      0.82      0.84       106\n",
      "           6       0.99      0.99      0.99        91\n",
      "           7       0.91      0.89      0.90        93\n",
      "           8       0.98      0.98      0.98        94\n",
      "           9       0.92      0.88      0.90       104\n",
      "          10       0.83      0.84      0.84        94\n",
      "          11       0.96      0.97      0.97       103\n",
      "          12       0.97      0.95      0.96       105\n",
      "          13       0.89      0.86      0.88        88\n",
      "          14       0.85      0.94      0.89       100\n",
      "          15       0.99      1.00      0.99        69\n",
      "          16       0.97      0.97      0.97        97\n",
      "          17       0.91      0.93      0.92        95\n",
      "          18       0.92      0.94      0.93       111\n",
      "          19       0.89      0.89      0.89       114\n",
      "          20       0.98      0.92      0.95       102\n",
      "          21       0.76      0.87      0.81        90\n",
      "          22       0.98      1.00      0.99       102\n",
      "          23       0.90      0.93      0.91        94\n",
      "          24       0.81      0.73      0.77        96\n",
      "          25       0.99      0.96      0.98       105\n",
      "          26       0.80      0.89      0.84        93\n",
      "          27       0.90      0.93      0.91       103\n",
      "          28       0.94      0.97      0.96       106\n",
      "          29       0.99      0.92      0.96        93\n",
      "          30       0.95      0.98      0.96        93\n",
      "          31       0.80      0.78      0.79       105\n",
      "          32       0.89      0.90      0.89        98\n",
      "          33       0.94      0.90      0.92       107\n",
      "          34       0.96      0.92      0.94       115\n",
      "          35       0.91      0.99      0.95       108\n",
      "          36       0.88      0.90      0.89       111\n",
      "          37       0.89      0.87      0.88       110\n",
      "          38       0.87      0.95      0.91        95\n",
      "          39       0.86      0.86      0.86        85\n",
      "          40       0.88      0.96      0.92        92\n",
      "          41       0.90      0.88      0.89       112\n",
      "          42       0.95      0.98      0.96        82\n",
      "          43       0.93      0.89      0.91       104\n",
      "          44       0.89      0.92      0.91       101\n",
      "          45       0.95      0.96      0.96        78\n",
      "          46       0.98      0.96      0.97        92\n",
      "          47       0.95      0.82      0.88       108\n",
      "          48       0.92      0.79      0.85       117\n",
      "          49       0.91      0.87      0.89       105\n",
      "          50       0.70      0.68      0.69        93\n",
      "          51       0.95      1.00      0.97        57\n",
      "          52       0.96      0.99      0.98        81\n",
      "          53       0.89      0.82      0.86       101\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.91      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 53,  accuracy score is 1.0\n",
      "at random state 53, confusion matrix is [[ 87   0   0 ...   0   0   0]\n",
      " [  0  98   0 ...   0   0   0]\n",
      " [  0   0 101 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  51   0   0]\n",
      " [  0   0   0 ...   1  80   0]\n",
      " [  0   1   2 ...   0   0  88]]\n",
      "at random state 53, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.86      0.84       101\n",
      "           1       0.88      0.89      0.89       110\n",
      "           2       0.94      0.94      0.94       107\n",
      "           3       0.97      0.93      0.95        89\n",
      "           4       0.92      0.95      0.94       109\n",
      "           5       0.87      0.76      0.81       110\n",
      "           6       0.99      0.99      0.99        95\n",
      "           7       0.84      0.91      0.88        89\n",
      "           8       0.97      0.98      0.97        93\n",
      "           9       0.92      0.94      0.93       103\n",
      "          10       0.91      0.87      0.89       110\n",
      "          11       0.91      0.91      0.91        88\n",
      "          12       0.95      0.97      0.96       107\n",
      "          13       0.88      0.88      0.88        95\n",
      "          14       0.92      0.94      0.93        94\n",
      "          15       0.98      0.99      0.98        84\n",
      "          16       0.95      0.95      0.95       112\n",
      "          17       0.94      0.90      0.92        91\n",
      "          18       0.91      0.93      0.92       100\n",
      "          19       0.89      0.90      0.90        92\n",
      "          20       0.96      0.96      0.96       112\n",
      "          21       0.76      0.87      0.81        89\n",
      "          22       0.97      0.99      0.98       107\n",
      "          23       0.85      0.91      0.88        97\n",
      "          24       0.73      0.79      0.76        89\n",
      "          25       0.98      0.97      0.98       106\n",
      "          26       0.83      0.86      0.84        88\n",
      "          27       0.93      0.93      0.93        98\n",
      "          28       0.98      0.93      0.96        92\n",
      "          29       0.97      0.99      0.98        86\n",
      "          30       0.96      0.97      0.96        90\n",
      "          31       0.76      0.83      0.79       101\n",
      "          32       0.95      0.91      0.93        96\n",
      "          33       0.87      0.95      0.91       102\n",
      "          34       0.94      0.88      0.91       102\n",
      "          35       0.94      0.99      0.97        99\n",
      "          36       0.85      0.83      0.84        94\n",
      "          37       0.91      0.84      0.87       114\n",
      "          38       0.83      0.92      0.87       106\n",
      "          39       0.95      0.97      0.96       107\n",
      "          40       0.93      0.95      0.94        98\n",
      "          41       0.80      0.90      0.85        99\n",
      "          42       0.98      0.96      0.97        99\n",
      "          43       0.91      0.91      0.91       104\n",
      "          44       0.94      0.86      0.90       113\n",
      "          45       0.98      1.00      0.99        80\n",
      "          46       1.00      0.98      0.99       100\n",
      "          47       0.93      0.83      0.88        90\n",
      "          48       0.97      0.87      0.92       101\n",
      "          49       0.85      0.82      0.84        99\n",
      "          50       0.79      0.68      0.73       114\n",
      "          51       0.98      1.00      0.99        51\n",
      "          52       1.00      0.98      0.99        82\n",
      "          53       0.88      0.79      0.83       112\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.91      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 54,  accuracy score is 1.0\n",
      "at random state 54, confusion matrix is [[93  0  0 ...  0  0  0]\n",
      " [ 0 84  0 ...  0  0  1]\n",
      " [ 0  0 93 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 52  0  0]\n",
      " [ 0  0  0 ...  0 74  0]\n",
      " [ 0  1  1 ...  0  0 76]]\n",
      "at random state 54, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.95      0.92        98\n",
      "           1       0.82      0.80      0.81       105\n",
      "           2       0.97      0.97      0.97        96\n",
      "           3       0.83      0.91      0.87        92\n",
      "           4       0.96      0.99      0.98       102\n",
      "           5       0.88      0.80      0.84       120\n",
      "           6       1.00      0.99      0.99        81\n",
      "           7       0.90      0.88      0.89        92\n",
      "           8       0.98      0.98      0.98        92\n",
      "           9       0.88      0.94      0.91       110\n",
      "          10       0.79      0.93      0.85        91\n",
      "          11       0.92      0.95      0.93        98\n",
      "          12       0.94      0.95      0.95       101\n",
      "          13       0.87      0.84      0.85       117\n",
      "          14       0.92      0.91      0.92        90\n",
      "          15       1.00      1.00      1.00        73\n",
      "          16       0.95      0.95      0.95       110\n",
      "          17       0.92      0.94      0.93       108\n",
      "          18       0.91      0.93      0.92        98\n",
      "          19       0.89      0.92      0.91        91\n",
      "          20       0.98      0.96      0.97        96\n",
      "          21       0.86      0.75      0.80       113\n",
      "          22       0.96      1.00      0.98        81\n",
      "          23       0.91      0.83      0.87       116\n",
      "          24       0.74      0.77      0.75       105\n",
      "          25       0.95      1.00      0.97        77\n",
      "          26       0.90      0.84      0.87       110\n",
      "          27       0.90      0.95      0.92        96\n",
      "          28       0.97      0.95      0.96       105\n",
      "          29       0.99      0.98      0.99       115\n",
      "          30       0.97      0.93      0.95        90\n",
      "          31       0.74      0.77      0.76        95\n",
      "          32       0.93      0.89      0.91       103\n",
      "          33       0.83      0.94      0.88        97\n",
      "          34       0.95      0.94      0.94        95\n",
      "          35       0.95      0.95      0.95       102\n",
      "          36       0.84      0.84      0.84        97\n",
      "          37       0.85      0.89      0.87       109\n",
      "          38       0.93      0.94      0.94       116\n",
      "          39       0.94      0.91      0.92       100\n",
      "          40       0.79      0.91      0.85       103\n",
      "          41       0.90      0.85      0.87       104\n",
      "          42       0.93      0.97      0.95       101\n",
      "          43       0.88      0.87      0.87        97\n",
      "          44       0.96      0.92      0.94        99\n",
      "          45       0.96      0.99      0.97        86\n",
      "          46       0.96      0.98      0.97        94\n",
      "          47       0.96      0.89      0.92        87\n",
      "          48       0.91      0.80      0.85       104\n",
      "          49       0.86      0.84      0.85       112\n",
      "          50       0.63      0.60      0.61       104\n",
      "          51       1.00      1.00      1.00        52\n",
      "          52       0.99      0.99      0.99        75\n",
      "          53       0.85      0.80      0.83        95\n",
      "\n",
      "    accuracy                           0.90      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.90      0.90      0.90      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 55,  accuracy score is 1.0\n",
      "at random state 55, confusion matrix is [[96  0  0 ...  0  0  0]\n",
      " [ 0 87  0 ...  0  0  0]\n",
      " [ 0  0 99 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 56  0  0]\n",
      " [ 0  0  0 ...  0 79  0]\n",
      " [ 0  0  2 ...  0  0 70]]\n",
      "at random state 55, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.93      0.90       103\n",
      "           1       0.81      0.86      0.84       101\n",
      "           2       0.97      1.00      0.99        99\n",
      "           3       0.81      0.88      0.84        83\n",
      "           4       0.91      0.96      0.93        91\n",
      "           5       0.89      0.76      0.82       105\n",
      "           6       0.99      0.99      0.99        84\n",
      "           7       0.91      0.93      0.92       114\n",
      "           8       1.00      0.99      1.00       109\n",
      "           9       0.85      0.87      0.86        95\n",
      "          10       0.88      0.89      0.89       104\n",
      "          11       0.96      0.96      0.96       102\n",
      "          12       0.98      0.96      0.97        93\n",
      "          13       0.89      0.79      0.84        97\n",
      "          14       0.89      0.93      0.91        96\n",
      "          15       1.00      0.99      0.99        80\n",
      "          16       1.00      0.90      0.95       100\n",
      "          17       0.93      0.94      0.94       104\n",
      "          18       0.85      0.90      0.87        86\n",
      "          19       0.87      0.91      0.89        86\n",
      "          20       0.98      0.91      0.94        98\n",
      "          21       0.81      0.78      0.79       117\n",
      "          22       0.98      0.97      0.98       108\n",
      "          23       0.82      0.78      0.80        99\n",
      "          24       0.83      0.72      0.77       102\n",
      "          25       0.97      1.00      0.98        98\n",
      "          26       0.87      0.83      0.85       109\n",
      "          27       0.92      0.93      0.92       109\n",
      "          28       0.98      0.95      0.97       104\n",
      "          29       0.98      0.99      0.98        87\n",
      "          30       0.98      0.96      0.97       115\n",
      "          31       0.72      0.81      0.76       106\n",
      "          32       0.92      0.91      0.92       102\n",
      "          33       0.80      0.85      0.82        97\n",
      "          34       0.95      0.96      0.95        96\n",
      "          35       0.94      0.95      0.95       104\n",
      "          36       0.89      0.89      0.89        95\n",
      "          37       0.89      0.91      0.90        96\n",
      "          38       0.93      0.90      0.92       105\n",
      "          39       0.90      0.90      0.90       102\n",
      "          40       0.89      0.84      0.86       101\n",
      "          41       0.84      0.93      0.89       104\n",
      "          42       0.90      1.00      0.95        88\n",
      "          43       0.93      0.89      0.91       105\n",
      "          44       0.92      0.94      0.93       102\n",
      "          45       0.99      0.99      0.99        88\n",
      "          46       0.98      1.00      0.99        98\n",
      "          47       0.88      0.88      0.88       114\n",
      "          48       0.92      0.82      0.86       109\n",
      "          49       0.83      0.92      0.87        87\n",
      "          50       0.64      0.67      0.66        94\n",
      "          51       0.98      1.00      0.99        56\n",
      "          52       0.99      1.00      0.99        79\n",
      "          53       0.90      0.78      0.83        90\n",
      "\n",
      "    accuracy                           0.90      5296\n",
      "   macro avg       0.91      0.91      0.90      5296\n",
      "weighted avg       0.90      0.90      0.90      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 56,  accuracy score is 1.0\n",
      "at random state 56, confusion matrix is [[ 92   0   0 ...   0   0   0]\n",
      " [  0  95   0 ...   0   0   1]\n",
      " [  0   0 113 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  55   0   0]\n",
      " [  0   0   0 ...   0  75   0]\n",
      " [  0   0   0 ...   0   0  94]]\n",
      "at random state 56, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.88      0.87       104\n",
      "           1       0.86      0.85      0.86       112\n",
      "           2       0.96      0.97      0.97       116\n",
      "           3       0.93      0.95      0.94       112\n",
      "           4       0.99      0.96      0.98       108\n",
      "           5       0.89      0.80      0.84       109\n",
      "           6       0.98      0.99      0.98        94\n",
      "           7       0.95      0.96      0.96        80\n",
      "           8       0.97      0.98      0.98       106\n",
      "           9       0.88      0.96      0.91        95\n",
      "          10       0.87      0.80      0.83       110\n",
      "          11       0.93      0.95      0.94        97\n",
      "          12       0.91      0.98      0.94        89\n",
      "          13       0.88      0.92      0.90        89\n",
      "          14       0.90      0.92      0.91        93\n",
      "          15       0.99      0.96      0.98        84\n",
      "          16       0.95      0.95      0.95       103\n",
      "          17       0.90      0.91      0.91        90\n",
      "          18       0.90      0.97      0.93        92\n",
      "          19       0.85      0.86      0.86       100\n",
      "          20       0.95      0.94      0.95       107\n",
      "          21       0.83      0.85      0.84       102\n",
      "          22       0.99      0.99      0.99        97\n",
      "          23       0.89      0.84      0.86       104\n",
      "          24       0.69      0.82      0.75        95\n",
      "          25       0.97      0.98      0.97        93\n",
      "          26       0.81      0.87      0.84        97\n",
      "          27       0.91      0.93      0.92        99\n",
      "          28       0.98      0.97      0.97        92\n",
      "          29       0.98      0.96      0.97       100\n",
      "          30       0.99      0.96      0.97        94\n",
      "          31       0.72      0.80      0.76        89\n",
      "          32       0.89      0.92      0.91       110\n",
      "          33       0.90      0.92      0.91       101\n",
      "          34       0.99      0.92      0.96       116\n",
      "          35       0.91      0.95      0.93        98\n",
      "          36       0.89      0.83      0.86       104\n",
      "          37       0.94      0.91      0.92       100\n",
      "          38       0.89      0.96      0.92       112\n",
      "          39       0.96      0.95      0.95        96\n",
      "          40       0.93      0.95      0.94       100\n",
      "          41       0.93      0.92      0.92       122\n",
      "          42       0.95      0.97      0.96        77\n",
      "          43       0.86      0.83      0.85        99\n",
      "          44       0.96      0.83      0.89       108\n",
      "          45       0.94      0.99      0.97        86\n",
      "          46       1.00      0.98      0.99        83\n",
      "          47       0.92      0.84      0.88        95\n",
      "          48       0.91      0.83      0.87       102\n",
      "          49       0.81      0.89      0.85        94\n",
      "          50       0.73      0.59      0.66       103\n",
      "          51       1.00      1.00      1.00        55\n",
      "          52       0.99      1.00      0.99        75\n",
      "          53       0.90      0.87      0.89       108\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.91      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 57,  accuracy score is 1.0\n",
      "at random state 57, confusion matrix is [[ 91   0   0 ...   0   0   0]\n",
      " [  0  92   0 ...   0   0   0]\n",
      " [  0   0  83 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  42   0   0]\n",
      " [  0   0   0 ...   0  78   0]\n",
      " [  0   1   1 ...   0   0 100]]\n",
      "at random state 57, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.91      0.89       100\n",
      "           1       0.83      0.85      0.84       108\n",
      "           2       0.95      0.97      0.96        86\n",
      "           3       0.91      0.86      0.89       122\n",
      "           4       0.93      0.98      0.95        97\n",
      "           5       0.86      0.84      0.85       109\n",
      "           6       0.95      0.99      0.97        90\n",
      "           7       0.90      0.90      0.90        84\n",
      "           8       1.00      0.99      0.99        92\n",
      "           9       0.90      0.92      0.91       103\n",
      "          10       0.82      0.87      0.84        91\n",
      "          11       0.92      0.93      0.92        95\n",
      "          12       0.93      0.95      0.94        94\n",
      "          13       0.94      0.79      0.86       113\n",
      "          14       0.91      0.92      0.92       100\n",
      "          15       0.99      0.97      0.98        79\n",
      "          16       0.99      0.97      0.98        91\n",
      "          17       0.92      0.93      0.93       115\n",
      "          18       0.91      0.93      0.92       103\n",
      "          19       0.88      0.90      0.89       102\n",
      "          20       0.97      0.96      0.96        99\n",
      "          21       0.84      0.82      0.83       103\n",
      "          22       0.97      0.99      0.98        97\n",
      "          23       0.83      0.85      0.84        99\n",
      "          24       0.72      0.76      0.74        96\n",
      "          25       0.98      0.98      0.98       103\n",
      "          26       0.86      0.77      0.81        92\n",
      "          27       0.94      0.93      0.94       102\n",
      "          28       0.96      0.95      0.95        96\n",
      "          29       0.97      0.97      0.97        97\n",
      "          30       0.98      0.92      0.95       103\n",
      "          31       0.77      0.73      0.75       113\n",
      "          32       0.93      0.90      0.92       114\n",
      "          33       0.86      0.95      0.90        97\n",
      "          34       0.96      0.97      0.97       106\n",
      "          35       0.94      0.94      0.94        89\n",
      "          36       0.84      0.84      0.84       125\n",
      "          37       0.89      0.95      0.92        93\n",
      "          38       0.88      0.96      0.92        93\n",
      "          39       0.91      0.95      0.93       110\n",
      "          40       0.91      0.90      0.90        99\n",
      "          41       0.83      0.95      0.89        82\n",
      "          42       0.95      0.94      0.95        89\n",
      "          43       0.88      0.83      0.85        92\n",
      "          44       0.95      0.96      0.95        99\n",
      "          45       0.97      1.00      0.98        89\n",
      "          46       0.96      0.97      0.97        78\n",
      "          47       0.94      0.88      0.91       100\n",
      "          48       0.89      0.85      0.87       101\n",
      "          49       0.87      0.89      0.88       109\n",
      "          50       0.67      0.67      0.67       115\n",
      "          51       1.00      1.00      1.00        42\n",
      "          52       1.00      0.99      0.99        79\n",
      "          53       0.93      0.83      0.88       121\n",
      "\n",
      "    accuracy                           0.90      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.90      0.90      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 58,  accuracy score is 1.0\n",
      "at random state 58, confusion matrix is [[107   0   0 ...   0   0   0]\n",
      " [  0  84   0 ...   0   0   1]\n",
      " [  0   0 103 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  45   0   0]\n",
      " [  0   0   0 ...   0  81   0]\n",
      " [  0   1   1 ...   0   0  77]]\n",
      "at random state 58, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.91      0.87       117\n",
      "           1       0.81      0.86      0.83        98\n",
      "           2       0.98      0.98      0.98       105\n",
      "           3       0.92      0.89      0.90       107\n",
      "           4       0.97      0.98      0.97        89\n",
      "           5       0.91      0.84      0.87       100\n",
      "           6       1.00      0.97      0.98        88\n",
      "           7       0.88      0.91      0.89       101\n",
      "           8       0.96      0.97      0.96       112\n",
      "           9       0.86      0.90      0.88        92\n",
      "          10       0.78      0.84      0.81        86\n",
      "          11       0.91      0.95      0.93       100\n",
      "          12       0.98      0.97      0.98       102\n",
      "          13       0.88      0.81      0.84        90\n",
      "          14       0.92      0.94      0.93       112\n",
      "          15       1.00      0.97      0.98        90\n",
      "          16       0.95      0.95      0.95        93\n",
      "          17       0.94      0.93      0.93       112\n",
      "          18       0.90      0.93      0.92       111\n",
      "          19       0.85      0.95      0.90        93\n",
      "          20       0.97      0.94      0.96       105\n",
      "          21       0.80      0.81      0.81       100\n",
      "          22       0.94      1.00      0.97        92\n",
      "          23       0.89      0.84      0.87       103\n",
      "          24       0.72      0.83      0.77       104\n",
      "          25       0.95      0.98      0.96       108\n",
      "          26       0.86      0.81      0.83       106\n",
      "          27       0.94      0.95      0.95       110\n",
      "          28       0.98      0.97      0.98       102\n",
      "          29       0.98      0.97      0.97        99\n",
      "          30       0.95      0.96      0.95        94\n",
      "          31       0.73      0.75      0.74        95\n",
      "          32       0.93      0.91      0.92       106\n",
      "          33       0.90      0.90      0.90       105\n",
      "          34       0.98      0.95      0.96       115\n",
      "          35       0.95      0.95      0.95       100\n",
      "          36       0.89      0.88      0.89       104\n",
      "          37       0.89      0.87      0.88        87\n",
      "          38       0.91      0.91      0.91        99\n",
      "          39       0.96      0.95      0.95        98\n",
      "          40       0.91      0.94      0.93        99\n",
      "          41       0.88      0.94      0.91        87\n",
      "          42       0.92      0.95      0.93        95\n",
      "          43       0.95      0.87      0.91        92\n",
      "          44       0.91      0.88      0.89       105\n",
      "          45       0.97      0.99      0.98        89\n",
      "          46       0.96      0.96      0.96        92\n",
      "          47       0.87      0.85      0.86        95\n",
      "          48       0.90      0.83      0.86       103\n",
      "          49       0.92      0.88      0.90        98\n",
      "          50       0.64      0.53      0.58        94\n",
      "          51       1.00      1.00      1.00        45\n",
      "          52       0.99      1.00      0.99        81\n",
      "          53       0.88      0.85      0.86        91\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.91      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 59,  accuracy score is 1.0\n",
      "at random state 59, confusion matrix is [[ 92   0   0 ...   0   0   0]\n",
      " [  0  86   0 ...   0   0   0]\n",
      " [  0   0 102 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  63   0   0]\n",
      " [  0   0   0 ...   0  81   0]\n",
      " [  0   0   2 ...   0   0  98]]\n",
      "at random state 59, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.93      0.87        99\n",
      "           1       0.83      0.83      0.83       104\n",
      "           2       0.96      0.95      0.96       107\n",
      "           3       0.95      0.83      0.88        83\n",
      "           4       0.96      0.97      0.97       129\n",
      "           5       0.93      0.78      0.85        97\n",
      "           6       0.98      0.99      0.98        86\n",
      "           7       0.85      0.93      0.89        97\n",
      "           8       0.95      0.98      0.96        95\n",
      "           9       0.85      0.91      0.88       105\n",
      "          10       0.80      0.86      0.83       109\n",
      "          11       0.91      0.92      0.91        99\n",
      "          12       0.92      1.00      0.96        98\n",
      "          13       0.86      0.86      0.86        98\n",
      "          14       0.95      0.96      0.95        90\n",
      "          15       0.98      0.99      0.98        88\n",
      "          16       0.99      0.98      0.98        91\n",
      "          17       0.93      0.87      0.90       106\n",
      "          18       0.87      0.90      0.89        99\n",
      "          19       0.90      0.96      0.93        97\n",
      "          20       0.97      0.91      0.94        96\n",
      "          21       0.82      0.76      0.79       104\n",
      "          22       0.96      1.00      0.98       100\n",
      "          23       0.79      0.92      0.85        97\n",
      "          24       0.85      0.77      0.80        94\n",
      "          25       0.94      0.98      0.96       104\n",
      "          26       0.87      0.78      0.83       106\n",
      "          27       0.90      0.86      0.88        95\n",
      "          28       0.97      0.96      0.96        92\n",
      "          29       0.99      0.95      0.97       102\n",
      "          30       0.98      0.98      0.98       104\n",
      "          31       0.71      0.76      0.73        90\n",
      "          32       0.87      0.92      0.89       101\n",
      "          33       0.82      0.95      0.88        99\n",
      "          34       0.98      0.96      0.97       112\n",
      "          35       0.95      0.93      0.94       113\n",
      "          36       0.83      0.76      0.79        91\n",
      "          37       0.89      0.86      0.88       103\n",
      "          38       0.90      0.89      0.90        84\n",
      "          39       0.92      0.91      0.92       103\n",
      "          40       0.89      0.91      0.90        96\n",
      "          41       0.88      0.91      0.89       108\n",
      "          42       0.98      0.96      0.97        89\n",
      "          43       0.85      0.83      0.84        90\n",
      "          44       0.88      0.94      0.91        96\n",
      "          45       0.95      0.97      0.96        87\n",
      "          46       0.95      0.99      0.97        89\n",
      "          47       0.87      0.90      0.88        88\n",
      "          48       0.90      0.83      0.86       122\n",
      "          49       0.91      0.91      0.91       102\n",
      "          50       0.71      0.70      0.71        93\n",
      "          51       1.00      1.00      1.00        63\n",
      "          52       0.99      0.99      0.99        82\n",
      "          53       0.96      0.79      0.87       124\n",
      "\n",
      "    accuracy                           0.90      5296\n",
      "   macro avg       0.90      0.90      0.90      5296\n",
      "weighted avg       0.90      0.90      0.90      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 60,  accuracy score is 1.0\n",
      "at random state 60, confusion matrix is [[91  0  0 ...  0  0  0]\n",
      " [ 0 87  0 ...  0  0  0]\n",
      " [ 0  0 99 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 42  0  0]\n",
      " [ 0  0  0 ...  0 80  0]\n",
      " [ 0  2  3 ...  0  0 89]]\n",
      "at random state 60, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.90      0.91       101\n",
      "           1       0.76      0.91      0.82        96\n",
      "           2       0.95      0.99      0.97       100\n",
      "           3       0.95      0.85      0.89       104\n",
      "           4       0.90      0.93      0.92       101\n",
      "           5       0.84      0.84      0.84       106\n",
      "           6       0.99      0.99      0.99        86\n",
      "           7       0.94      0.94      0.94       100\n",
      "           8       0.96      1.00      0.98       102\n",
      "           9       0.91      0.89      0.90       104\n",
      "          10       0.88      0.91      0.90       114\n",
      "          11       0.94      0.96      0.95        97\n",
      "          12       0.95      0.92      0.93        97\n",
      "          13       0.91      0.78      0.84       101\n",
      "          14       0.85      0.90      0.88        90\n",
      "          15       0.99      0.99      0.99        91\n",
      "          16       0.95      0.96      0.96       104\n",
      "          17       0.91      0.88      0.89       105\n",
      "          18       0.92      0.91      0.92        89\n",
      "          19       0.94      0.91      0.93       104\n",
      "          20       0.98      0.94      0.96        96\n",
      "          21       0.79      0.86      0.83        94\n",
      "          22       0.97      0.99      0.98       103\n",
      "          23       0.85      0.94      0.90       106\n",
      "          24       0.78      0.77      0.78       104\n",
      "          25       0.99      0.99      0.99        93\n",
      "          26       0.87      0.84      0.85       111\n",
      "          27       0.89      0.90      0.89        88\n",
      "          28       0.98      0.92      0.95        93\n",
      "          29       0.98      0.97      0.97        94\n",
      "          30       0.94      0.92      0.93       109\n",
      "          31       0.79      0.65      0.71       111\n",
      "          32       0.94      0.89      0.91       115\n",
      "          33       0.91      0.94      0.92        97\n",
      "          34       0.95      0.95      0.95       101\n",
      "          35       0.95      1.00      0.97       123\n",
      "          36       0.82      0.84      0.83        95\n",
      "          37       0.89      0.87      0.88        82\n",
      "          38       0.91      0.96      0.93        90\n",
      "          39       0.91      0.96      0.93       101\n",
      "          40       0.92      0.92      0.92       107\n",
      "          41       0.94      0.96      0.95       106\n",
      "          42       0.91      1.00      0.95        84\n",
      "          43       0.90      0.87      0.88       109\n",
      "          44       0.90      0.94      0.92        95\n",
      "          45       1.00      0.98      0.99        94\n",
      "          46       0.98      1.00      0.99        94\n",
      "          47       0.92      0.88      0.90        91\n",
      "          48       0.90      0.80      0.85       104\n",
      "          49       0.87      0.88      0.87        96\n",
      "          50       0.60      0.67      0.63        88\n",
      "          51       1.00      1.00      1.00        42\n",
      "          52       0.96      1.00      0.98        80\n",
      "          53       0.91      0.82      0.86       108\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.91      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 61,  accuracy score is 1.0\n",
      "at random state 61, confusion matrix is [[ 96   0   0 ...   0   0   0]\n",
      " [  0  87   0 ...   0   0   0]\n",
      " [  0   0 104 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  51   0   0]\n",
      " [  0   0   0 ...   0  78   0]\n",
      " [  0   1   3 ...   0   0  78]]\n",
      "at random state 61, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.91      0.87       106\n",
      "           1       0.79      0.77      0.78       113\n",
      "           2       0.95      1.00      0.97       104\n",
      "           3       0.94      0.87      0.90        89\n",
      "           4       0.98      0.93      0.95        98\n",
      "           5       0.89      0.84      0.86       112\n",
      "           6       0.99      0.99      0.99        84\n",
      "           7       0.91      0.91      0.91        96\n",
      "           8       0.98      0.97      0.98       102\n",
      "           9       0.90      0.93      0.91       113\n",
      "          10       0.87      0.88      0.87       118\n",
      "          11       0.93      0.95      0.94        96\n",
      "          12       0.96      0.93      0.94       108\n",
      "          13       0.81      0.80      0.81        81\n",
      "          14       0.91      0.91      0.91       106\n",
      "          15       1.00      0.97      0.99        78\n",
      "          16       0.94      0.86      0.90       111\n",
      "          17       0.95      0.92      0.94       105\n",
      "          18       0.91      0.93      0.92       110\n",
      "          19       0.93      0.92      0.92       108\n",
      "          20       0.98      0.93      0.95        98\n",
      "          21       0.85      0.83      0.84        93\n",
      "          22       0.96      0.98      0.97        93\n",
      "          23       0.82      0.88      0.85        94\n",
      "          24       0.78      0.77      0.78        93\n",
      "          25       0.96      0.99      0.98       104\n",
      "          26       0.87      0.77      0.81       103\n",
      "          27       0.94      0.94      0.94       104\n",
      "          28       0.95      0.95      0.95       101\n",
      "          29       0.96      0.99      0.97        86\n",
      "          30       0.95      0.97      0.96       104\n",
      "          31       0.79      0.80      0.79       110\n",
      "          32       0.91      0.91      0.91       102\n",
      "          33       0.90      0.91      0.90        95\n",
      "          34       0.96      0.95      0.95        99\n",
      "          35       0.93      0.95      0.94        98\n",
      "          36       0.83      0.87      0.85        93\n",
      "          37       0.86      0.92      0.89        99\n",
      "          38       0.90      0.93      0.92       104\n",
      "          39       0.89      0.94      0.91        99\n",
      "          40       0.86      0.92      0.89       106\n",
      "          41       0.84      0.92      0.88       106\n",
      "          42       0.94      0.96      0.95        85\n",
      "          43       0.94      0.87      0.90        98\n",
      "          44       0.92      0.93      0.92        95\n",
      "          45       0.98      0.99      0.98        89\n",
      "          46       0.98      0.98      0.98        94\n",
      "          47       0.90      0.84      0.87        94\n",
      "          48       0.87      0.89      0.88        90\n",
      "          49       0.88      0.89      0.89       110\n",
      "          50       0.69      0.69      0.69        89\n",
      "          51       1.00      1.00      1.00        51\n",
      "          52       1.00      0.99      0.99        79\n",
      "          53       0.91      0.78      0.84       100\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.91      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 62,  accuracy score is 1.0\n",
      "at random state 62, confusion matrix is [[102   0   0 ...   0   0   0]\n",
      " [  0  87   0 ...   0   0   0]\n",
      " [  0   0  98 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  43   0   0]\n",
      " [  0   0   0 ...   0  74   0]\n",
      " [  0   0   0 ...   0   0  81]]\n",
      "at random state 62, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.89      0.88       114\n",
      "           1       0.82      0.86      0.84       101\n",
      "           2       0.97      0.98      0.98       100\n",
      "           3       0.91      0.85      0.88       101\n",
      "           4       0.93      0.99      0.96        84\n",
      "           5       0.87      0.81      0.84        91\n",
      "           6       1.00      1.00      1.00        88\n",
      "           7       0.97      0.92      0.94       101\n",
      "           8       0.98      0.97      0.97       115\n",
      "           9       0.88      0.88      0.88       104\n",
      "          10       0.85      0.85      0.85        96\n",
      "          11       0.87      0.98      0.92       104\n",
      "          12       0.91      0.95      0.92        91\n",
      "          13       0.84      0.82      0.83        91\n",
      "          14       0.94      0.93      0.94       104\n",
      "          15       0.98      0.98      0.98        86\n",
      "          16       0.98      0.95      0.97       108\n",
      "          17       0.89      0.94      0.92        72\n",
      "          18       0.92      0.89      0.90        96\n",
      "          19       0.93      0.92      0.93       105\n",
      "          20       0.98      0.95      0.96        98\n",
      "          21       0.82      0.80      0.81        99\n",
      "          22       0.97      0.99      0.98       104\n",
      "          23       0.81      0.95      0.87       109\n",
      "          24       0.74      0.70      0.72        94\n",
      "          25       0.96      0.98      0.97       105\n",
      "          26       0.85      0.86      0.85       105\n",
      "          27       0.97      0.91      0.94       100\n",
      "          28       1.00      0.97      0.99       118\n",
      "          29       0.99      0.97      0.98        98\n",
      "          30       0.99      0.98      0.99       106\n",
      "          31       0.74      0.83      0.78        88\n",
      "          32       0.96      0.91      0.93       109\n",
      "          33       0.91      0.88      0.89        96\n",
      "          34       0.98      0.96      0.97       101\n",
      "          35       0.93      0.94      0.94       105\n",
      "          36       0.85      0.85      0.85       103\n",
      "          37       0.86      0.94      0.90        83\n",
      "          38       0.89      0.94      0.92       103\n",
      "          39       0.95      0.93      0.94       115\n",
      "          40       0.90      0.90      0.90       105\n",
      "          41       0.93      0.89      0.91       111\n",
      "          42       0.96      0.98      0.97        99\n",
      "          43       0.90      0.88      0.89        91\n",
      "          44       0.93      0.87      0.90        99\n",
      "          45       0.93      1.00      0.96        91\n",
      "          46       0.99      0.98      0.98        89\n",
      "          47       0.92      0.85      0.89       101\n",
      "          48       0.94      0.87      0.90       104\n",
      "          49       0.91      0.92      0.92       106\n",
      "          50       0.69      0.69      0.69        96\n",
      "          51       0.96      1.00      0.98        43\n",
      "          52       0.99      0.99      0.99        75\n",
      "          53       0.86      0.85      0.86        95\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.91      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 63,  accuracy score is 1.0\n",
      "at random state 63, confusion matrix is [[ 85   0   0 ...   0   0   0]\n",
      " [  0  89   0 ...   0   0   1]\n",
      " [  0   0 106 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  42   0   0]\n",
      " [  0   0   0 ...   0  71   0]\n",
      " [  0   0   1 ...   0   0  82]]\n",
      "at random state 63, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.90      0.87        94\n",
      "           1       0.79      0.85      0.82       105\n",
      "           2       0.95      0.95      0.95       111\n",
      "           3       0.93      0.88      0.90       101\n",
      "           4       0.97      0.93      0.95        95\n",
      "           5       0.87      0.80      0.83        97\n",
      "           6       0.98      0.99      0.98        83\n",
      "           7       0.93      0.91      0.92       126\n",
      "           8       0.97      0.99      0.98       102\n",
      "           9       0.90      0.94      0.92       109\n",
      "          10       0.81      0.91      0.86       101\n",
      "          11       0.90      0.97      0.93       106\n",
      "          12       0.97      1.00      0.98        89\n",
      "          13       0.93      0.87      0.90       114\n",
      "          14       0.88      0.91      0.90       100\n",
      "          15       0.98      0.98      0.98        66\n",
      "          16       0.91      0.93      0.92       107\n",
      "          17       0.89      0.94      0.91        90\n",
      "          18       0.93      0.90      0.92       115\n",
      "          19       0.84      0.84      0.84       104\n",
      "          20       0.98      0.92      0.95       101\n",
      "          21       0.86      0.85      0.86       101\n",
      "          22       0.98      0.98      0.98        93\n",
      "          23       0.88      0.86      0.87        93\n",
      "          24       0.81      0.72      0.76       104\n",
      "          25       0.98      0.99      0.98        85\n",
      "          26       0.86      0.81      0.83       103\n",
      "          27       0.88      0.92      0.90       103\n",
      "          28       0.99      0.95      0.97       110\n",
      "          29       0.97      0.98      0.98       100\n",
      "          30       0.96      0.97      0.96        95\n",
      "          31       0.69      0.81      0.75        98\n",
      "          32       0.91      0.93      0.92       103\n",
      "          33       0.88      0.93      0.91       107\n",
      "          34       1.00      0.91      0.95        95\n",
      "          35       0.94      0.95      0.94       107\n",
      "          36       0.89      0.83      0.86        95\n",
      "          37       0.90      0.91      0.91        92\n",
      "          38       0.88      0.89      0.89        95\n",
      "          39       0.94      0.87      0.90       104\n",
      "          40       0.89      0.97      0.93        96\n",
      "          41       0.93      0.91      0.92        90\n",
      "          42       0.96      0.99      0.97        97\n",
      "          43       0.94      0.90      0.92       113\n",
      "          44       0.88      0.94      0.91        89\n",
      "          45       0.99      0.97      0.98        72\n",
      "          46       0.98      0.98      0.98        96\n",
      "          47       0.89      0.88      0.89       112\n",
      "          48       0.91      0.82      0.86       103\n",
      "          49       0.93      0.93      0.93       107\n",
      "          50       0.68      0.63      0.65       110\n",
      "          51       0.98      1.00      0.99        42\n",
      "          52       0.99      1.00      0.99        71\n",
      "          53       0.89      0.83      0.86        99\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.91      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 64,  accuracy score is 1.0\n",
      "at random state 64, confusion matrix is [[ 90   0   0 ...   0   0   0]\n",
      " [  0  93   0 ...   0   0   1]\n",
      " [  0   0 109 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  41   0   0]\n",
      " [  0   0   0 ...   1  72   0]\n",
      " [  0   0   1 ...   0   0  79]]\n",
      "at random state 64, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91        99\n",
      "           1       0.87      0.85      0.86       109\n",
      "           2       0.92      0.99      0.96       110\n",
      "           3       0.92      0.87      0.89        97\n",
      "           4       0.89      0.95      0.92        86\n",
      "           5       0.87      0.89      0.88        91\n",
      "           6       0.96      0.99      0.97        88\n",
      "           7       0.92      0.92      0.92       115\n",
      "           8       0.98      1.00      0.99       108\n",
      "           9       0.91      0.95      0.93        79\n",
      "          10       0.85      0.86      0.85       113\n",
      "          11       0.93      0.96      0.94        96\n",
      "          12       0.91      1.00      0.95        96\n",
      "          13       0.89      0.88      0.88       104\n",
      "          14       0.96      0.92      0.94       111\n",
      "          15       0.99      0.95      0.97        95\n",
      "          16       0.95      0.93      0.94       106\n",
      "          17       0.93      0.92      0.93        89\n",
      "          18       0.92      0.90      0.91        99\n",
      "          19       0.97      0.88      0.92       106\n",
      "          20       0.95      0.91      0.93        82\n",
      "          21       0.75      0.83      0.79       100\n",
      "          22       0.99      0.98      0.99       111\n",
      "          23       0.84      0.94      0.89        93\n",
      "          24       0.73      0.74      0.73        88\n",
      "          25       0.95      0.97      0.96       107\n",
      "          26       0.80      0.84      0.82       102\n",
      "          27       0.88      0.94      0.91        97\n",
      "          28       0.94      0.95      0.94        96\n",
      "          29       0.96      0.96      0.96        89\n",
      "          30       0.97      0.97      0.97       111\n",
      "          31       0.72      0.73      0.73        93\n",
      "          32       0.95      0.91      0.93       101\n",
      "          33       0.88      0.89      0.89       112\n",
      "          34       1.00      0.94      0.97        95\n",
      "          35       0.94      0.96      0.95       107\n",
      "          36       0.85      0.79      0.82       104\n",
      "          37       0.85      0.87      0.86       102\n",
      "          38       0.90      0.89      0.90       103\n",
      "          39       0.84      0.95      0.89        82\n",
      "          40       0.88      0.93      0.90       116\n",
      "          41       0.87      0.90      0.89        83\n",
      "          42       0.94      0.96      0.95        96\n",
      "          43       0.91      0.85      0.88       113\n",
      "          44       0.89      0.88      0.89        97\n",
      "          45       0.98      0.97      0.97        87\n",
      "          46       0.95      0.97      0.96        97\n",
      "          47       0.97      0.85      0.90       110\n",
      "          48       0.88      0.81      0.84       104\n",
      "          49       0.92      0.91      0.92       104\n",
      "          50       0.71      0.69      0.70       105\n",
      "          51       0.98      1.00      0.99        41\n",
      "          52       1.00      0.97      0.99        74\n",
      "          53       0.90      0.81      0.85        97\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.90      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 65,  accuracy score is 1.0\n",
      "at random state 65, confusion matrix is [[97  0  0 ...  0  0  0]\n",
      " [ 0 75  0 ...  0  0  2]\n",
      " [ 0  0 96 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 52  0  0]\n",
      " [ 0  0  0 ...  1 80  0]\n",
      " [ 0  1  0 ...  0  0 82]]\n",
      "at random state 65, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.92      0.88       106\n",
      "           1       0.77      0.81      0.79        93\n",
      "           2       0.98      0.98      0.98        98\n",
      "           3       0.91      0.85      0.88        97\n",
      "           4       0.97      0.96      0.97       108\n",
      "           5       0.93      0.85      0.89       107\n",
      "           6       0.96      0.99      0.97        75\n",
      "           7       0.95      0.92      0.94       103\n",
      "           8       0.97      0.94      0.96       107\n",
      "           9       0.95      0.91      0.93       108\n",
      "          10       0.86      0.84      0.85        95\n",
      "          11       0.90      0.93      0.92       106\n",
      "          12       0.97      0.93      0.95        95\n",
      "          13       0.90      0.84      0.87       101\n",
      "          14       0.89      0.95      0.92       102\n",
      "          15       0.97      0.97      0.97        88\n",
      "          16       0.96      0.96      0.96       114\n",
      "          17       0.94      0.90      0.92        91\n",
      "          18       0.91      0.91      0.91       111\n",
      "          19       0.91      0.92      0.91        98\n",
      "          20       0.94      0.94      0.94       108\n",
      "          21       0.81      0.81      0.81        95\n",
      "          22       1.00      1.00      1.00        99\n",
      "          23       0.83      0.92      0.87        86\n",
      "          24       0.76      0.77      0.77       111\n",
      "          25       0.94      1.00      0.97        96\n",
      "          26       0.87      0.86      0.87        96\n",
      "          27       0.88      0.92      0.90       110\n",
      "          28       0.97      0.96      0.96       100\n",
      "          29       0.99      0.98      0.99       110\n",
      "          30       0.96      0.99      0.97        97\n",
      "          31       0.75      0.74      0.74        92\n",
      "          32       0.96      0.92      0.94       109\n",
      "          33       0.89      0.93      0.91       104\n",
      "          34       0.99      0.89      0.94        98\n",
      "          35       0.89      0.91      0.90       106\n",
      "          36       0.88      0.81      0.84       104\n",
      "          37       0.86      0.94      0.90        89\n",
      "          38       0.87      0.92      0.89        96\n",
      "          39       0.89      0.94      0.91        95\n",
      "          40       0.92      0.96      0.94       104\n",
      "          41       0.81      0.96      0.88        93\n",
      "          42       0.95      0.99      0.97        72\n",
      "          43       0.90      0.87      0.88       110\n",
      "          44       0.91      0.90      0.90        98\n",
      "          45       0.95      1.00      0.98        79\n",
      "          46       0.99      0.95      0.97       100\n",
      "          47       0.91      0.88      0.89       101\n",
      "          48       0.94      0.84      0.89        90\n",
      "          49       0.88      0.89      0.88       102\n",
      "          50       0.71      0.68      0.69       105\n",
      "          51       0.98      1.00      0.99        52\n",
      "          52       0.98      0.99      0.98        81\n",
      "          53       0.87      0.78      0.82       105\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.91      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 66,  accuracy score is 1.0\n",
      "at random state 66, confusion matrix is [[86  0  0 ...  0  0  0]\n",
      " [ 0 87  0 ...  0  0  1]\n",
      " [ 0  0 95 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 51  0  0]\n",
      " [ 0  0  0 ...  0 76  0]\n",
      " [ 0  0  1 ...  0  0 84]]\n",
      "at random state 66, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.92      0.91        93\n",
      "           1       0.86      0.86      0.86       101\n",
      "           2       0.95      0.98      0.96        97\n",
      "           3       0.88      0.89      0.88        88\n",
      "           4       0.95      0.98      0.96        89\n",
      "           5       0.86      0.82      0.84        84\n",
      "           6       0.98      0.98      0.98        86\n",
      "           7       0.91      0.91      0.91        91\n",
      "           8       0.96      0.99      0.98       102\n",
      "           9       0.89      0.91      0.90       101\n",
      "          10       0.87      0.87      0.87       118\n",
      "          11       0.95      0.93      0.94       127\n",
      "          12       0.97      0.99      0.98       111\n",
      "          13       0.87      0.85      0.86       106\n",
      "          14       0.93      0.91      0.92       100\n",
      "          15       1.00      0.99      0.99        81\n",
      "          16       0.98      0.97      0.98       101\n",
      "          17       0.90      0.91      0.91       100\n",
      "          18       0.87      0.91      0.89        86\n",
      "          19       0.87      0.90      0.89       108\n",
      "          20       0.98      0.93      0.95       111\n",
      "          21       0.83      0.87      0.85       110\n",
      "          22       0.99      0.99      0.99        98\n",
      "          23       0.90      0.87      0.89       120\n",
      "          24       0.78      0.74      0.76        99\n",
      "          25       0.97      0.99      0.98        98\n",
      "          26       0.85      0.85      0.85       114\n",
      "          27       0.94      0.95      0.95       109\n",
      "          28       0.93      0.96      0.94        96\n",
      "          29       0.99      0.97      0.98        92\n",
      "          30       0.99      0.98      0.98        93\n",
      "          31       0.80      0.75      0.78       108\n",
      "          32       0.93      0.89      0.91        95\n",
      "          33       0.90      0.93      0.92       102\n",
      "          34       0.98      0.92      0.95       101\n",
      "          35       0.97      0.95      0.96       107\n",
      "          36       0.85      0.89      0.87        91\n",
      "          37       0.83      0.96      0.89        82\n",
      "          38       0.95      0.85      0.90       108\n",
      "          39       0.91      0.90      0.91        96\n",
      "          40       0.95      0.93      0.94        98\n",
      "          41       0.88      0.93      0.91       103\n",
      "          42       0.90      0.99      0.94        78\n",
      "          43       0.91      0.87      0.89        91\n",
      "          44       0.90      0.91      0.90       106\n",
      "          45       0.98      0.99      0.99       105\n",
      "          46       0.97      0.98      0.97        87\n",
      "          47       0.91      0.85      0.88       100\n",
      "          48       0.89      0.83      0.86       112\n",
      "          49       0.84      0.92      0.88        92\n",
      "          50       0.61      0.67      0.64        97\n",
      "          51       1.00      1.00      1.00        51\n",
      "          52       0.99      1.00      0.99        76\n",
      "          53       0.93      0.84      0.88       100\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.91      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 67,  accuracy score is 1.0\n",
      "at random state 67, confusion matrix is [[ 85   0   0 ...   0   0   0]\n",
      " [  0  85   0 ...   0   0   0]\n",
      " [  0   0 101 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  44   0   0]\n",
      " [  0   0   0 ...   0  86   0]\n",
      " [  0   1   2 ...   0   0  94]]\n",
      "at random state 67, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.85      0.88       100\n",
      "           1       0.80      0.79      0.79       108\n",
      "           2       0.95      0.97      0.96       104\n",
      "           3       0.92      0.91      0.91       116\n",
      "           4       0.93      0.97      0.95        99\n",
      "           5       0.76      0.86      0.81        95\n",
      "           6       0.99      1.00      0.99        87\n",
      "           7       0.91      0.87      0.89       102\n",
      "           8       0.96      0.98      0.97        97\n",
      "           9       0.98      0.89      0.93       106\n",
      "          10       0.76      0.89      0.82        87\n",
      "          11       0.94      0.97      0.95        98\n",
      "          12       0.96      0.99      0.97        95\n",
      "          13       0.80      0.89      0.85        92\n",
      "          14       0.92      0.95      0.94       102\n",
      "          15       0.99      0.95      0.97        73\n",
      "          16       0.97      0.91      0.94       104\n",
      "          17       0.95      0.88      0.91       100\n",
      "          18       0.97      0.91      0.94       103\n",
      "          19       0.88      0.92      0.90        99\n",
      "          20       0.98      0.90      0.94        91\n",
      "          21       0.77      0.80      0.78        88\n",
      "          22       0.98      1.00      0.99        92\n",
      "          23       0.91      0.95      0.93       106\n",
      "          24       0.76      0.80      0.78       109\n",
      "          25       0.91      0.97      0.94        95\n",
      "          26       0.86      0.85      0.85       119\n",
      "          27       0.92      0.89      0.90        98\n",
      "          28       0.94      0.97      0.96       103\n",
      "          29       0.97      0.86      0.91       108\n",
      "          30       0.97      0.93      0.95       103\n",
      "          31       0.74      0.81      0.77       104\n",
      "          32       0.91      0.95      0.93       104\n",
      "          33       0.85      0.92      0.89        92\n",
      "          34       0.99      0.96      0.97        89\n",
      "          35       0.86      0.94      0.90        98\n",
      "          36       0.91      0.82      0.86       109\n",
      "          37       0.81      0.90      0.86       102\n",
      "          38       0.93      0.87      0.90       105\n",
      "          39       0.94      0.93      0.94        88\n",
      "          40       0.89      0.95      0.92        92\n",
      "          41       0.93      0.94      0.93       110\n",
      "          42       0.97      0.96      0.96        93\n",
      "          43       0.88      0.90      0.89       101\n",
      "          44       0.89      0.94      0.91       101\n",
      "          45       0.97      1.00      0.98        85\n",
      "          46       0.98      0.95      0.96       100\n",
      "          47       0.95      0.85      0.90       106\n",
      "          48       0.95      0.88      0.91        98\n",
      "          49       0.87      0.84      0.86        95\n",
      "          50       0.73      0.65      0.68       102\n",
      "          51       1.00      1.00      1.00        44\n",
      "          52       0.99      1.00      0.99        86\n",
      "          53       0.95      0.83      0.89       113\n",
      "\n",
      "    accuracy                           0.90      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.90      0.91      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 68,  accuracy score is 1.0\n",
      "at random state 68, confusion matrix is [[96  0  0 ...  0  0  0]\n",
      " [ 0 87  0 ...  0  0  1]\n",
      " [ 0  0 97 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 55  0  0]\n",
      " [ 0  0  0 ...  0 79  0]\n",
      " [ 0  1  0 ...  0  0 82]]\n",
      "at random state 68, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.94      0.91       102\n",
      "           1       0.83      0.87      0.85       100\n",
      "           2       0.96      0.99      0.97        98\n",
      "           3       0.91      0.90      0.90       108\n",
      "           4       0.94      0.96      0.95       106\n",
      "           5       0.88      0.81      0.84       113\n",
      "           6       0.97      1.00      0.98        88\n",
      "           7       0.92      0.94      0.93        83\n",
      "           8       0.99      0.96      0.98        85\n",
      "           9       0.95      0.86      0.90       115\n",
      "          10       0.82      0.86      0.84        97\n",
      "          11       0.93      0.96      0.94       105\n",
      "          12       0.94      0.95      0.94       108\n",
      "          13       0.85      0.85      0.85       101\n",
      "          14       0.94      0.93      0.94       106\n",
      "          15       0.99      0.99      0.99        86\n",
      "          16       0.98      0.93      0.95        96\n",
      "          17       0.92      0.92      0.92        77\n",
      "          18       0.94      0.92      0.93       128\n",
      "          19       0.88      0.92      0.90       100\n",
      "          20       0.95      0.91      0.93        89\n",
      "          21       0.95      0.82      0.88        93\n",
      "          22       0.97      1.00      0.98        86\n",
      "          23       0.86      0.89      0.87        98\n",
      "          24       0.69      0.76      0.72        95\n",
      "          25       0.97      0.98      0.98       103\n",
      "          26       0.90      0.83      0.86        99\n",
      "          27       0.91      0.95      0.93       101\n",
      "          28       0.96      0.95      0.95        96\n",
      "          29       0.98      0.98      0.98        90\n",
      "          30       0.96      0.93      0.94        99\n",
      "          31       0.79      0.71      0.75       115\n",
      "          32       0.93      0.89      0.91        87\n",
      "          33       0.86      0.97      0.91       102\n",
      "          34       0.96      0.92      0.94       112\n",
      "          35       0.97      0.96      0.96        98\n",
      "          36       0.88      0.85      0.86        98\n",
      "          37       0.94      0.89      0.91       107\n",
      "          38       0.88      0.98      0.93        93\n",
      "          39       0.94      0.95      0.94       114\n",
      "          40       0.93      0.95      0.94       111\n",
      "          41       0.84      0.93      0.88        97\n",
      "          42       0.96      0.99      0.97        87\n",
      "          43       0.89      0.90      0.90       105\n",
      "          44       0.98      0.94      0.96        99\n",
      "          45       0.97      1.00      0.98        93\n",
      "          46       0.96      0.97      0.96        95\n",
      "          47       0.91      0.82      0.86       103\n",
      "          48       0.89      0.88      0.88        99\n",
      "          49       0.80      0.89      0.84        98\n",
      "          50       0.65      0.66      0.65        99\n",
      "          51       0.96      1.00      0.98        55\n",
      "          52       0.99      1.00      0.99        79\n",
      "          53       0.90      0.83      0.86        99\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.91      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 69,  accuracy score is 1.0\n",
      "at random state 69, confusion matrix is [[106   0   0 ...   0   0   0]\n",
      " [  0 101   0 ...   0   0   0]\n",
      " [  0   0  97 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  53   0   0]\n",
      " [  0   0   0 ...   0  82   0]\n",
      " [  0   1   0 ...   0   0  90]]\n",
      "at random state 69, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.91      0.90       116\n",
      "           1       0.89      0.85      0.87       119\n",
      "           2       0.93      1.00      0.97        97\n",
      "           3       0.95      0.87      0.91        97\n",
      "           4       0.93      0.98      0.96       100\n",
      "           5       0.90      0.78      0.84        88\n",
      "           6       0.98      0.99      0.98        87\n",
      "           7       0.94      0.92      0.93        91\n",
      "           8       0.97      1.00      0.98       115\n",
      "           9       0.90      0.94      0.92       103\n",
      "          10       0.79      0.88      0.83        88\n",
      "          11       0.93      0.94      0.94       108\n",
      "          12       0.96      0.94      0.95        99\n",
      "          13       0.88      0.87      0.88        94\n",
      "          14       0.90      0.90      0.90       106\n",
      "          15       0.96      0.95      0.96        85\n",
      "          16       0.97      0.96      0.97       116\n",
      "          17       0.97      0.92      0.95       113\n",
      "          18       0.89      0.98      0.93        98\n",
      "          19       0.87      0.87      0.87        86\n",
      "          20       0.96      0.96      0.96        97\n",
      "          21       0.77      0.79      0.78        98\n",
      "          22       0.99      1.00      1.00       113\n",
      "          23       0.87      0.93      0.90       102\n",
      "          24       0.76      0.72      0.74       101\n",
      "          25       0.92      0.95      0.94        85\n",
      "          26       0.88      0.79      0.84       102\n",
      "          27       0.90      0.93      0.92       106\n",
      "          28       0.93      0.94      0.94       102\n",
      "          29       0.99      0.95      0.97        86\n",
      "          30       0.98      0.95      0.97       109\n",
      "          31       0.82      0.72      0.77       105\n",
      "          32       0.90      0.93      0.91        94\n",
      "          33       0.87      0.88      0.88       101\n",
      "          34       0.98      0.89      0.93       115\n",
      "          35       0.96      0.95      0.95       100\n",
      "          36       0.87      0.84      0.86        99\n",
      "          37       0.92      0.87      0.89       100\n",
      "          38       0.85      0.93      0.89        84\n",
      "          39       0.93      0.95      0.94        96\n",
      "          40       0.83      0.97      0.89        88\n",
      "          41       0.91      0.87      0.89       110\n",
      "          42       0.95      0.92      0.94        91\n",
      "          43       0.91      0.87      0.89        99\n",
      "          44       0.91      0.92      0.91        86\n",
      "          45       0.97      0.97      0.97        89\n",
      "          46       0.94      0.99      0.96        93\n",
      "          47       0.89      0.93      0.91       101\n",
      "          48       0.90      0.87      0.89       102\n",
      "          49       0.91      0.91      0.91       103\n",
      "          50       0.64      0.76      0.69        90\n",
      "          51       1.00      1.00      1.00        53\n",
      "          52       0.99      1.00      0.99        82\n",
      "          53       0.94      0.83      0.88       108\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.91      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 70,  accuracy score is 1.0\n",
      "at random state 70, confusion matrix is [[94  0  0 ...  0  0  0]\n",
      " [ 0 83  0 ...  0  0  0]\n",
      " [ 0  0 92 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 46  0  0]\n",
      " [ 0  0  0 ...  1 74  0]\n",
      " [ 0  2  1 ...  0  0 79]]\n",
      "at random state 70, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.90      0.86       104\n",
      "           1       0.76      0.82      0.79       101\n",
      "           2       0.96      0.96      0.96        96\n",
      "           3       0.94      0.93      0.93        96\n",
      "           4       0.97      0.97      0.97       102\n",
      "           5       0.89      0.80      0.84       111\n",
      "           6       0.98      0.99      0.98        82\n",
      "           7       0.88      0.96      0.92        92\n",
      "           8       0.95      0.96      0.95       109\n",
      "           9       0.88      0.88      0.88       104\n",
      "          10       0.84      0.87      0.86       119\n",
      "          11       0.93      0.94      0.93        96\n",
      "          12       0.94      0.92      0.93       110\n",
      "          13       0.87      0.82      0.84       111\n",
      "          14       0.93      0.87      0.90       111\n",
      "          15       0.99      0.97      0.98        79\n",
      "          16       0.96      0.97      0.96       112\n",
      "          17       0.91      0.91      0.91        99\n",
      "          18       0.90      0.94      0.92        90\n",
      "          19       0.85      0.91      0.88       102\n",
      "          20       0.99      0.93      0.96       106\n",
      "          21       0.84      0.78      0.81        98\n",
      "          22       0.99      0.99      0.99        99\n",
      "          23       0.88      0.92      0.90        93\n",
      "          24       0.82      0.77      0.79       104\n",
      "          25       0.99      0.97      0.98        86\n",
      "          26       0.89      0.76      0.82       104\n",
      "          27       0.89      0.91      0.90       107\n",
      "          28       0.94      0.97      0.95        92\n",
      "          29       0.99      0.97      0.98        97\n",
      "          30       0.98      0.98      0.98       104\n",
      "          31       0.72      0.78      0.75        93\n",
      "          32       0.92      0.88      0.90        94\n",
      "          33       0.93      0.90      0.92        94\n",
      "          34       0.97      0.95      0.96       101\n",
      "          35       0.95      0.97      0.96       107\n",
      "          36       0.85      0.89      0.87        99\n",
      "          37       0.83      0.82      0.83       101\n",
      "          38       0.84      0.88      0.86        94\n",
      "          39       0.93      0.98      0.95        94\n",
      "          40       0.90      0.96      0.93       102\n",
      "          41       0.89      0.88      0.89       118\n",
      "          42       0.96      0.99      0.97        79\n",
      "          43       0.93      0.87      0.90       101\n",
      "          44       0.89      0.88      0.88        96\n",
      "          45       0.91      0.98      0.94        83\n",
      "          46       0.97      0.94      0.96        90\n",
      "          47       0.93      0.92      0.93       106\n",
      "          48       0.91      0.86      0.89       100\n",
      "          49       0.85      0.85      0.85       102\n",
      "          50       0.69      0.65      0.67       111\n",
      "          51       0.98      1.00      0.99        46\n",
      "          52       0.96      0.97      0.97        76\n",
      "          53       0.88      0.85      0.86        93\n",
      "\n",
      "    accuracy                           0.90      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.90      0.90      0.90      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 71,  accuracy score is 1.0\n",
      "at random state 71, confusion matrix is [[106   0   0 ...   0   0   0]\n",
      " [  0  84   0 ...   0   0   0]\n",
      " [  0   0 102 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  57   0   0]\n",
      " [  0   0   0 ...   0  76   0]\n",
      " [  0   0   1 ...   0   0  94]]\n",
      "at random state 71, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.88      0.88       120\n",
      "           1       0.88      0.85      0.86        99\n",
      "           2       0.96      0.99      0.98       103\n",
      "           3       0.87      0.86      0.87       102\n",
      "           4       0.96      0.95      0.96       107\n",
      "           5       0.82      0.85      0.84       103\n",
      "           6       0.99      0.99      0.99        87\n",
      "           7       0.86      0.93      0.89       102\n",
      "           8       0.98      0.94      0.96       104\n",
      "           9       0.95      0.91      0.93       118\n",
      "          10       0.86      0.90      0.88        88\n",
      "          11       0.95      0.96      0.96       112\n",
      "          12       0.96      0.96      0.96        91\n",
      "          13       0.86      0.87      0.87       103\n",
      "          14       0.91      0.92      0.91       109\n",
      "          15       0.97      1.00      0.99        70\n",
      "          16       0.92      0.94      0.93       103\n",
      "          17       0.93      0.94      0.93        97\n",
      "          18       0.93      0.91      0.92       108\n",
      "          19       0.88      0.92      0.90       102\n",
      "          20       0.98      0.94      0.96       104\n",
      "          21       0.80      0.86      0.83       105\n",
      "          22       0.97      0.98      0.98       101\n",
      "          23       0.87      0.86      0.87       101\n",
      "          24       0.82      0.70      0.76        97\n",
      "          25       0.97      0.97      0.97       108\n",
      "          26       0.89      0.87      0.88        92\n",
      "          27       0.89      0.90      0.89       103\n",
      "          28       0.96      0.94      0.95       110\n",
      "          29       0.96      0.96      0.96       101\n",
      "          30       0.97      0.97      0.97       100\n",
      "          31       0.74      0.80      0.77        99\n",
      "          32       0.86      0.90      0.88        89\n",
      "          33       0.91      0.86      0.88       104\n",
      "          34       0.99      0.94      0.96       101\n",
      "          35       0.89      0.95      0.92        96\n",
      "          36       0.89      0.84      0.87        90\n",
      "          37       0.87      0.84      0.86        90\n",
      "          38       0.91      0.88      0.89        88\n",
      "          39       0.85      0.89      0.87        79\n",
      "          40       0.88      0.93      0.91        99\n",
      "          41       0.84      0.85      0.84        95\n",
      "          42       0.94      0.98      0.96        95\n",
      "          43       0.93      0.88      0.90        99\n",
      "          44       0.92      0.90      0.91       100\n",
      "          45       0.98      0.99      0.98        89\n",
      "          46       0.99      0.96      0.97        93\n",
      "          47       0.92      0.84      0.88       118\n",
      "          48       0.91      0.86      0.88        92\n",
      "          49       0.89      0.87      0.88       102\n",
      "          50       0.58      0.66      0.62        86\n",
      "          51       0.98      1.00      0.99        57\n",
      "          52       0.96      1.00      0.98        76\n",
      "          53       0.93      0.86      0.90       109\n",
      "\n",
      "    accuracy                           0.90      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.90      0.90      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 72,  accuracy score is 1.0\n",
      "at random state 72, confusion matrix is [[81  0  0 ...  0  0  0]\n",
      " [ 0 90  0 ...  0  0  0]\n",
      " [ 0  0 91 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 51  0  0]\n",
      " [ 0  0  0 ...  1 78  0]\n",
      " [ 0  0  1 ...  0  0 87]]\n",
      "at random state 72, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.84      0.86        97\n",
      "           1       0.79      0.87      0.83       104\n",
      "           2       0.95      0.99      0.97        92\n",
      "           3       0.90      0.90      0.90       111\n",
      "           4       0.97      0.98      0.98       105\n",
      "           5       0.87      0.83      0.85       105\n",
      "           6       0.97      1.00      0.99        74\n",
      "           7       0.94      0.90      0.92       115\n",
      "           8       0.99      0.98      0.98        99\n",
      "           9       0.88      0.92      0.90        99\n",
      "          10       0.86      0.84      0.85       106\n",
      "          11       0.91      0.95      0.93       100\n",
      "          12       0.91      0.96      0.93       112\n",
      "          13       0.93      0.82      0.87       100\n",
      "          14       0.92      0.91      0.92       107\n",
      "          15       1.00      0.99      0.99        78\n",
      "          16       0.95      0.97      0.96       106\n",
      "          17       0.95      0.91      0.93        88\n",
      "          18       0.90      0.87      0.89       108\n",
      "          19       0.90      0.95      0.93       111\n",
      "          20       0.98      0.89      0.93       104\n",
      "          21       0.85      0.78      0.81       107\n",
      "          22       1.00      1.00      1.00        91\n",
      "          23       0.83      0.79      0.81       104\n",
      "          24       0.75      0.72      0.73       100\n",
      "          25       0.98      0.99      0.98        83\n",
      "          26       0.79      0.81      0.80       102\n",
      "          27       0.91      0.93      0.92        95\n",
      "          28       0.96      0.96      0.96        96\n",
      "          29       0.98      0.95      0.97       103\n",
      "          30       0.97      0.93      0.95        97\n",
      "          31       0.71      0.76      0.74        84\n",
      "          32       0.91      0.94      0.93       102\n",
      "          33       0.88      0.92      0.90       123\n",
      "          34       0.97      0.95      0.96       100\n",
      "          35       0.96      0.99      0.98       104\n",
      "          36       0.82      0.82      0.82        95\n",
      "          37       0.83      0.93      0.88        96\n",
      "          38       0.90      0.94      0.92        95\n",
      "          39       0.91      0.98      0.94        94\n",
      "          40       0.89      0.94      0.91        94\n",
      "          41       0.80      0.92      0.86        88\n",
      "          42       0.98      0.98      0.98        89\n",
      "          43       0.96      0.92      0.94       106\n",
      "          44       0.92      0.86      0.89       108\n",
      "          45       0.97      0.98      0.97        89\n",
      "          46       0.99      0.97      0.98       108\n",
      "          47       0.93      0.92      0.93       102\n",
      "          48       0.93      0.86      0.89        86\n",
      "          49       0.86      0.90      0.88       106\n",
      "          50       0.64      0.64      0.64        92\n",
      "          51       0.98      1.00      0.99        51\n",
      "          52       1.00      0.97      0.99        80\n",
      "          53       0.92      0.83      0.87       105\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.91      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 73,  accuracy score is 1.0\n",
      "at random state 73, confusion matrix is [[97  0  0 ...  0  0  0]\n",
      " [ 0 99  0 ...  0  0  0]\n",
      " [ 0  0 89 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 46  0  0]\n",
      " [ 0  0  0 ...  0 71  0]\n",
      " [ 0  0  1 ...  0  0 71]]\n",
      "at random state 73, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.91       109\n",
      "           1       0.85      0.85      0.85       117\n",
      "           2       0.96      1.00      0.98        89\n",
      "           3       0.94      0.92      0.93        97\n",
      "           4       0.98      0.95      0.96       111\n",
      "           5       0.86      0.84      0.85        99\n",
      "           6       1.00      1.00      1.00        96\n",
      "           7       0.91      0.93      0.92       103\n",
      "           8       0.99      0.97      0.98       103\n",
      "           9       0.93      0.92      0.92        99\n",
      "          10       0.88      0.85      0.86       116\n",
      "          11       0.89      0.96      0.93        97\n",
      "          12       0.96      0.99      0.98       108\n",
      "          13       0.87      0.89      0.88       104\n",
      "          14       0.89      0.91      0.90        99\n",
      "          15       0.98      0.98      0.98        89\n",
      "          16       0.94      0.98      0.96        98\n",
      "          17       0.93      0.91      0.92       104\n",
      "          18       0.92      0.87      0.89       104\n",
      "          19       0.84      0.90      0.87        97\n",
      "          20       0.97      0.97      0.97        97\n",
      "          21       0.85      0.83      0.84       101\n",
      "          22       0.98      0.99      0.98        92\n",
      "          23       0.89      0.88      0.89       101\n",
      "          24       0.75      0.73      0.74       105\n",
      "          25       0.96      1.00      0.98        97\n",
      "          26       0.90      0.85      0.87       110\n",
      "          27       0.89      0.96      0.92        97\n",
      "          28       0.95      0.97      0.96        89\n",
      "          29       0.99      0.97      0.98       100\n",
      "          30       0.99      0.95      0.97       103\n",
      "          31       0.86      0.75      0.80       100\n",
      "          32       0.88      0.91      0.89       102\n",
      "          33       0.84      0.93      0.88       114\n",
      "          34       0.96      0.94      0.95       111\n",
      "          35       0.93      0.93      0.93        84\n",
      "          36       0.91      0.81      0.86       106\n",
      "          37       0.86      0.90      0.88        88\n",
      "          38       0.92      0.91      0.92       102\n",
      "          39       0.94      0.92      0.93        91\n",
      "          40       0.85      0.92      0.88       114\n",
      "          41       0.90      0.87      0.89       109\n",
      "          42       0.94      0.98      0.96        87\n",
      "          43       0.90      0.94      0.92       100\n",
      "          44       0.91      0.91      0.91        97\n",
      "          45       0.98      0.98      0.98        86\n",
      "          46       0.91      0.97      0.94        70\n",
      "          47       0.86      0.87      0.86        99\n",
      "          48       0.92      0.90      0.91        87\n",
      "          49       0.92      0.92      0.92       113\n",
      "          50       0.66      0.70      0.68        91\n",
      "          51       1.00      1.00      1.00        46\n",
      "          52       1.00      0.99      0.99        72\n",
      "          53       0.88      0.74      0.80        96\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.91      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 74,  accuracy score is 1.0\n",
      "at random state 74, confusion matrix is [[77  0  0 ...  0  0  0]\n",
      " [ 0 96  0 ...  0  0  0]\n",
      " [ 0  0 96 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 47  0  0]\n",
      " [ 0  0  0 ...  0 84  0]\n",
      " [ 0  0  0 ...  0  0 87]]\n",
      "at random state 74, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.96      0.92        80\n",
      "           1       0.89      0.78      0.83       123\n",
      "           2       0.95      0.97      0.96        99\n",
      "           3       0.93      0.83      0.87       105\n",
      "           4       0.95      0.94      0.95       105\n",
      "           5       0.88      0.86      0.87       107\n",
      "           6       0.98      1.00      0.99        81\n",
      "           7       0.96      0.89      0.93       104\n",
      "           8       0.96      1.00      0.98        98\n",
      "           9       0.89      0.90      0.90       101\n",
      "          10       0.78      0.92      0.85        89\n",
      "          11       0.95      0.95      0.95       106\n",
      "          12       0.92      0.97      0.94        94\n",
      "          13       0.82      0.84      0.83        97\n",
      "          14       0.87      0.89      0.88        91\n",
      "          15       0.99      1.00      0.99        79\n",
      "          16       0.96      0.93      0.94        91\n",
      "          17       0.92      0.94      0.93       121\n",
      "          18       0.91      0.94      0.92        97\n",
      "          19       0.92      0.93      0.93       106\n",
      "          20       0.99      0.93      0.96       120\n",
      "          21       0.81      0.85      0.83       104\n",
      "          22       0.98      0.98      0.98        89\n",
      "          23       0.85      0.88      0.86        93\n",
      "          24       0.78      0.76      0.77        99\n",
      "          25       0.99      1.00      0.99        99\n",
      "          26       0.83      0.75      0.79       110\n",
      "          27       0.91      0.88      0.89       100\n",
      "          28       0.96      0.96      0.96        96\n",
      "          29       0.95      0.97      0.96        94\n",
      "          30       0.98      0.95      0.97       117\n",
      "          31       0.74      0.74      0.74       101\n",
      "          32       0.88      0.89      0.88        96\n",
      "          33       0.90      0.90      0.90       107\n",
      "          34       0.98      0.95      0.97       109\n",
      "          35       0.94      0.94      0.94       107\n",
      "          36       0.91      0.82      0.86       107\n",
      "          37       0.88      0.88      0.88        91\n",
      "          38       0.85      0.96      0.90        94\n",
      "          39       0.91      0.93      0.92       103\n",
      "          40       0.90      0.98      0.94        95\n",
      "          41       0.87      0.84      0.86        82\n",
      "          42       0.94      0.95      0.94        93\n",
      "          43       0.88      0.85      0.86       120\n",
      "          44       0.88      0.90      0.89       102\n",
      "          45       0.99      0.97      0.98        80\n",
      "          46       0.97      0.98      0.97        93\n",
      "          47       0.93      0.86      0.89        98\n",
      "          48       0.85      0.84      0.84        93\n",
      "          49       0.90      0.88      0.89       108\n",
      "          50       0.59      0.69      0.63        89\n",
      "          51       1.00      1.00      1.00        47\n",
      "          52       0.99      1.00      0.99        84\n",
      "          53       0.92      0.85      0.88       102\n",
      "\n",
      "    accuracy                           0.90      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.90      0.90      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 75,  accuracy score is 1.0\n",
      "at random state 75, confusion matrix is [[ 89   0   0 ...   0   0   0]\n",
      " [  0  85   0 ...   0   0   0]\n",
      " [  0   0 114 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  48   0   0]\n",
      " [  0   0   0 ...   0  54   0]\n",
      " [  0   0   1 ...   0   0  88]]\n",
      "at random state 75, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.89      0.89       100\n",
      "           1       0.86      0.85      0.85       100\n",
      "           2       0.97      0.98      0.98       116\n",
      "           3       0.91      0.86      0.88        92\n",
      "           4       0.96      0.97      0.97       101\n",
      "           5       0.88      0.85      0.86       104\n",
      "           6       0.95      0.99      0.97        80\n",
      "           7       0.94      0.90      0.92       114\n",
      "           8       0.97      0.99      0.98        98\n",
      "           9       0.93      0.89      0.91       101\n",
      "          10       0.88      0.94      0.91       112\n",
      "          11       0.95      0.95      0.95       115\n",
      "          12       0.95      0.97      0.96       105\n",
      "          13       0.88      0.89      0.88       103\n",
      "          14       0.88      0.92      0.90       108\n",
      "          15       0.97      0.94      0.95        78\n",
      "          16       0.96      0.95      0.95       110\n",
      "          17       0.96      0.95      0.95       111\n",
      "          18       0.92      0.89      0.91        92\n",
      "          19       0.90      0.88      0.89       105\n",
      "          20       1.00      0.96      0.98        99\n",
      "          21       0.80      0.85      0.82        92\n",
      "          22       0.98      0.99      0.98        94\n",
      "          23       0.86      0.86      0.86       109\n",
      "          24       0.81      0.78      0.79        91\n",
      "          25       0.92      0.99      0.95        94\n",
      "          26       0.84      0.83      0.83        94\n",
      "          27       0.86      0.95      0.91        85\n",
      "          28       0.98      0.96      0.97        99\n",
      "          29       0.99      0.98      0.99       111\n",
      "          30       0.98      0.98      0.98       113\n",
      "          31       0.77      0.70      0.73        99\n",
      "          32       0.93      0.84      0.89       102\n",
      "          33       0.84      0.89      0.87        94\n",
      "          34       0.94      0.92      0.93        89\n",
      "          35       0.99      0.94      0.97       106\n",
      "          36       0.82      0.74      0.78        96\n",
      "          37       0.91      0.93      0.92       105\n",
      "          38       0.86      0.97      0.91       106\n",
      "          39       0.95      0.96      0.95       108\n",
      "          40       0.92      0.95      0.94       103\n",
      "          41       0.91      0.95      0.93        87\n",
      "          42       0.98      0.92      0.95        98\n",
      "          43       0.84      0.93      0.88        94\n",
      "          44       0.94      0.93      0.93        94\n",
      "          45       0.97      0.97      0.97        95\n",
      "          46       0.96      0.99      0.98       104\n",
      "          47       0.92      0.86      0.89        99\n",
      "          48       0.94      0.86      0.90        87\n",
      "          49       0.89      0.88      0.89       113\n",
      "          50       0.57      0.63      0.60        83\n",
      "          51       0.98      1.00      0.99        48\n",
      "          52       0.92      0.98      0.95        55\n",
      "          53       0.88      0.84      0.86       105\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.91      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 76,  accuracy score is 1.0\n",
      "at random state 76, confusion matrix is [[105   0   0 ...   0   0   0]\n",
      " [  0  84   0 ...   0   0   0]\n",
      " [  0   0  88 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  52   0   0]\n",
      " [  0   0   0 ...   1  76   0]\n",
      " [  0   1   2 ...   0   0  83]]\n",
      "at random state 76, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.89      0.89       118\n",
      "           1       0.83      0.86      0.84        98\n",
      "           2       0.93      0.97      0.95        91\n",
      "           3       0.92      0.89      0.90       106\n",
      "           4       0.96      0.98      0.97       111\n",
      "           5       0.86      0.83      0.85        89\n",
      "           6       0.99      0.97      0.98        91\n",
      "           7       0.94      0.92      0.93       111\n",
      "           8       0.97      0.99      0.98        99\n",
      "           9       0.88      0.94      0.91        96\n",
      "          10       0.78      0.88      0.83        94\n",
      "          11       0.94      0.93      0.94       111\n",
      "          12       0.94      0.96      0.95       105\n",
      "          13       0.87      0.86      0.86        97\n",
      "          14       0.89      0.93      0.91        97\n",
      "          15       0.96      0.96      0.96        83\n",
      "          16       0.95      0.97      0.96        90\n",
      "          17       0.91      0.92      0.91        84\n",
      "          18       0.89      0.90      0.89       103\n",
      "          19       0.93      0.87      0.90       110\n",
      "          20       0.99      0.89      0.94       122\n",
      "          21       0.78      0.86      0.82        93\n",
      "          22       0.98      1.00      0.99       113\n",
      "          23       0.88      0.90      0.89       103\n",
      "          24       0.77      0.69      0.73       106\n",
      "          25       0.98      0.96      0.97       106\n",
      "          26       0.81      0.75      0.78       102\n",
      "          27       0.88      0.87      0.88       114\n",
      "          28       0.99      0.95      0.97       115\n",
      "          29       0.97      0.98      0.97        94\n",
      "          30       0.99      0.94      0.96        84\n",
      "          31       0.71      0.72      0.71        97\n",
      "          32       0.95      0.91      0.93       100\n",
      "          33       0.90      0.95      0.93       100\n",
      "          34       0.97      0.93      0.95       109\n",
      "          35       0.90      0.96      0.93        95\n",
      "          36       0.92      0.78      0.84        90\n",
      "          37       0.82      0.88      0.84        96\n",
      "          38       0.86      0.86      0.86        95\n",
      "          39       0.94      0.95      0.95       105\n",
      "          40       0.87      0.97      0.92       100\n",
      "          41       0.93      0.85      0.89       106\n",
      "          42       0.98      0.99      0.98        92\n",
      "          43       0.93      0.95      0.94       105\n",
      "          44       0.86      0.95      0.90        93\n",
      "          45       0.99      1.00      0.99        80\n",
      "          46       0.96      0.99      0.97        98\n",
      "          47       0.92      0.85      0.89        95\n",
      "          48       0.88      0.79      0.83        99\n",
      "          49       0.84      0.90      0.87        84\n",
      "          50       0.62      0.69      0.65        96\n",
      "          51       0.96      1.00      0.98        52\n",
      "          52       0.99      0.99      0.99        77\n",
      "          53       0.95      0.86      0.91        96\n",
      "\n",
      "    accuracy                           0.90      5296\n",
      "   macro avg       0.91      0.91      0.90      5296\n",
      "weighted avg       0.91      0.90      0.90      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 77,  accuracy score is 1.0\n",
      "at random state 77, confusion matrix is [[ 91   0   0 ...   0   0   0]\n",
      " [  0  84   0 ...   0   0   0]\n",
      " [  0   0 104 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  57   0   0]\n",
      " [  0   0   0 ...   1  78   0]\n",
      " [  0   0   3 ...   0   0  68]]\n",
      "at random state 77, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.88      0.87       103\n",
      "           1       0.81      0.89      0.85        94\n",
      "           2       0.97      0.99      0.98       105\n",
      "           3       0.94      0.81      0.87       111\n",
      "           4       0.94      1.00      0.97        93\n",
      "           5       0.87      0.82      0.84       104\n",
      "           6       0.97      1.00      0.98        89\n",
      "           7       0.88      0.91      0.89       100\n",
      "           8       0.95      0.98      0.96        96\n",
      "           9       0.92      0.91      0.92       119\n",
      "          10       0.81      0.90      0.85       110\n",
      "          11       0.92      0.94      0.93       108\n",
      "          12       0.96      0.99      0.98       103\n",
      "          13       0.87      0.87      0.87       101\n",
      "          14       0.85      0.91      0.88        90\n",
      "          15       0.99      0.98      0.98        93\n",
      "          16       0.99      0.94      0.97       104\n",
      "          17       0.88      0.89      0.89       102\n",
      "          18       0.86      0.96      0.91        91\n",
      "          19       0.94      0.89      0.91       108\n",
      "          20       0.99      0.94      0.96       110\n",
      "          21       0.83      0.86      0.84       104\n",
      "          22       0.99      0.99      0.99       101\n",
      "          23       0.79      0.89      0.83        99\n",
      "          24       0.80      0.63      0.71       103\n",
      "          25       0.99      0.98      0.99       101\n",
      "          26       0.88      0.78      0.83       105\n",
      "          27       0.88      0.91      0.89        92\n",
      "          28       0.95      0.90      0.93        83\n",
      "          29       0.96      0.95      0.95        98\n",
      "          30       0.97      0.94      0.96        82\n",
      "          31       0.78      0.81      0.79        98\n",
      "          32       0.89      0.90      0.90       102\n",
      "          33       0.92      0.92      0.92       109\n",
      "          34       0.95      0.93      0.94       103\n",
      "          35       0.94      0.97      0.96       104\n",
      "          36       0.90      0.85      0.88       110\n",
      "          37       0.83      0.90      0.87       100\n",
      "          38       0.89      0.92      0.90        87\n",
      "          39       0.92      0.93      0.92        99\n",
      "          40       0.93      0.89      0.91        97\n",
      "          41       0.87      0.86      0.86       106\n",
      "          42       0.92      0.95      0.94        84\n",
      "          43       0.88      0.87      0.88        93\n",
      "          44       0.93      0.86      0.90       100\n",
      "          45       0.99      0.96      0.97        80\n",
      "          46       0.95      1.00      0.97        86\n",
      "          47       0.93      0.87      0.90       129\n",
      "          48       0.85      0.91      0.88        95\n",
      "          49       0.85      0.84      0.84        97\n",
      "          50       0.67      0.77      0.72        94\n",
      "          51       0.98      1.00      0.99        57\n",
      "          52       0.97      0.99      0.98        79\n",
      "          53       0.96      0.80      0.87        85\n",
      "\n",
      "    accuracy                           0.90      5296\n",
      "   macro avg       0.91      0.91      0.90      5296\n",
      "weighted avg       0.91      0.90      0.90      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 78,  accuracy score is 1.0\n",
      "at random state 78, confusion matrix is [[92  0  0 ...  0  0  0]\n",
      " [ 0 90  0 ...  0  0  1]\n",
      " [ 0  0 98 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 55  0  0]\n",
      " [ 0  0  0 ...  1 62  0]\n",
      " [ 0  0  1 ...  0  0 72]]\n",
      "at random state 78, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.84      0.86       109\n",
      "           1       0.83      0.88      0.85       102\n",
      "           2       0.93      0.97      0.95       101\n",
      "           3       0.93      0.85      0.89       106\n",
      "           4       0.94      0.96      0.95        85\n",
      "           5       0.83      0.82      0.83        90\n",
      "           6       0.99      0.98      0.98        95\n",
      "           7       0.89      0.95      0.92        95\n",
      "           8       0.98      0.97      0.97       122\n",
      "           9       0.91      0.91      0.91       104\n",
      "          10       0.82      0.89      0.85        96\n",
      "          11       0.94      0.94      0.94       107\n",
      "          12       0.98      0.95      0.97       109\n",
      "          13       0.84      0.82      0.83        99\n",
      "          14       0.90      0.92      0.91        84\n",
      "          15       0.96      0.96      0.96        82\n",
      "          16       0.99      0.93      0.96       110\n",
      "          17       0.89      0.93      0.91        90\n",
      "          18       0.92      0.94      0.93       108\n",
      "          19       0.87      0.98      0.92        89\n",
      "          20       0.99      0.89      0.93        97\n",
      "          21       0.86      0.81      0.84        94\n",
      "          22       0.97      1.00      0.99       101\n",
      "          23       0.88      0.90      0.89       109\n",
      "          24       0.79      0.76      0.78       118\n",
      "          25       0.95      0.98      0.96        98\n",
      "          26       0.84      0.82      0.83        99\n",
      "          27       0.93      0.89      0.91       112\n",
      "          28       0.96      0.92      0.94       105\n",
      "          29       0.98      0.98      0.98        82\n",
      "          30       0.95      0.96      0.96       106\n",
      "          31       0.82      0.70      0.75       105\n",
      "          32       0.91      0.89      0.90        96\n",
      "          33       0.83      0.97      0.89        94\n",
      "          34       0.95      0.91      0.93        99\n",
      "          35       0.96      0.97      0.96        89\n",
      "          36       0.89      0.82      0.85        82\n",
      "          37       0.93      0.89      0.91       113\n",
      "          38       0.91      0.92      0.91       109\n",
      "          39       0.91      0.94      0.93       108\n",
      "          40       0.91      0.92      0.91       104\n",
      "          41       0.88      0.88      0.88       101\n",
      "          42       0.93      0.97      0.95        96\n",
      "          43       0.96      0.91      0.94       105\n",
      "          44       0.92      0.96      0.94        91\n",
      "          45       0.98      1.00      0.99        98\n",
      "          46       0.98      1.00      0.99        96\n",
      "          47       0.91      0.87      0.89       117\n",
      "          48       0.93      0.91      0.92        98\n",
      "          49       0.78      0.92      0.85        77\n",
      "          50       0.62      0.70      0.66        99\n",
      "          51       0.98      1.00      0.99        55\n",
      "          52       0.97      0.95      0.96        65\n",
      "          53       0.90      0.76      0.82        95\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.91      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 79,  accuracy score is 1.0\n",
      "at random state 79, confusion matrix is [[100   0   0 ...   0   0   0]\n",
      " [  0  71   0 ...   0   0   1]\n",
      " [  0   0 115 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  52   0   0]\n",
      " [  0   0   0 ...   0  85   0]\n",
      " [  0   0   0 ...   0   0  69]]\n",
      "at random state 79, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.90      0.89       111\n",
      "           1       0.78      0.88      0.83        81\n",
      "           2       0.99      0.97      0.98       119\n",
      "           3       0.96      0.87      0.91        98\n",
      "           4       0.94      0.96      0.95       105\n",
      "           5       0.93      0.84      0.88       104\n",
      "           6       1.00      0.99      0.99        78\n",
      "           7       0.90      0.91      0.90        95\n",
      "           8       0.98      0.98      0.98        90\n",
      "           9       0.97      0.87      0.92       100\n",
      "          10       0.80      0.81      0.81       101\n",
      "          11       0.93      0.95      0.94       105\n",
      "          12       0.93      0.99      0.96       102\n",
      "          13       0.90      0.85      0.87        98\n",
      "          14       0.90      0.93      0.91       103\n",
      "          15       1.00      0.93      0.96        73\n",
      "          16       0.97      0.92      0.94        83\n",
      "          17       0.97      0.92      0.94       102\n",
      "          18       0.95      0.91      0.93       117\n",
      "          19       0.87      0.90      0.89        90\n",
      "          20       0.93      0.97      0.95        88\n",
      "          21       0.77      0.84      0.80        99\n",
      "          22       0.97      0.98      0.98       106\n",
      "          23       0.87      0.91      0.89       102\n",
      "          24       0.80      0.72      0.76       112\n",
      "          25       0.92      0.98      0.95        91\n",
      "          26       0.87      0.78      0.82       103\n",
      "          27       0.92      0.94      0.93       109\n",
      "          28       0.97      0.96      0.96        98\n",
      "          29       0.99      0.98      0.99       114\n",
      "          30       0.97      1.00      0.98        98\n",
      "          31       0.81      0.79      0.80        99\n",
      "          32       0.88      0.94      0.91        97\n",
      "          33       0.85      0.92      0.88        98\n",
      "          34       0.96      0.96      0.96        94\n",
      "          35       0.90      0.95      0.92        97\n",
      "          36       0.83      0.81      0.82        96\n",
      "          37       0.94      0.92      0.93       100\n",
      "          38       0.90      0.97      0.93       115\n",
      "          39       0.94      0.95      0.94        98\n",
      "          40       0.95      0.88      0.91       106\n",
      "          41       0.87      0.99      0.93       107\n",
      "          42       0.90      0.98      0.94        97\n",
      "          43       0.92      0.88      0.90        93\n",
      "          44       0.92      0.91      0.91       109\n",
      "          45       1.00      1.00      1.00        88\n",
      "          46       0.97      0.99      0.98       107\n",
      "          47       0.91      0.88      0.90        98\n",
      "          48       0.97      0.85      0.90        98\n",
      "          49       0.84      0.91      0.88        94\n",
      "          50       0.59      0.67      0.62        96\n",
      "          51       1.00      1.00      1.00        52\n",
      "          52       0.99      1.00      0.99        85\n",
      "          53       0.91      0.71      0.80        97\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.91      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 80,  accuracy score is 1.0\n",
      "at random state 80, confusion matrix is [[102   0   0 ...   0   0   0]\n",
      " [  0  87   0 ...   0   0   1]\n",
      " [  0   0  89 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  53   0   0]\n",
      " [  0   0   0 ...   1  76   0]\n",
      " [  0   0   2 ...   0   0  78]]\n",
      "at random state 80, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.92      0.89       111\n",
      "           1       0.81      0.86      0.84       101\n",
      "           2       0.93      0.98      0.95        91\n",
      "           3       0.93      0.89      0.91        91\n",
      "           4       0.91      0.97      0.94        96\n",
      "           5       0.88      0.82      0.85       111\n",
      "           6       0.97      1.00      0.98        92\n",
      "           7       0.93      0.92      0.93       101\n",
      "           8       0.98      0.98      0.98        97\n",
      "           9       0.94      0.88      0.91       106\n",
      "          10       0.83      0.86      0.85       100\n",
      "          11       0.96      0.96      0.96       112\n",
      "          12       0.94      0.94      0.94       108\n",
      "          13       0.85      0.91      0.88        97\n",
      "          14       0.88      0.95      0.91        94\n",
      "          15       1.00      0.98      0.99        84\n",
      "          16       0.95      0.95      0.95       103\n",
      "          17       0.91      0.86      0.89        94\n",
      "          18       0.89      0.93      0.91        98\n",
      "          19       0.92      0.93      0.92        99\n",
      "          20       0.94      0.93      0.94       109\n",
      "          21       0.84      0.85      0.85        95\n",
      "          22       0.98      0.99      0.98        98\n",
      "          23       0.85      0.92      0.88        90\n",
      "          24       0.84      0.71      0.77       105\n",
      "          25       0.99      1.00      0.99        86\n",
      "          26       0.89      0.81      0.85       107\n",
      "          27       0.85      0.92      0.88       101\n",
      "          28       0.93      0.95      0.94        86\n",
      "          29       0.99      0.97      0.98       115\n",
      "          30       0.99      0.95      0.97        98\n",
      "          31       0.75      0.80      0.77       102\n",
      "          32       0.95      0.93      0.94       103\n",
      "          33       0.88      0.88      0.88        92\n",
      "          34       0.97      0.92      0.94       101\n",
      "          35       0.91      0.97      0.94        87\n",
      "          36       0.90      0.83      0.86       109\n",
      "          37       0.91      0.92      0.91       115\n",
      "          38       0.91      0.94      0.92       102\n",
      "          39       0.97      0.94      0.95       101\n",
      "          40       0.87      0.97      0.92        95\n",
      "          41       0.87      0.90      0.88       111\n",
      "          42       0.97      0.95      0.96        77\n",
      "          43       0.88      0.90      0.89       109\n",
      "          44       0.96      0.92      0.94       114\n",
      "          45       0.95      0.99      0.97        82\n",
      "          46       0.99      0.98      0.99       109\n",
      "          47       0.90      0.82      0.86        87\n",
      "          48       0.89      0.85      0.87       105\n",
      "          49       0.89      0.93      0.91       101\n",
      "          50       0.70      0.65      0.67        93\n",
      "          51       0.98      1.00      0.99        53\n",
      "          52       0.99      0.97      0.98        78\n",
      "          53       0.89      0.83      0.86        94\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.91      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 81,  accuracy score is 1.0\n",
      "at random state 81, confusion matrix is [[ 91   0   0 ...   0   0   0]\n",
      " [  0  90   0 ...   0   0   0]\n",
      " [  0   0 109 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  48   0   0]\n",
      " [  0   0   0 ...   1  69   0]\n",
      " [  0   1   1 ...   0   0  92]]\n",
      "at random state 81, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.91      0.87       100\n",
      "           1       0.84      0.83      0.84       108\n",
      "           2       0.96      0.98      0.97       111\n",
      "           3       0.90      0.91      0.90        96\n",
      "           4       0.91      0.96      0.93        97\n",
      "           5       0.91      0.82      0.86       104\n",
      "           6       1.00      0.99      0.99        82\n",
      "           7       0.90      0.94      0.92       100\n",
      "           8       0.97      0.98      0.97        99\n",
      "           9       0.91      0.89      0.90       104\n",
      "          10       0.90      0.85      0.87       104\n",
      "          11       0.93      0.95      0.94       109\n",
      "          12       0.96      0.99      0.97        95\n",
      "          13       0.86      0.90      0.88       112\n",
      "          14       0.92      0.93      0.93        90\n",
      "          15       0.99      0.98      0.98        82\n",
      "          16       0.93      0.95      0.94       105\n",
      "          17       0.95      0.95      0.95        93\n",
      "          18       0.94      0.93      0.93       115\n",
      "          19       0.89      0.92      0.90        95\n",
      "          20       0.98      0.92      0.95        88\n",
      "          21       0.77      0.86      0.81        92\n",
      "          22       0.96      0.97      0.96        97\n",
      "          23       0.84      0.86      0.85        91\n",
      "          24       0.71      0.74      0.73       101\n",
      "          25       0.98      0.98      0.98       100\n",
      "          26       0.85      0.86      0.85        99\n",
      "          27       0.89      0.91      0.90       110\n",
      "          28       0.95      0.94      0.94       100\n",
      "          29       0.99      0.98      0.98        93\n",
      "          30       1.00      0.91      0.95       104\n",
      "          31       0.77      0.78      0.77        96\n",
      "          32       0.93      0.90      0.92        94\n",
      "          33       0.90      0.93      0.92       116\n",
      "          34       0.94      0.92      0.93       101\n",
      "          35       0.91      0.98      0.94        85\n",
      "          36       0.86      0.84      0.85       110\n",
      "          37       0.91      0.91      0.91        96\n",
      "          38       0.92      0.94      0.93       103\n",
      "          39       0.95      0.92      0.93       106\n",
      "          40       0.88      0.91      0.90       113\n",
      "          41       0.89      0.87      0.88       120\n",
      "          42       0.95      0.96      0.95        93\n",
      "          43       0.86      0.92      0.89        97\n",
      "          44       0.96      0.92      0.94        95\n",
      "          45       0.96      0.99      0.97        94\n",
      "          46       0.97      0.96      0.97        72\n",
      "          47       0.92      0.84      0.88       101\n",
      "          48       0.92      0.88      0.90        94\n",
      "          49       0.90      0.91      0.90       108\n",
      "          50       0.67      0.61      0.63        99\n",
      "          51       0.96      1.00      0.98        48\n",
      "          52       0.97      0.97      0.97        71\n",
      "          53       0.94      0.85      0.89       108\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.91      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 82,  accuracy score is 1.0\n",
      "at random state 82, confusion matrix is [[89  0  0 ...  0  0  0]\n",
      " [ 0 91  0 ...  0  0  1]\n",
      " [ 0  0 90 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 45  0  0]\n",
      " [ 0  0  0 ...  0 79  0]\n",
      " [ 0  1  1 ...  0  0 77]]\n",
      "at random state 82, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88       101\n",
      "           1       0.83      0.83      0.83       109\n",
      "           2       0.94      0.97      0.95        93\n",
      "           3       0.91      0.82      0.86        88\n",
      "           4       0.98      0.96      0.97       100\n",
      "           5       0.87      0.86      0.87        94\n",
      "           6       0.98      1.00      0.99       100\n",
      "           7       0.90      0.84      0.87       109\n",
      "           8       0.99      0.97      0.98        98\n",
      "           9       0.92      0.89      0.91       104\n",
      "          10       0.92      0.85      0.88       105\n",
      "          11       0.96      0.92      0.94       117\n",
      "          12       0.90      0.95      0.92        97\n",
      "          13       0.90      0.88      0.89       109\n",
      "          14       0.88      0.92      0.90       108\n",
      "          15       0.98      0.99      0.98        84\n",
      "          16       0.99      0.92      0.95       101\n",
      "          17       0.92      0.98      0.95        91\n",
      "          18       0.86      0.95      0.90        94\n",
      "          19       0.87      0.91      0.89        98\n",
      "          20       0.97      0.95      0.96       102\n",
      "          21       0.78      0.83      0.80       109\n",
      "          22       0.96      0.99      0.97        87\n",
      "          23       0.83      0.89      0.86        91\n",
      "          24       0.80      0.78      0.79        85\n",
      "          25       0.98      0.98      0.98        96\n",
      "          26       0.84      0.84      0.84       107\n",
      "          27       0.90      0.94      0.92        96\n",
      "          28       0.98      0.91      0.94       116\n",
      "          29       0.94      1.00      0.97        94\n",
      "          30       0.97      0.95      0.96       109\n",
      "          31       0.76      0.83      0.79        98\n",
      "          32       0.94      0.91      0.93       112\n",
      "          33       0.88      0.85      0.87       106\n",
      "          34       0.98      0.95      0.96        91\n",
      "          35       0.94      0.94      0.94        98\n",
      "          36       0.90      0.83      0.86       106\n",
      "          37       0.81      0.89      0.85       101\n",
      "          38       0.88      0.92      0.90        99\n",
      "          39       0.93      0.92      0.93        91\n",
      "          40       0.92      0.91      0.91       100\n",
      "          41       0.89      0.93      0.91       103\n",
      "          42       0.95      0.97      0.96        95\n",
      "          43       0.88      0.91      0.89       108\n",
      "          44       0.92      0.91      0.92       101\n",
      "          45       0.94      0.99      0.96        90\n",
      "          46       0.97      0.96      0.96        95\n",
      "          47       0.92      0.86      0.89        99\n",
      "          48       0.88      0.88      0.88       105\n",
      "          49       0.89      0.90      0.89        99\n",
      "          50       0.69      0.72      0.71        85\n",
      "          51       1.00      1.00      1.00        45\n",
      "          52       1.00      0.99      0.99        80\n",
      "          53       0.88      0.79      0.83        97\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.91      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 83,  accuracy score is 1.0\n",
      "at random state 83, confusion matrix is [[ 91   0   0 ...   0   0   0]\n",
      " [  0  70   0 ...   0   0   0]\n",
      " [  0   0 106 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  56   0   0]\n",
      " [  0   0   0 ...   0  62   0]\n",
      " [  0   2   3 ...   0   0  81]]\n",
      "at random state 83, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.91      0.89       100\n",
      "           1       0.77      0.81      0.79        86\n",
      "           2       0.94      1.00      0.97       106\n",
      "           3       0.94      0.87      0.91        95\n",
      "           4       0.97      0.97      0.97       103\n",
      "           5       0.90      0.83      0.86       110\n",
      "           6       0.99      0.98      0.98        96\n",
      "           7       0.94      0.86      0.90        95\n",
      "           8       0.98      0.99      0.98        91\n",
      "           9       0.92      0.91      0.91        96\n",
      "          10       0.87      0.86      0.86       106\n",
      "          11       0.96      0.96      0.96       102\n",
      "          12       0.93      0.95      0.94        99\n",
      "          13       0.89      0.83      0.86        99\n",
      "          14       0.94      0.90      0.92        98\n",
      "          15       0.96      0.98      0.97        84\n",
      "          16       0.97      0.93      0.95        90\n",
      "          17       0.91      0.96      0.93       103\n",
      "          18       0.92      0.96      0.94        96\n",
      "          19       0.87      0.94      0.91       103\n",
      "          20       0.97      0.97      0.97        99\n",
      "          21       0.73      0.85      0.79        99\n",
      "          22       0.98      0.98      0.98       109\n",
      "          23       0.86      0.96      0.91       102\n",
      "          24       0.79      0.70      0.74       101\n",
      "          25       0.99      0.96      0.98       103\n",
      "          26       0.87      0.80      0.83       104\n",
      "          27       0.89      0.88      0.88        97\n",
      "          28       0.94      0.92      0.93       113\n",
      "          29       0.95      1.00      0.97        87\n",
      "          30       0.98      0.96      0.97        92\n",
      "          31       0.71      0.72      0.71        97\n",
      "          32       0.98      0.94      0.96       102\n",
      "          33       0.88      0.88      0.88        90\n",
      "          34       0.96      0.91      0.94       112\n",
      "          35       0.94      0.99      0.97       100\n",
      "          36       0.85      0.79      0.82       104\n",
      "          37       0.86      0.93      0.89        96\n",
      "          38       0.90      0.93      0.92       106\n",
      "          39       0.92      0.95      0.93       102\n",
      "          40       0.85      0.88      0.87       100\n",
      "          41       0.90      0.90      0.90       102\n",
      "          42       0.93      0.97      0.95       109\n",
      "          43       0.86      0.83      0.84        94\n",
      "          44       0.90      0.84      0.87       101\n",
      "          45       0.95      1.00      0.97        95\n",
      "          46       1.00      0.98      0.99       103\n",
      "          47       0.91      0.85      0.88        98\n",
      "          48       0.95      0.85      0.90       111\n",
      "          49       0.85      0.95      0.90        93\n",
      "          50       0.59      0.63      0.61        99\n",
      "          51       1.00      1.00      1.00        56\n",
      "          52       0.97      1.00      0.98        62\n",
      "          53       0.88      0.81      0.84       100\n",
      "\n",
      "    accuracy                           0.90      5296\n",
      "   macro avg       0.91      0.91      0.90      5296\n",
      "weighted avg       0.91      0.90      0.90      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 84,  accuracy score is 1.0\n",
      "at random state 84, confusion matrix is [[73  0  0 ...  0  0  0]\n",
      " [ 0 81  0 ...  0  0  1]\n",
      " [ 0  0 91 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 52  0  0]\n",
      " [ 0  0  0 ...  0 88  0]\n",
      " [ 0  2  1 ...  0  0 83]]\n",
      "at random state 84, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.91      0.86        80\n",
      "           1       0.80      0.90      0.85        90\n",
      "           2       0.96      0.97      0.96        94\n",
      "           3       0.93      0.82      0.87       108\n",
      "           4       0.95      0.97      0.96       101\n",
      "           5       0.84      0.81      0.83        91\n",
      "           6       0.99      0.99      0.99        96\n",
      "           7       0.90      0.91      0.91       101\n",
      "           8       0.97      0.99      0.98       101\n",
      "           9       0.90      0.91      0.90       109\n",
      "          10       0.86      0.89      0.88        93\n",
      "          11       0.95      0.94      0.94        93\n",
      "          12       0.95      0.97      0.96       100\n",
      "          13       0.80      0.79      0.79        95\n",
      "          14       0.92      0.90      0.91       101\n",
      "          15       0.99      0.99      0.99        85\n",
      "          16       0.96      0.96      0.96       112\n",
      "          17       0.91      0.93      0.92       103\n",
      "          18       0.93      0.91      0.92        96\n",
      "          19       0.91      0.95      0.93       111\n",
      "          20       0.94      0.92      0.93       102\n",
      "          21       0.82      0.84      0.83        89\n",
      "          22       0.98      1.00      0.99       111\n",
      "          23       0.86      0.92      0.89        96\n",
      "          24       0.82      0.77      0.79        98\n",
      "          25       0.96      0.99      0.98       102\n",
      "          26       0.84      0.78      0.81       100\n",
      "          27       0.90      0.91      0.90       103\n",
      "          28       0.99      0.95      0.97       103\n",
      "          29       0.98      0.99      0.99       103\n",
      "          30       0.98      0.95      0.96       112\n",
      "          31       0.76      0.74      0.75        81\n",
      "          32       0.94      0.92      0.93       100\n",
      "          33       0.87      0.89      0.88       108\n",
      "          34       0.96      0.94      0.95        98\n",
      "          35       0.95      0.97      0.96       101\n",
      "          36       0.83      0.78      0.80        89\n",
      "          37       0.85      0.88      0.87       100\n",
      "          38       0.90      0.94      0.92       100\n",
      "          39       0.95      0.95      0.95       114\n",
      "          40       0.92      0.92      0.92       102\n",
      "          41       0.84      0.89      0.86       100\n",
      "          42       0.95      0.99      0.97       106\n",
      "          43       0.88      0.90      0.89       101\n",
      "          44       0.94      0.90      0.92       115\n",
      "          45       0.98      0.97      0.97        88\n",
      "          46       0.95      0.96      0.96        79\n",
      "          47       0.90      0.87      0.89        94\n",
      "          48       0.95      0.82      0.88       101\n",
      "          49       0.90      0.90      0.90       106\n",
      "          50       0.74      0.78      0.76        96\n",
      "          51       1.00      1.00      1.00        52\n",
      "          52       1.00      0.99      0.99        89\n",
      "          53       0.89      0.86      0.87        97\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.91      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 85,  accuracy score is 1.0\n",
      "at random state 85, confusion matrix is [[100   0   0 ...   0   0   0]\n",
      " [  0  87   0 ...   0   0   0]\n",
      " [  0   0 103 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  52   0   0]\n",
      " [  0   0   0 ...   1  70   0]\n",
      " [  0   1   1 ...   0   0  88]]\n",
      "at random state 85, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89       114\n",
      "           1       0.84      0.92      0.88        95\n",
      "           2       0.96      0.99      0.98       104\n",
      "           3       0.89      0.88      0.88        96\n",
      "           4       0.94      0.96      0.95        99\n",
      "           5       0.92      0.81      0.86       104\n",
      "           6       1.00      0.99      0.99        90\n",
      "           7       0.90      0.90      0.90        97\n",
      "           8       1.00      0.95      0.98       108\n",
      "           9       0.94      0.87      0.90       104\n",
      "          10       0.77      0.87      0.82        95\n",
      "          11       0.91      0.94      0.93        99\n",
      "          12       0.97      0.95      0.96       109\n",
      "          13       0.88      0.87      0.88       102\n",
      "          14       0.93      0.93      0.93       107\n",
      "          15       0.97      1.00      0.99        76\n",
      "          16       0.95      0.98      0.96        93\n",
      "          17       0.89      0.90      0.89       103\n",
      "          18       0.88      0.89      0.89       110\n",
      "          19       0.86      0.85      0.86        88\n",
      "          20       0.96      0.92      0.94       108\n",
      "          21       0.79      0.88      0.83        91\n",
      "          22       0.98      1.00      0.99        88\n",
      "          23       0.89      0.88      0.88        98\n",
      "          24       0.76      0.78      0.77        91\n",
      "          25       1.00      0.96      0.98       106\n",
      "          26       0.86      0.78      0.82       103\n",
      "          27       0.87      0.95      0.91       102\n",
      "          28       0.95      0.96      0.96       103\n",
      "          29       0.99      0.97      0.98        94\n",
      "          30       0.97      0.96      0.97       104\n",
      "          31       0.73      0.77      0.75       101\n",
      "          32       0.93      0.95      0.94        95\n",
      "          33       0.91      0.89      0.90        87\n",
      "          34       0.97      0.94      0.96       102\n",
      "          35       0.96      0.95      0.95       118\n",
      "          36       0.91      0.83      0.87       107\n",
      "          37       0.88      0.86      0.87        98\n",
      "          38       0.89      0.94      0.91       109\n",
      "          39       0.93      0.96      0.94        91\n",
      "          40       0.94      0.97      0.96       100\n",
      "          41       0.86      0.93      0.89        99\n",
      "          42       0.97      0.97      0.97        95\n",
      "          43       0.88      0.95      0.91        94\n",
      "          44       0.90      0.89      0.90        93\n",
      "          45       0.96      0.99      0.97        86\n",
      "          46       0.96      0.97      0.96        90\n",
      "          47       0.91      0.88      0.90       113\n",
      "          48       0.92      0.83      0.87       101\n",
      "          49       0.84      0.92      0.88        86\n",
      "          50       0.70      0.65      0.67       113\n",
      "          51       0.98      1.00      0.99        52\n",
      "          52       0.96      0.97      0.97        72\n",
      "          53       0.88      0.78      0.83       113\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.91      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 86,  accuracy score is 1.0\n",
      "at random state 86, confusion matrix is [[99  0  0 ...  0  0  0]\n",
      " [ 0 86  0 ...  0  0  0]\n",
      " [ 0  0 89 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 54  0  0]\n",
      " [ 0  0  0 ...  0 66  0]\n",
      " [ 0  0  2 ...  0  0 70]]\n",
      "at random state 86, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.92       107\n",
      "           1       0.79      0.87      0.83        99\n",
      "           2       0.96      0.97      0.96        92\n",
      "           3       0.95      0.88      0.92       104\n",
      "           4       0.95      0.91      0.93       105\n",
      "           5       0.88      0.87      0.87        99\n",
      "           6       0.99      0.99      0.99        77\n",
      "           7       0.93      0.92      0.93       109\n",
      "           8       0.96      0.98      0.97       101\n",
      "           9       0.88      0.92      0.90        90\n",
      "          10       0.84      0.92      0.88       111\n",
      "          11       0.89      0.93      0.91       100\n",
      "          12       0.96      0.98      0.97        93\n",
      "          13       0.92      0.87      0.89       101\n",
      "          14       0.96      0.96      0.96        91\n",
      "          15       0.96      0.97      0.97        79\n",
      "          16       0.98      0.91      0.94       103\n",
      "          17       0.90      0.91      0.91        93\n",
      "          18       0.90      0.92      0.91        99\n",
      "          19       0.90      0.90      0.90       103\n",
      "          20       0.96      0.92      0.94        97\n",
      "          21       0.80      0.83      0.81       103\n",
      "          22       1.00      0.99      1.00       111\n",
      "          23       0.84      0.89      0.87        98\n",
      "          24       0.69      0.75      0.72        96\n",
      "          25       0.95      0.98      0.96        91\n",
      "          26       0.90      0.76      0.82       123\n",
      "          27       0.90      0.88      0.89       107\n",
      "          28       0.96      0.95      0.96       116\n",
      "          29       0.97      0.98      0.97        87\n",
      "          30       0.94      0.95      0.94        98\n",
      "          31       0.77      0.73      0.75        95\n",
      "          32       0.91      0.92      0.91        97\n",
      "          33       0.86      0.88      0.87       104\n",
      "          34       0.96      0.94      0.95       108\n",
      "          35       0.89      0.95      0.92        96\n",
      "          36       0.89      0.81      0.85        91\n",
      "          37       0.89      0.81      0.85       106\n",
      "          38       0.79      0.96      0.87        97\n",
      "          39       0.85      0.96      0.90        95\n",
      "          40       0.88      0.89      0.88        89\n",
      "          41       0.90      0.93      0.91       120\n",
      "          42       0.92      0.95      0.93        93\n",
      "          43       0.91      0.87      0.89        92\n",
      "          44       0.96      0.90      0.93       109\n",
      "          45       0.97      0.99      0.98       113\n",
      "          46       0.96      0.94      0.95        99\n",
      "          47       0.95      0.87      0.90       104\n",
      "          48       0.86      0.82      0.84        99\n",
      "          49       0.92      0.94      0.93        94\n",
      "          50       0.65      0.63      0.64       103\n",
      "          51       1.00      1.00      1.00        54\n",
      "          52       0.99      1.00      0.99        66\n",
      "          53       0.90      0.79      0.84        89\n",
      "\n",
      "    accuracy                           0.90      5296\n",
      "   macro avg       0.90      0.90      0.90      5296\n",
      "weighted avg       0.90      0.90      0.90      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 87,  accuracy score is 1.0\n",
      "at random state 87, confusion matrix is [[ 87   0   0 ...   0   0   0]\n",
      " [  0  94   0 ...   0   0   1]\n",
      " [  0   0 110 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  48   0   0]\n",
      " [  0   0   0 ...   1  67   0]\n",
      " [  0   1   1 ...   0   0  62]]\n",
      "at random state 87, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89        99\n",
      "           1       0.80      0.90      0.84       105\n",
      "           2       0.94      0.99      0.96       111\n",
      "           3       0.88      0.87      0.88       102\n",
      "           4       0.97      0.90      0.93       111\n",
      "           5       0.88      0.78      0.82       117\n",
      "           6       0.99      0.99      0.99        91\n",
      "           7       0.93      0.90      0.92       105\n",
      "           8       0.97      0.97      0.97        88\n",
      "           9       0.93      0.89      0.91       113\n",
      "          10       0.83      0.86      0.84       107\n",
      "          11       0.89      0.95      0.92       107\n",
      "          12       0.95      0.94      0.94        93\n",
      "          13       0.87      0.90      0.88        98\n",
      "          14       0.91      0.88      0.89        99\n",
      "          15       0.96      0.98      0.97        83\n",
      "          16       0.92      0.98      0.95        91\n",
      "          17       0.92      0.92      0.92       105\n",
      "          18       0.92      0.87      0.89       101\n",
      "          19       0.89      0.87      0.88       102\n",
      "          20       0.97      0.96      0.97       102\n",
      "          21       0.83      0.86      0.85       103\n",
      "          22       0.98      0.99      0.98        83\n",
      "          23       0.86      0.89      0.88        92\n",
      "          24       0.74      0.71      0.72        92\n",
      "          25       0.97      0.98      0.98       101\n",
      "          26       0.87      0.88      0.87       102\n",
      "          27       0.88      0.91      0.89       101\n",
      "          28       0.99      0.93      0.96        97\n",
      "          29       0.97      0.98      0.97        92\n",
      "          30       0.95      0.95      0.95       101\n",
      "          31       0.75      0.87      0.81        95\n",
      "          32       0.91      0.91      0.91       112\n",
      "          33       0.92      0.89      0.90        99\n",
      "          34       0.96      0.95      0.95        99\n",
      "          35       0.97      0.96      0.96        99\n",
      "          36       0.89      0.82      0.86       102\n",
      "          37       0.86      0.90      0.88        83\n",
      "          38       0.89      0.95      0.92        92\n",
      "          39       0.92      0.95      0.94       106\n",
      "          40       0.89      0.92      0.90       110\n",
      "          41       0.91      0.89      0.90       108\n",
      "          42       0.94      0.95      0.95        88\n",
      "          43       0.85      0.89      0.87        92\n",
      "          44       0.89      0.91      0.90       102\n",
      "          45       0.96      0.98      0.97        95\n",
      "          46       0.96      1.00      0.98       101\n",
      "          47       0.94      0.82      0.87        98\n",
      "          48       0.95      0.83      0.89       101\n",
      "          49       0.85      0.93      0.89       116\n",
      "          50       0.77      0.64      0.70       111\n",
      "          51       0.96      1.00      0.98        48\n",
      "          52       0.97      0.99      0.98        68\n",
      "          53       0.89      0.81      0.84        77\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.90      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 88,  accuracy score is 1.0\n",
      "at random state 88, confusion matrix is [[ 86   0   0 ...   0   0   0]\n",
      " [  0 104   0 ...   0   0   0]\n",
      " [  0   0  84 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  64   0   0]\n",
      " [  0   0   0 ...   2  74   0]\n",
      " [  0   0   2 ...   0   0  84]]\n",
      "at random state 88, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.88      0.86        98\n",
      "           1       0.88      0.88      0.88       118\n",
      "           2       0.98      1.00      0.99        84\n",
      "           3       0.84      0.92      0.88        91\n",
      "           4       0.99      0.95      0.97       110\n",
      "           5       0.80      0.82      0.81        91\n",
      "           6       0.97      1.00      0.99        73\n",
      "           7       0.82      0.88      0.85        85\n",
      "           8       0.93      0.95      0.94        99\n",
      "           9       0.88      0.96      0.92       113\n",
      "          10       0.89      0.86      0.87       105\n",
      "          11       0.94      0.93      0.94       103\n",
      "          12       0.93      0.97      0.95        98\n",
      "          13       0.87      0.87      0.87        90\n",
      "          14       0.91      0.91      0.91       103\n",
      "          15       0.98      0.98      0.98        87\n",
      "          16       0.93      0.93      0.93        85\n",
      "          17       0.92      0.93      0.92       107\n",
      "          18       0.88      0.92      0.90        91\n",
      "          19       0.96      0.85      0.90       106\n",
      "          20       0.98      0.96      0.97        89\n",
      "          21       0.84      0.85      0.85       124\n",
      "          22       0.99      0.98      0.99       116\n",
      "          23       0.88      0.89      0.88       110\n",
      "          24       0.76      0.73      0.74        96\n",
      "          25       0.95      0.99      0.97        80\n",
      "          26       0.87      0.89      0.88       101\n",
      "          27       0.84      0.93      0.88        99\n",
      "          28       0.95      0.95      0.95        99\n",
      "          29       0.99      0.99      0.99        97\n",
      "          30       0.98      0.94      0.96       106\n",
      "          31       0.83      0.75      0.79       102\n",
      "          32       0.90      0.95      0.93       108\n",
      "          33       0.88      0.92      0.90        87\n",
      "          34       1.00      0.87      0.93       103\n",
      "          35       0.95      0.96      0.95        97\n",
      "          36       0.72      0.88      0.80        86\n",
      "          37       0.79      0.89      0.84        87\n",
      "          38       0.89      0.89      0.89       106\n",
      "          39       0.91      0.96      0.93        97\n",
      "          40       0.86      0.95      0.90        91\n",
      "          41       0.89      0.86      0.88       108\n",
      "          42       0.96      0.99      0.97        91\n",
      "          43       0.96      0.80      0.87       117\n",
      "          44       0.96      0.86      0.91       124\n",
      "          45       0.99      0.98      0.98        82\n",
      "          46       0.97      0.99      0.98        93\n",
      "          47       0.94      0.92      0.93        99\n",
      "          48       0.97      0.81      0.88       105\n",
      "          49       0.88      0.89      0.88       116\n",
      "          50       0.68      0.68      0.68        97\n",
      "          51       0.97      1.00      0.98        64\n",
      "          52       1.00      0.97      0.99        76\n",
      "          53       0.93      0.79      0.86       106\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.91      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 89,  accuracy score is 1.0\n",
      "at random state 89, confusion matrix is [[ 98   0   0 ...   0   0   0]\n",
      " [  0  86   0 ...   0   0   0]\n",
      " [  0   0 112 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  55   0   0]\n",
      " [  0   0   0 ...   1  82   0]\n",
      " [  0   1   1 ...   0   0  90]]\n",
      "at random state 89, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88       112\n",
      "           1       0.80      0.82      0.81       105\n",
      "           2       0.95      0.99      0.97       113\n",
      "           3       0.89      0.86      0.88        99\n",
      "           4       0.96      0.93      0.94        95\n",
      "           5       0.89      0.86      0.87       105\n",
      "           6       0.97      0.99      0.98        93\n",
      "           7       0.88      0.90      0.89        94\n",
      "           8       1.00      0.98      0.99       108\n",
      "           9       0.87      0.90      0.89        83\n",
      "          10       0.80      0.86      0.83        94\n",
      "          11       0.91      0.92      0.91       104\n",
      "          12       0.96      0.96      0.96        95\n",
      "          13       0.84      0.86      0.85        99\n",
      "          14       0.90      0.93      0.91        94\n",
      "          15       0.99      0.97      0.98        75\n",
      "          16       0.94      0.95      0.94       108\n",
      "          17       0.92      0.87      0.89        97\n",
      "          18       0.89      0.94      0.92       106\n",
      "          19       0.92      0.84      0.88        92\n",
      "          20       0.96      0.93      0.95       102\n",
      "          21       0.71      0.79      0.75        85\n",
      "          22       0.96      1.00      0.98       105\n",
      "          23       0.88      0.92      0.90       111\n",
      "          24       0.79      0.72      0.75        97\n",
      "          25       0.98      0.97      0.97       100\n",
      "          26       0.85      0.80      0.82        93\n",
      "          27       0.87      0.85      0.86       100\n",
      "          28       0.96      0.95      0.96       111\n",
      "          29       0.98      0.98      0.98        91\n",
      "          30       0.98      0.97      0.98       108\n",
      "          31       0.72      0.78      0.75       102\n",
      "          32       0.91      0.88      0.89        98\n",
      "          33       0.88      0.89      0.88       102\n",
      "          34       0.97      0.91      0.94       103\n",
      "          35       1.00      0.98      0.99       111\n",
      "          36       0.92      0.82      0.87       111\n",
      "          37       0.88      0.91      0.90        91\n",
      "          38       0.94      0.91      0.92        99\n",
      "          39       0.89      0.91      0.90        94\n",
      "          40       0.93      0.88      0.90       121\n",
      "          41       0.81      0.88      0.84        92\n",
      "          42       0.91      0.99      0.95        82\n",
      "          43       0.88      0.89      0.88       107\n",
      "          44       0.91      0.93      0.92        95\n",
      "          45       0.96      0.99      0.97        86\n",
      "          46       0.99      0.99      0.99        88\n",
      "          47       0.88      0.92      0.90        84\n",
      "          48       0.96      0.85      0.90       115\n",
      "          49       0.88      0.89      0.89       104\n",
      "          50       0.57      0.59      0.58        92\n",
      "          51       0.98      1.00      0.99        55\n",
      "          52       1.00      0.98      0.99        84\n",
      "          53       0.88      0.85      0.87       106\n",
      "\n",
      "    accuracy                           0.90      5296\n",
      "   macro avg       0.90      0.90      0.90      5296\n",
      "weighted avg       0.90      0.90      0.90      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 90,  accuracy score is 1.0\n",
      "at random state 90, confusion matrix is [[95  0  0 ...  0  0  0]\n",
      " [ 0 90  0 ...  0  0  1]\n",
      " [ 0  0 88 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 51  0  0]\n",
      " [ 0  0  0 ...  1 72  0]\n",
      " [ 0  0  0 ...  0  0 66]]\n",
      "at random state 90, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.91      0.87       104\n",
      "           1       0.80      0.83      0.81       108\n",
      "           2       0.97      0.99      0.98        89\n",
      "           3       0.92      0.87      0.89       115\n",
      "           4       0.96      0.96      0.96       102\n",
      "           5       0.82      0.82      0.82       109\n",
      "           6       0.99      0.98      0.98        89\n",
      "           7       0.91      0.90      0.91        93\n",
      "           8       0.97      0.98      0.98       109\n",
      "           9       0.91      0.88      0.90        92\n",
      "          10       0.77      0.87      0.82        95\n",
      "          11       0.95      0.96      0.96       101\n",
      "          12       0.95      0.96      0.96       110\n",
      "          13       0.87      0.86      0.86       106\n",
      "          14       0.93      0.95      0.94        93\n",
      "          15       1.00      1.00      1.00        90\n",
      "          16       0.92      0.99      0.96        87\n",
      "          17       0.89      0.84      0.87        81\n",
      "          18       0.91      0.93      0.92       105\n",
      "          19       0.89      0.94      0.91       108\n",
      "          20       0.97      0.94      0.95       101\n",
      "          21       0.86      0.84      0.85       108\n",
      "          22       0.99      0.99      0.99       110\n",
      "          23       0.82      0.87      0.84       111\n",
      "          24       0.71      0.75      0.73        96\n",
      "          25       0.97      0.99      0.98       104\n",
      "          26       0.86      0.78      0.82        88\n",
      "          27       0.89      0.91      0.90        86\n",
      "          28       0.96      0.95      0.95       100\n",
      "          29       0.96      0.94      0.95       108\n",
      "          30       0.99      0.96      0.97        98\n",
      "          31       0.72      0.80      0.76       108\n",
      "          32       0.92      0.93      0.93       100\n",
      "          33       0.86      0.91      0.89        93\n",
      "          34       0.96      0.91      0.94       103\n",
      "          35       0.97      0.96      0.97       119\n",
      "          36       0.90      0.83      0.86        95\n",
      "          37       0.85      0.92      0.88        91\n",
      "          38       0.93      0.90      0.92       102\n",
      "          39       0.96      0.92      0.94        96\n",
      "          40       0.91      0.91      0.91       117\n",
      "          41       0.94      0.95      0.95       104\n",
      "          42       0.95      0.99      0.97        87\n",
      "          43       0.93      0.91      0.92        96\n",
      "          44       0.88      0.88      0.88       102\n",
      "          45       0.94      0.96      0.95        75\n",
      "          46       0.96      0.99      0.97        86\n",
      "          47       0.94      0.88      0.91       104\n",
      "          48       0.95      0.83      0.89        98\n",
      "          49       0.87      0.79      0.83       106\n",
      "          50       0.67      0.58      0.62       116\n",
      "          51       0.96      1.00      0.98        51\n",
      "          52       0.99      0.99      0.99        73\n",
      "          53       0.84      0.85      0.84        78\n",
      "\n",
      "    accuracy                           0.90      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.90      0.90      0.90      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 91,  accuracy score is 1.0\n",
      "at random state 91, confusion matrix is [[95  0  0 ...  0  0  0]\n",
      " [ 0 91  0 ...  0  0  1]\n",
      " [ 0  0 96 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 55  0  0]\n",
      " [ 0  0  0 ...  1 73  0]\n",
      " [ 0  0  1 ...  0  0 82]]\n",
      "at random state 91, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.90      0.88       105\n",
      "           1       0.83      0.84      0.84       108\n",
      "           2       0.94      0.99      0.96        97\n",
      "           3       0.94      0.88      0.91       115\n",
      "           4       0.94      0.96      0.95       104\n",
      "           5       0.88      0.79      0.84       102\n",
      "           6       0.98      0.98      0.98        90\n",
      "           7       0.91      0.92      0.92        93\n",
      "           8       0.97      0.98      0.97        89\n",
      "           9       0.92      0.94      0.93        98\n",
      "          10       0.82      0.86      0.84        98\n",
      "          11       0.94      0.97      0.96       105\n",
      "          12       0.96      0.97      0.97       109\n",
      "          13       0.88      0.86      0.87       105\n",
      "          14       0.92      0.89      0.91        94\n",
      "          15       0.97      0.99      0.98        90\n",
      "          16       0.95      0.96      0.95        96\n",
      "          17       0.95      0.93      0.94       114\n",
      "          18       0.91      0.92      0.91        95\n",
      "          19       0.87      0.88      0.87       105\n",
      "          20       0.95      0.92      0.93        97\n",
      "          21       0.78      0.83      0.81        95\n",
      "          22       1.00      1.00      1.00        94\n",
      "          23       0.83      0.89      0.86        95\n",
      "          24       0.78      0.74      0.76       102\n",
      "          25       0.98      0.98      0.98       105\n",
      "          26       0.83      0.76      0.79        94\n",
      "          27       0.87      0.90      0.89        83\n",
      "          28       0.97      0.97      0.97       110\n",
      "          29       0.99      0.99      0.99       106\n",
      "          30       0.99      0.92      0.95       101\n",
      "          31       0.74      0.78      0.76        96\n",
      "          32       0.94      0.92      0.93        98\n",
      "          33       0.88      0.89      0.88       103\n",
      "          34       0.97      0.95      0.96       100\n",
      "          35       0.91      0.97      0.94        94\n",
      "          36       0.85      0.91      0.88       102\n",
      "          37       0.89      0.92      0.91        91\n",
      "          38       0.85      0.93      0.89        95\n",
      "          39       0.95      0.95      0.95       100\n",
      "          40       0.98      0.93      0.95       102\n",
      "          41       0.88      0.90      0.89        84\n",
      "          42       0.96      0.97      0.97       104\n",
      "          43       0.93      0.86      0.89       103\n",
      "          44       0.90      0.91      0.90       108\n",
      "          45       0.98      1.00      0.99        88\n",
      "          46       1.00      0.96      0.98        98\n",
      "          47       0.87      0.95      0.91        98\n",
      "          48       0.89      0.83      0.86        96\n",
      "          49       0.88      0.89      0.88       110\n",
      "          50       0.73      0.66      0.69       105\n",
      "          51       0.96      1.00      0.98        55\n",
      "          52       0.99      0.96      0.97        76\n",
      "          53       0.95      0.85      0.90        96\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.91      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 92,  accuracy score is 1.0\n",
      "at random state 92, confusion matrix is [[88  0  0 ...  0  0  0]\n",
      " [ 0 84  0 ...  0  0  0]\n",
      " [ 0  0 99 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 50  0  0]\n",
      " [ 0  0  0 ...  0 82  0]\n",
      " [ 0  0  1 ...  0  0 93]]\n",
      "at random state 92, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.90      0.85        98\n",
      "           1       0.88      0.90      0.89        93\n",
      "           2       0.96      1.00      0.98        99\n",
      "           3       0.98      0.85      0.91       107\n",
      "           4       0.98      0.95      0.97       103\n",
      "           5       0.88      0.78      0.83        98\n",
      "           6       0.97      0.97      0.97        79\n",
      "           7       0.92      0.95      0.94        88\n",
      "           8       0.97      0.99      0.98       109\n",
      "           9       0.94      0.92      0.93       104\n",
      "          10       0.84      0.89      0.86        97\n",
      "          11       0.93      0.96      0.94       107\n",
      "          12       1.00      0.96      0.98       108\n",
      "          13       0.88      0.89      0.89       100\n",
      "          14       0.94      0.93      0.94       103\n",
      "          15       0.98      0.97      0.97        86\n",
      "          16       0.94      0.95      0.95       101\n",
      "          17       1.00      0.91      0.95        99\n",
      "          18       0.92      0.94      0.93       101\n",
      "          19       0.91      0.91      0.91       105\n",
      "          20       0.97      0.94      0.95        95\n",
      "          21       0.82      0.86      0.84       103\n",
      "          22       0.99      0.97      0.98       124\n",
      "          23       0.82      0.92      0.87       108\n",
      "          24       0.78      0.71      0.74       103\n",
      "          25       0.97      0.99      0.98        93\n",
      "          26       0.84      0.80      0.82       100\n",
      "          27       0.91      0.93      0.92        99\n",
      "          28       0.94      0.96      0.95        85\n",
      "          29       0.97      0.98      0.97       114\n",
      "          30       0.98      0.99      0.98        83\n",
      "          31       0.80      0.77      0.78       104\n",
      "          32       0.87      0.92      0.89        91\n",
      "          33       0.91      0.89      0.90        98\n",
      "          34       0.99      0.95      0.97       107\n",
      "          35       0.93      0.95      0.94       102\n",
      "          36       0.90      0.82      0.86        95\n",
      "          37       0.95      0.92      0.93        98\n",
      "          38       0.90      0.94      0.92        96\n",
      "          39       0.88      0.96      0.92       102\n",
      "          40       0.92      0.95      0.93       104\n",
      "          41       0.89      0.92      0.91        89\n",
      "          42       0.98      0.95      0.96        93\n",
      "          43       0.87      0.87      0.87        95\n",
      "          44       0.94      0.92      0.93       101\n",
      "          45       0.96      0.99      0.97        97\n",
      "          46       0.98      0.99      0.98        95\n",
      "          47       0.89      0.86      0.88        95\n",
      "          48       0.92      0.86      0.89       100\n",
      "          49       0.86      0.88      0.87        98\n",
      "          50       0.63      0.75      0.69        99\n",
      "          51       0.98      1.00      0.99        50\n",
      "          52       0.99      1.00      0.99        82\n",
      "          53       0.93      0.82      0.87       113\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.92      0.92      0.92      5296\n",
      "weighted avg       0.92      0.91      0.91      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 93,  accuracy score is 1.0\n",
      "at random state 93, confusion matrix is [[80  0  0 ...  0  0  0]\n",
      " [ 0 93  0 ...  0  0  0]\n",
      " [ 0  0 96 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 44  0  0]\n",
      " [ 0  0  0 ...  1 73  0]\n",
      " [ 0  0  1 ...  0  0 83]]\n",
      "at random state 93, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.87      0.87        92\n",
      "           1       0.83      0.89      0.86       105\n",
      "           2       0.96      1.00      0.98        96\n",
      "           3       0.89      0.83      0.86        99\n",
      "           4       0.97      0.96      0.96        99\n",
      "           5       0.83      0.87      0.85        97\n",
      "           6       0.98      0.98      0.98        89\n",
      "           7       0.94      0.94      0.94       109\n",
      "           8       0.98      0.96      0.97       101\n",
      "           9       0.89      0.92      0.90        87\n",
      "          10       0.82      0.88      0.85        93\n",
      "          11       0.90      0.93      0.91       114\n",
      "          12       0.94      0.97      0.95       106\n",
      "          13       0.92      0.83      0.87        96\n",
      "          14       0.94      0.92      0.93       106\n",
      "          15       1.00      1.00      1.00        88\n",
      "          16       1.00      0.96      0.98       105\n",
      "          17       0.96      0.97      0.97       104\n",
      "          18       0.88      0.92      0.90       102\n",
      "          19       0.88      0.88      0.88       104\n",
      "          20       0.97      0.95      0.96        95\n",
      "          21       0.88      0.85      0.87        96\n",
      "          22       0.99      0.98      0.99       103\n",
      "          23       0.80      0.86      0.83       101\n",
      "          24       0.72      0.71      0.71        93\n",
      "          25       1.00      0.98      0.99       117\n",
      "          26       0.89      0.77      0.82       112\n",
      "          27       0.92      0.87      0.90       118\n",
      "          28       0.97      0.98      0.97        88\n",
      "          29       0.98      0.99      0.99       104\n",
      "          30       0.92      0.97      0.95        89\n",
      "          31       0.74      0.81      0.77        99\n",
      "          32       0.96      0.94      0.95        86\n",
      "          33       0.89      0.92      0.91        99\n",
      "          34       0.98      0.97      0.98       104\n",
      "          35       0.98      0.97      0.97        89\n",
      "          36       0.90      0.90      0.90       105\n",
      "          37       0.92      0.95      0.93        96\n",
      "          38       0.93      0.93      0.93        95\n",
      "          39       0.85      0.96      0.90        90\n",
      "          40       0.90      0.94      0.92       108\n",
      "          41       0.90      0.95      0.93        98\n",
      "          42       0.93      0.98      0.95        93\n",
      "          43       0.92      0.87      0.89       101\n",
      "          44       0.96      0.94      0.95       117\n",
      "          45       0.94      0.98      0.96        86\n",
      "          46       0.99      0.95      0.97        88\n",
      "          47       0.95      0.89      0.92        91\n",
      "          48       0.93      0.86      0.89       108\n",
      "          49       0.94      0.90      0.92       114\n",
      "          50       0.66      0.63      0.64       107\n",
      "          51       0.98      1.00      0.99        44\n",
      "          52       0.96      0.99      0.97        74\n",
      "          53       0.92      0.86      0.89        96\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.92      0.92      0.92      5296\n",
      "weighted avg       0.91      0.91      0.91      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 94,  accuracy score is 1.0\n",
      "at random state 94, confusion matrix is [[101   0   0 ...   0   0   0]\n",
      " [  0  81   0 ...   0   0   1]\n",
      " [  0   0 104 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  46   0   0]\n",
      " [  0   0   0 ...   1  64   0]\n",
      " [  0   0   2 ...   0   0  84]]\n",
      "at random state 94, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.96      0.89       105\n",
      "           1       0.78      0.90      0.84        90\n",
      "           2       0.97      0.97      0.97       107\n",
      "           3       0.90      0.90      0.90       100\n",
      "           4       0.93      0.99      0.96        99\n",
      "           5       0.90      0.76      0.82       114\n",
      "           6       0.97      0.98      0.97        98\n",
      "           7       0.92      0.93      0.92        95\n",
      "           8       0.97      0.98      0.98       117\n",
      "           9       0.92      0.92      0.92        86\n",
      "          10       0.83      0.87      0.85       103\n",
      "          11       0.96      0.96      0.96        92\n",
      "          12       0.96      0.92      0.94        98\n",
      "          13       0.91      0.87      0.89       109\n",
      "          14       0.94      0.89      0.91       114\n",
      "          15       1.00      0.99      0.99        81\n",
      "          16       0.94      0.91      0.93       103\n",
      "          17       0.98      0.92      0.95       103\n",
      "          18       0.97      0.96      0.96        89\n",
      "          19       0.92      0.88      0.90       113\n",
      "          20       0.95      0.93      0.94        96\n",
      "          21       0.83      0.77      0.80       101\n",
      "          22       0.97      0.97      0.97       113\n",
      "          23       0.80      0.89      0.84        89\n",
      "          24       0.79      0.71      0.75       103\n",
      "          25       0.99      0.97      0.98        86\n",
      "          26       0.81      0.75      0.78        99\n",
      "          27       0.88      0.89      0.89       110\n",
      "          28       0.95      0.99      0.97        99\n",
      "          29       0.98      0.98      0.98        89\n",
      "          30       0.94      0.97      0.95        94\n",
      "          31       0.66      0.82      0.73        79\n",
      "          32       0.97      0.94      0.95        95\n",
      "          33       0.87      0.96      0.91       101\n",
      "          34       0.94      0.92      0.93       111\n",
      "          35       0.91      0.96      0.94       111\n",
      "          36       0.76      0.84      0.80        88\n",
      "          37       0.91      0.93      0.92       100\n",
      "          38       0.91      0.91      0.91        98\n",
      "          39       0.95      0.96      0.95        97\n",
      "          40       0.88      0.94      0.91        96\n",
      "          41       0.89      0.94      0.91       102\n",
      "          42       0.97      0.96      0.96        96\n",
      "          43       0.94      0.84      0.89       122\n",
      "          44       0.93      0.89      0.91       114\n",
      "          45       0.97      1.00      0.99       101\n",
      "          46       0.99      0.94      0.96        85\n",
      "          47       0.96      0.87      0.91       103\n",
      "          48       0.84      0.84      0.84        90\n",
      "          49       0.84      0.90      0.87       101\n",
      "          50       0.72      0.62      0.67       100\n",
      "          51       0.98      1.00      0.99        46\n",
      "          52       0.98      0.98      0.98        65\n",
      "          53       0.87      0.84      0.85       100\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.91      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 95,  accuracy score is 1.0\n",
      "at random state 95, confusion matrix is [[ 98   0   0 ...   0   0   0]\n",
      " [  0  94   0 ...   0   0   0]\n",
      " [  0   0 111 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  47   0   0]\n",
      " [  0   0   0 ...   0  82   0]\n",
      " [  0   2   0 ...   0   0  82]]\n",
      "at random state 95, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.92      0.91       107\n",
      "           1       0.83      0.85      0.84       110\n",
      "           2       0.97      0.99      0.98       112\n",
      "           3       0.88      0.88      0.88        98\n",
      "           4       0.96      0.97      0.96        98\n",
      "           5       0.86      0.87      0.87       103\n",
      "           6       0.98      1.00      0.99        86\n",
      "           7       0.89      0.93      0.91        94\n",
      "           8       0.92      0.98      0.95        87\n",
      "           9       0.95      0.87      0.91       102\n",
      "          10       0.85      0.82      0.83       100\n",
      "          11       0.93      0.92      0.93       109\n",
      "          12       0.95      0.98      0.96       123\n",
      "          13       0.85      0.80      0.82        88\n",
      "          14       0.90      0.92      0.91        95\n",
      "          15       1.00      0.98      0.99        84\n",
      "          16       0.95      0.93      0.94        99\n",
      "          17       0.92      0.89      0.91       102\n",
      "          18       0.89      0.92      0.90        96\n",
      "          19       0.93      0.93      0.93       107\n",
      "          20       0.97      0.95      0.96        99\n",
      "          21       0.83      0.76      0.79       109\n",
      "          22       0.99      0.99      0.99       106\n",
      "          23       0.86      0.89      0.88       107\n",
      "          24       0.76      0.76      0.76        94\n",
      "          25       0.93      0.99      0.96        93\n",
      "          26       0.76      0.78      0.77        83\n",
      "          27       0.89      0.93      0.91       102\n",
      "          28       0.98      0.98      0.98       104\n",
      "          29       0.97      0.97      0.97        79\n",
      "          30       0.98      0.97      0.98       103\n",
      "          31       0.74      0.72      0.73        99\n",
      "          32       0.95      0.92      0.93        97\n",
      "          33       0.88      0.89      0.88        99\n",
      "          34       0.91      0.89      0.90        95\n",
      "          35       0.94      0.96      0.95        91\n",
      "          36       0.87      0.83      0.85       106\n",
      "          37       0.92      0.86      0.89       101\n",
      "          38       0.86      0.94      0.90       107\n",
      "          39       0.95      0.96      0.96       106\n",
      "          40       0.89      0.95      0.92       112\n",
      "          41       0.85      0.89      0.87        99\n",
      "          42       0.96      0.93      0.95        87\n",
      "          43       0.89      0.93      0.91        98\n",
      "          44       0.90      0.94      0.92        98\n",
      "          45       1.00      0.99      0.99        81\n",
      "          46       1.00      0.98      0.99        90\n",
      "          47       0.90      0.85      0.88        99\n",
      "          48       0.86      0.89      0.88       100\n",
      "          49       0.94      0.90      0.92       105\n",
      "          50       0.65      0.61      0.63       119\n",
      "          51       1.00      1.00      1.00        47\n",
      "          52       1.00      1.00      1.00        82\n",
      "          53       0.85      0.83      0.84        99\n",
      "\n",
      "    accuracy                           0.90      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.90      0.90      0.90      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 96,  accuracy score is 1.0\n",
      "at random state 96, confusion matrix is [[100   0   0 ...   0   0   0]\n",
      " [  0  85   0 ...   0   0   0]\n",
      " [  0   0 104 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  47   0   0]\n",
      " [  0   0   0 ...   0  81   0]\n",
      " [  0   1   0 ...   0   0  75]]\n",
      "at random state 96, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.91       109\n",
      "           1       0.83      0.87      0.85        98\n",
      "           2       0.99      1.00      1.00       104\n",
      "           3       0.89      0.88      0.88       109\n",
      "           4       0.98      0.97      0.98       102\n",
      "           5       0.86      0.77      0.81        99\n",
      "           6       0.98      0.99      0.98        82\n",
      "           7       0.89      0.94      0.91        97\n",
      "           8       0.97      1.00      0.98        97\n",
      "           9       0.91      0.91      0.91       101\n",
      "          10       0.83      0.84      0.84       107\n",
      "          11       0.96      0.94      0.95       116\n",
      "          12       0.99      0.98      0.99       111\n",
      "          13       0.90      0.88      0.89        97\n",
      "          14       0.93      0.95      0.94        86\n",
      "          15       1.00      0.95      0.97        82\n",
      "          16       0.99      0.93      0.96       103\n",
      "          17       0.90      0.96      0.93       103\n",
      "          18       0.93      0.93      0.93       106\n",
      "          19       0.84      0.85      0.84       104\n",
      "          20       0.95      0.92      0.93        95\n",
      "          21       0.83      0.83      0.83        99\n",
      "          22       0.99      0.99      0.99        83\n",
      "          23       0.83      0.86      0.84        99\n",
      "          24       0.73      0.72      0.73        98\n",
      "          25       0.97      0.99      0.98        96\n",
      "          26       0.85      0.80      0.82        91\n",
      "          27       0.95      0.94      0.95        87\n",
      "          28       0.97      0.96      0.97       102\n",
      "          29       0.99      0.99      0.99        96\n",
      "          30       0.98      0.97      0.98       108\n",
      "          31       0.81      0.76      0.78        96\n",
      "          32       0.95      0.91      0.93       103\n",
      "          33       0.86      0.88      0.87        99\n",
      "          34       0.98      0.96      0.97        92\n",
      "          35       0.96      0.97      0.96       112\n",
      "          36       0.89      0.88      0.88        97\n",
      "          37       0.90      0.86      0.88        92\n",
      "          38       0.90      0.96      0.93        99\n",
      "          39       0.90      0.95      0.92       113\n",
      "          40       0.90      0.93      0.91       101\n",
      "          41       0.87      0.92      0.89       110\n",
      "          42       0.92      0.96      0.94        80\n",
      "          43       0.93      0.94      0.94       101\n",
      "          44       0.96      0.88      0.92       110\n",
      "          45       0.98      0.99      0.99       100\n",
      "          46       0.98      0.97      0.97        96\n",
      "          47       0.90      0.87      0.88       112\n",
      "          48       0.95      0.82      0.88       102\n",
      "          49       0.80      0.95      0.86        91\n",
      "          50       0.70      0.73      0.72       105\n",
      "          51       1.00      1.00      1.00        47\n",
      "          52       0.99      1.00      0.99        81\n",
      "          53       0.86      0.83      0.85        90\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.91      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 97,  accuracy score is 1.0\n",
      "at random state 97, confusion matrix is [[ 93   0   0 ...   0   0   0]\n",
      " [  0  95   0 ...   0   0   1]\n",
      " [  0   0 100 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  59   0   0]\n",
      " [  0   0   0 ...   0  89   0]\n",
      " [  0   0   1 ...   0   0  82]]\n",
      "at random state 97, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.96      0.92        97\n",
      "           1       0.87      0.89      0.88       107\n",
      "           2       0.97      1.00      0.99       100\n",
      "           3       0.90      0.81      0.85       107\n",
      "           4       0.98      0.96      0.97        99\n",
      "           5       0.95      0.88      0.92       111\n",
      "           6       0.97      0.97      0.97        77\n",
      "           7       0.94      0.93      0.94       110\n",
      "           8       1.00      1.00      1.00        93\n",
      "           9       0.91      0.91      0.91        82\n",
      "          10       0.91      0.85      0.88        94\n",
      "          11       0.92      0.96      0.94       118\n",
      "          12       0.95      0.93      0.94       114\n",
      "          13       0.85      0.89      0.87        99\n",
      "          14       0.85      0.89      0.87        88\n",
      "          15       0.98      0.97      0.97        97\n",
      "          16       0.97      0.96      0.97       107\n",
      "          17       0.95      0.94      0.95       111\n",
      "          18       0.88      0.91      0.90       101\n",
      "          19       0.89      0.90      0.90        94\n",
      "          20       1.00      0.95      0.98       103\n",
      "          21       0.78      0.83      0.81       103\n",
      "          22       1.00      0.98      0.99        95\n",
      "          23       0.85      0.89      0.87       103\n",
      "          24       0.73      0.75      0.74        88\n",
      "          25       0.96      0.95      0.95        91\n",
      "          26       0.79      0.82      0.81        89\n",
      "          27       0.87      0.93      0.90       109\n",
      "          28       0.97      0.89      0.93       103\n",
      "          29       0.94      0.97      0.95       104\n",
      "          30       0.97      0.97      0.97        99\n",
      "          31       0.77      0.72      0.74       106\n",
      "          32       0.94      0.89      0.91        88\n",
      "          33       0.91      0.93      0.92       115\n",
      "          34       0.92      0.93      0.93        91\n",
      "          35       0.94      0.98      0.96        96\n",
      "          36       0.85      0.89      0.87        95\n",
      "          37       0.89      0.93      0.91        95\n",
      "          38       0.92      0.94      0.93        98\n",
      "          39       0.91      0.95      0.93        88\n",
      "          40       0.91      0.93      0.92       106\n",
      "          41       0.91      0.85      0.88       112\n",
      "          42       0.98      0.98      0.98        84\n",
      "          43       0.95      0.87      0.91        94\n",
      "          44       0.91      0.95      0.93       111\n",
      "          45       0.98      1.00      0.99        95\n",
      "          46       0.99      0.99      0.99        88\n",
      "          47       0.91      0.89      0.90        90\n",
      "          48       0.91      0.88      0.90        98\n",
      "          49       0.95      0.95      0.95       106\n",
      "          50       0.62      0.62      0.62        99\n",
      "          51       1.00      1.00      1.00        59\n",
      "          52       0.95      1.00      0.97        89\n",
      "          53       0.94      0.82      0.88       100\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.91      5296\n",
      "\n",
      "\n",
      "\n",
      "at random state 98,  accuracy score is 1.0\n",
      "at random state 98, confusion matrix is [[ 87   0   0 ...   0   0   0]\n",
      " [  0  71   0 ...   0   0   0]\n",
      " [  0   0 105 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  61   0   0]\n",
      " [  0   0   0 ...   1  82   0]\n",
      " [  0   0   3 ...   0   0  80]]\n",
      "at random state 98, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.98      0.89        89\n",
      "           1       0.80      0.82      0.81        87\n",
      "           2       0.96      0.97      0.97       108\n",
      "           3       0.90      0.85      0.88       103\n",
      "           4       0.95      0.95      0.95       120\n",
      "           5       0.89      0.82      0.85        99\n",
      "           6       0.98      0.99      0.98        81\n",
      "           7       0.94      0.88      0.91       110\n",
      "           8       0.96      0.97      0.96        93\n",
      "           9       0.93      0.93      0.93        97\n",
      "          10       0.86      0.92      0.89        99\n",
      "          11       0.92      0.98      0.95        92\n",
      "          12       0.99      0.90      0.94       107\n",
      "          13       0.83      0.89      0.86        81\n",
      "          14       0.89      0.94      0.91       108\n",
      "          15       1.00      1.00      1.00        88\n",
      "          16       0.97      0.96      0.97       112\n",
      "          17       0.93      0.95      0.94        93\n",
      "          18       0.88      0.94      0.91        96\n",
      "          19       0.89      0.95      0.92       131\n",
      "          20       0.97      0.95      0.96        98\n",
      "          21       0.71      0.89      0.79        88\n",
      "          22       0.98      0.99      0.98        93\n",
      "          23       0.84      0.89      0.86       114\n",
      "          24       0.81      0.74      0.77       103\n",
      "          25       1.00      0.97      0.98        87\n",
      "          26       0.90      0.66      0.76       106\n",
      "          27       0.79      0.93      0.86        96\n",
      "          28       0.96      0.96      0.96        96\n",
      "          29       0.99      0.99      0.99       110\n",
      "          30       0.98      0.96      0.97       106\n",
      "          31       0.74      0.87      0.80        86\n",
      "          32       0.97      0.92      0.95       104\n",
      "          33       0.93      0.84      0.89       101\n",
      "          34       0.96      0.89      0.92       109\n",
      "          35       0.96      1.00      0.98        85\n",
      "          36       0.80      0.93      0.86        88\n",
      "          37       0.82      0.89      0.85       102\n",
      "          38       0.89      0.91      0.90       105\n",
      "          39       0.89      0.89      0.89       104\n",
      "          40       0.91      0.84      0.88        96\n",
      "          41       0.85      0.90      0.87        98\n",
      "          42       0.95      0.97      0.96        95\n",
      "          43       0.94      0.79      0.86       105\n",
      "          44       0.93      0.87      0.90        97\n",
      "          45       0.98      0.96      0.97        97\n",
      "          46       0.94      0.99      0.97        84\n",
      "          47       0.90      0.83      0.87        89\n",
      "          48       0.89      0.78      0.83       110\n",
      "          49       0.91      0.88      0.89       110\n",
      "          50       0.67      0.57      0.62       102\n",
      "          51       0.98      1.00      0.99        61\n",
      "          52       0.96      0.99      0.98        83\n",
      "          53       0.86      0.85      0.86        94\n",
      "\n",
      "    accuracy                           0.90      5296\n",
      "   macro avg       0.90      0.90      0.90      5296\n",
      "weighted avg       0.90      0.90      0.90      5296\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at random state 99,  accuracy score is 1.0\n",
      "at random state 99, confusion matrix is [[93  0  0 ...  0  0  0]\n",
      " [ 0 83  0 ...  0  0  0]\n",
      " [ 0  0 76 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 55  0  0]\n",
      " [ 0  0  0 ...  0 84  0]\n",
      " [ 0  1  1 ...  0  0 82]]\n",
      "at random state 99, classification report is               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.91      0.90       102\n",
      "           1       0.84      0.83      0.83       100\n",
      "           2       0.96      0.96      0.96        79\n",
      "           3       0.91      0.84      0.87        97\n",
      "           4       0.94      0.99      0.96       109\n",
      "           5       0.86      0.89      0.88        92\n",
      "           6       0.96      1.00      0.98        91\n",
      "           7       0.87      0.94      0.90        90\n",
      "           8       0.98      0.99      0.99       112\n",
      "           9       0.87      0.89      0.88       101\n",
      "          10       0.83      0.88      0.86        95\n",
      "          11       0.95      0.93      0.94       105\n",
      "          12       0.95      0.97      0.96       107\n",
      "          13       0.83      0.91      0.87        98\n",
      "          14       0.97      0.91      0.94       109\n",
      "          15       0.98      0.99      0.98        81\n",
      "          16       0.98      0.96      0.97        91\n",
      "          17       0.97      0.95      0.96       104\n",
      "          18       0.89      0.94      0.91        94\n",
      "          19       0.87      0.87      0.87       102\n",
      "          20       0.95      0.94      0.94       109\n",
      "          21       0.84      0.83      0.84       103\n",
      "          22       0.99      0.96      0.97        97\n",
      "          23       0.83      0.94      0.88        96\n",
      "          24       0.77      0.75      0.76       108\n",
      "          25       0.99      0.98      0.99       103\n",
      "          26       0.87      0.83      0.85        93\n",
      "          27       0.94      0.94      0.94       108\n",
      "          28       0.92      0.96      0.94        82\n",
      "          29       0.98      0.97      0.97        96\n",
      "          30       0.98      0.95      0.96        98\n",
      "          31       0.76      0.82      0.79        82\n",
      "          32       0.91      0.90      0.91        94\n",
      "          33       0.87      0.92      0.89       107\n",
      "          34       0.98      0.92      0.95       114\n",
      "          35       0.92      0.96      0.94       111\n",
      "          36       0.79      0.85      0.82       105\n",
      "          37       0.90      0.92      0.91        88\n",
      "          38       0.91      0.89      0.90       105\n",
      "          39       0.93      0.92      0.93        93\n",
      "          40       0.89      0.91      0.90       105\n",
      "          41       0.92      0.93      0.92       109\n",
      "          42       0.92      0.98      0.95        83\n",
      "          43       0.89      0.82      0.85        92\n",
      "          44       0.91      0.89      0.90       118\n",
      "          45       0.97      0.99      0.98        87\n",
      "          46       1.00      0.99      0.99        97\n",
      "          47       0.86      0.81      0.83       113\n",
      "          48       0.91      0.83      0.87        93\n",
      "          49       0.89      0.90      0.90       105\n",
      "          50       0.74      0.69      0.71        96\n",
      "          51       1.00      1.00      1.00        55\n",
      "          52       1.00      0.99      0.99        85\n",
      "          53       0.95      0.77      0.85       107\n",
      "\n",
      "    accuracy                           0.91      5296\n",
      "   macro avg       0.91      0.91      0.91      5296\n",
      "weighted avg       0.91      0.91      0.91      5296\n",
      "\n",
      "\n",
      "\n",
      "Max accuracy at random state 50 = 0.9193731117824774\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf=RandomForestClassifier()\n",
    "model_selection(rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e216f95e",
   "metadata": {},
   "source": [
    "CV score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "56da4685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.65986973 0.81784703 0.77620397 0.71133144 0.51388102]\n",
      "0.6958266362761876\n",
      "0.10581776447974223\n"
     ]
    }
   ],
   "source": [
    "score=cross_val_score(rf,xx,yy,cv=5)\n",
    "print(score)\n",
    "print(score.mean())\n",
    "print(score.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd3d8e2",
   "metadata": {},
   "source": [
    "we will be proceeding with the random forest classifier model since the testing accuracy is high and the difference with the CV score is also lower as compared to other models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d927af30",
   "metadata": {},
   "source": [
    "# hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63959d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "cf9fd2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict={\"n_estimators\":[100,150,200],\n",
    "      \"min_samples_split\":[2,5,6],\n",
    "      \"max_features\":['sqrt', 'log2'],\n",
    "      \"criterion\":['gini', 'entropy', 'log_loss']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "81bc77e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gd=GridSearchCV(estimator=rf,param_grid=dict,cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f1fee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gd.fit(xx,yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044a8b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "gd.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6285bc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "gd.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cae91e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
